{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_GELU_DO_0.4_RedLR_노드수바꿈.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "d4h9A4H3E_XB"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoonJaeHoon/Dacon_Sloan-Digital-Sky-Survey_SDSS/blob/master/Tutorial_GELU_DO_0_4_RedLR_%EB%85%B8%EB%93%9C%EC%88%98%EB%B0%94%EA%BF%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkIsyTZNAJ2P",
        "colab_type": "code",
        "outputId": "35576e59-889c-4340-8ea0-7d37da3607e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoP1zL0pARNV",
        "colab_type": "code",
        "outputId": "71131619-cedc-42ef-d8ef-6d364fbb3b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report #for model evaluation\n",
        "from sklearn.metrics import confusion_matrix #for model evaluation\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit #for data splitting\n",
        "np.random.seed(123) #ensure reproducibility\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('seaborn') # matplotlib 도 종류가 다양하기 때문에 seaborn 스타일로 지정한 거임.\n",
        "sns.set(font_scale=1) # (기본으로) 폰트 크기 2.5로 지정 미리 해놓는거임, 2.5면 꽤 크게 나옴\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd \n",
        "\n",
        "from numpy.random import seed\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "#train = pd.read_csv('drive/My Drive/데이콘_천체유형/train.csv', index_col=0)\n",
        "train = pd.read_csv('drive/My Drive/데이콘_천체유형/이상치_최대한_적게_train.csv', index_col=None)\n",
        "test = pd.read_csv('drive/My Drive/데이콘_천체유형/test.csv', index_col=0)\n",
        "sample_submission = pd.read_csv('drive/My Drive/데이콘_천체유형/sample_submission.csv', index_col=0)\n",
        "\n",
        "#######################################################################################################\n",
        "## petroMag_g 이상치 하나 더 없애기\n",
        "name = 'petroMag_g'\n",
        "drop_index = np.argmax(np.array(train[name]))\n",
        "train = train.drop(index=drop_index,axis=0)\n",
        "######################################################################################################\n",
        "\n",
        "print('csv 파일 (train, test, sample)을 불러왔습니다')\n",
        "print('train shape : {0}'.format(train.shape))\n",
        "print('test shape : {0}'.format(test.shape))\n",
        "print('='*50)\n",
        "\n",
        "train_type_num = train['type_num']\n",
        "needscaling_train = train.drop(['type','fiberID','type_num'],axis=1)\n",
        "needscaling_test = test.drop(['fiberID'], axis=1)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(needscaling_train)\n",
        "scaled_train = pd.DataFrame(sc.transform(needscaling_train),\n",
        "                      columns=needscaling_train.columns,\n",
        "                      index = needscaling_train.index)\n",
        "\n",
        "scaled_test = pd.DataFrame(sc.transform(needscaling_test),\n",
        "                      columns=needscaling_test.columns,\n",
        "                      index = needscaling_test.index)\n",
        "\n",
        "scaled_train['type_num'] = train_type_num\n",
        "train_x = scaled_train.drop(['type_num'],axis=1)\n",
        "train_y = scaled_train['type_num']\n",
        "test_x = scaled_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "csv 파일 (train, test, sample)을 불러왔습니다\n",
            "train shape : (199898, 23)\n",
            "test shape : (10009, 21)\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4h9A4H3E_XB",
        "colab_type": "text"
      },
      "source": [
        "## petroMag_g 의 maximum 값 하나는 없애도 됨. ( 합리적으로)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKEZBrFkCwzC",
        "colab_type": "code",
        "outputId": "849981fa-bc18-4741-89ce-5d856547cd7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# setting outlier symbol, title, xlabel\n",
        "name = 'petroMag_g'\n",
        "plt.boxplot(train[name], sym=\"bo\")\n",
        "plt.title('Box plot of {0}'.format(name))\n",
        "plt.xticks([1], [name])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# setting outlier symbol, title, xlabel\n",
        "name = 'petroMag_g'\n",
        "plt.boxplot(test[name], sym=\"bo\")\n",
        "plt.title('Box plot of {0}'.format(name))\n",
        "plt.xticks([1], [name])\n",
        "plt.show()\n",
        "\n",
        "Q1 = train.describe().loc['25%',name]\n",
        "Q3 = train.describe().loc['75%',name]\n",
        "minimum = train.describe().loc['min',name]\n",
        "maximum = train.describe().loc['max',name]\n",
        "IQR = Q3-Q1\n",
        "\n",
        "print(maximum - Q3 + (1.5*IQR))\n",
        "print(Q1 - (1.5*IQR) -  minimum)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEPCAYAAABY9lNGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSapkQEiSKv1/86kb8I35pz8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEPCAYAAAC6Kkg/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZRUlEQVR4nO3de5TdZX3v8feEQAomEBiGgJoLInwr\n3hCIFw6CijdaOSqtCoqgPRXThRTbUqq2VbRaQBTbiktQKyI5uBAbe85BY/VYCQeN4SYqIF8VSLjL\nMKhJlCYkM+eP329cO8Nc9jN7Jr894f1aK4vZz/d3efaQ7M9+nue3969naGgISZJKzGq6A5Kkmcfw\nkCQVMzwkScUMD0lSMcNDklTM8JAkFZvddAekJkTE1cDyzPzcdjjXnwFnA08CFmfmwHSfU5puhoe2\nq4hYCywAtgKPAd8DlmXmPQ12a0wRsQS4C9g5M7dMYv+dgQuAF2bmD7upbyOOcXNmPq+lfW/gfuD+\nzFzScWe1w3HaSk04LjPnAvsBvwA+2XB/ptMC4PeAW5s4eUS0+wZxt4h4VsvjN1OFijQqRx5qTGb+\nV0R8Bfin4baI2IMqTI4Ffgt8FvjHzByMiE8D+2TmH9XbngccDrw8M7f5qoSIeBvwDuAHwFuBB4DT\nMvPbI/sREbOA99Xb7wp8Azg9M38NXFNv9quIAHhFZq4esf8c4DzgjXXTl4G/ARbX5x/e/7rMfNmI\nfZdQvUi/k2pqqwf4eGZ+rKVvZ9V9mw98m2qk9shofQOi3vY64GTg0xHx/nGe37DLgFOAv64fnwx8\nsd5nuK/vqR/vA9wD/G1mfrWu7QR8tD7GBuDjVP8fxx0VRcT+wKXA84A1QAJ7ZOZJY+2j7uDIQ42J\niN2ANwHfb2n+JLAH8DTgaKoXsbfXtb8Cnh0Rb4uIFwP/AzhlZHC0eAFwB7A38AFgRUTsNcp2b6v/\nvLQ+71zgwrp2VP3f+Zk5d2Rw1P4WeCFwCPBc4PnA32XmT4Fntuz/slH2HfZS4EDglcDfRMTL6/bT\ngddR/S6eDPwS+NQEfXsBcCfVqOcjEzy/YcuBEyJip4g4uN5mzYht7gBeTPX/54PA8ojYr669gyrw\nDwEOrfvcjsupgq6XKjzf2uZ+apjhoSb8e0T8Cvg11bvl8+F3715PAN6bmRsycy3VO9i3AmTmb+uf\nL6B6sTs9M+8d5zwPAf+UmY9l5hVU72r/cJTt3gJckJl3ZuZG4L1UL6TtjszfAnwoMx/KzH6qF9bS\nF8EPZuZvMvPHwCXAiXX7Mqp3+Pdm5iaqF9g/nqBv92fmJzNzS2Y+2ubzu5fq9/NyqsC+bORBM/PK\nzLw/Mwfr3+fPqIISqlHXP9f9/CVw7kRPOCIWAUuB92fm5sy8FvjfE+2n7uC0lZrwusz8v3VYvBZY\nVb/bHQJ2Bta1bLsOeMrwg8xcExF3Uk2dfHmC89w3YlSyjurd+0hPHuWcs6neubdjtP1HO894Wi8Y\nWAc8u/55MfDViBhsqW+doG8jLz5o9/l9kWqEcgTVCOOg1mJEnAz8JbCkbppLNaobPkfredu5AOLJ\nwCP1m4LW/Ra2sa8a5shDjcnMrZm5gurF8EjgYaorsBa3bLYIuG/4QUScBsyhuhLorAlO8ZSI6Blx\nrPtH2e7+Uc65hWoxv52vnR5t/9HOM57WF8zW/e8Bjs3M+S1/fi8z7xunbyPbx3t+rf6NamR2Z2be\n3VqIiMVU60/vAnozcz5wC9UaDVRrSk8d4/mM5QFgr3r6smQ/dQHDQ42JiJ6IeC2wJ/CTzNxKNZr4\nSETMq1+w/pJqioqIOAj4MHAS1bTQWRFxyDin2Af484jYOSLeADwD+Poo230J+IuI2D8i5gL/CFxR\nL/T2A4NUawVj+RLwdxHRV1/i+v7hPhf4+4jYLSKeSbXGc0XdfhHV72MxQH2O19a1dvo20fP7ncz8\nDfAy4E9HOcaTqEKpv+7H24HWq7O+DJwREU+JiPlUFwyMKzPXATcAZ0fELhHxIuC4ifZTd3DaSk34\nPxGxlerFaB3VovfwpaynUy2a3wn8F9W73c/X8/PLgfOGPy8REe8DLouIw+v1gJHWUC1CP0z1LvuP\nx/iA3uepplCuobqs9j/qfpCZv42IjwDfrT+z8erM/P6I/T8M7A78qH58Zd1WYhXwc6o3dB/LzG/W\n7f9M9e7+mxHxZKp1nCuA/zVa38Y49pjPb6TMvGGM9tsi4uPAaqrA+iLw3ZZNPks1zfUjYD3wL8BL\nqEaV43kL8AVggGrh/Apgpwn2URfo8WZQ2hHVl+r+aWYe2XRfxjMVH/TrRhFxLHBRZi6ecONt97sC\nuD0zPzA9PdNUceQhqWMRsSvVpcDfpFqI/wDw1Tb2Wwo8QhWgr6S6gGLCK7XUPMND0lToobpE+Qrg\nUeBrVGs/RMTGMfY5luqDjyuoPudxL/BnmfmDMbZXF3HaSpJU7Ikw8phD9UGkB5h48U6SVNmJ6vvn\nrgced0HKEyE8lgL/r+lOSNIM9WLg2pGNT4TweADgl7/8DYODTtGp+/T2zmVgYKxlAakZs2b1sOee\nT4L6NXSkJ0J4bAUYHBwyPNS1/LupLjbqdL+fMJckFTM8JEnFDA9JUjHDQ5JU7ImwYC51pdW3PsiK\nVXfwyPpN7LX7HI4/+gBe9Mx9m+6W1BbDQ2rA6lsf5NKVt7N5S3WPp4H1m7h05e0ABohmBKetpAas\nWHXH74Jj2OYtg6xYdUdDPZLKGB5SAwbWj3b7kbHbpW5jeEgN6N19TlG71G0MD6kBxx99ALvM3vaf\n3y6zZ3H80Qc01COpjAvmUgOGF8W92kozlSMPSVIxRx5SA7xUVzOdIw+pAV6qq5nO8JAa4KW6mukM\nD0lSMcNDklTM8JAkFfNqK2kKHXXUC7j99p9MuN0f/sVX6enpeVz70NAQ++yz+7j7/v7vP4Nrrlkz\n6T5KU6FnaKh7750cEQcBlwK9wABwcmb+rPAwS4C7BgY2ep9oFbvxH17PQQv2aLobHfvpL37NYX//\n1aa7oRlk1qweenvnAuwPrB1Z7/aRx0XApzJzeUScBFwMvKzhPukJpPQFdypGHl/7xOvH3XcyI4/D\niraWJta14RER+wCHAq+om74EXBgRfZnZ31zPpLG1+6L+J+f+56jtPT09PPTQ+qnskjQtujY8gIXA\nfZm5FSAzt0bE/XV7cXjUwy+p6/X1zWu6C9KEujk8ppRrHpop+vs3NN0FqXXNY/T6duxLqXuAp0TE\nTgD1f59ct0uSGtS14ZGZDwE3AyfWTScCP3C9QzuCOTs/frF8vHap23RteNSWAadHxE+B0+vH0ox3\n8qufwciLrXp6qnZpJujqNY/MvB14QdP9kKaaN4PSTNfVHxKcIkvwQ4LqYn1981wkV9eZ6EOC3T5t\nJUnqQoaHJKmY4SFJKmZ4SJKKGR6SpGKGhySpmOEhSSpmeEiSihkekqRihockqVhXf7eVtCNbfeuD\nfreVZizDQ2rA6lsf5NKVt7N5yyAAA+s3cenK2wEMEM0ITltJDVix6o7fBcewzVsGWbHqjoZ6JJUx\nPKQGDKzfVNQudRvDQ2pA7+5zitqlbmN4SA04/ugD2GX2tv/8dpk9i+OPPqChHkllXDCXGuCdBDXT\nTeudBCPiU8AxwCZgI3BGZt5Q1xYAl1Hd6e9R4NTMXDNRbRKW4J0E1cW8k6C6UdN3ElwJPDsznwuc\nA1zRUjsHuCYzDwJOA5ZHRE8bNUlSw6Y1PDLzqsx8rH64GnhqRAyf843ARfV211KNTg5voyZJatj2\nXPN4F/C1zByMiF6gJzMfbqnfDSyMiDvHqgHXT/bk9fBL6kp9ffOa7oJUpKPwiIibgEVjlBdk5tZ6\nuxOANwNHdXK+TrjmoW7lmoe6Ucuax6g6Co/MPHSibSLi9cBHgGMy8xf1fgMRQUTs3TLCWATcM16t\nk75KkqbOtK55RMRrgAuAV2Xm2hHlK4Fl9XZHArsCN7ZRkyQ1bLrXPC4BNgNfiYjhtmMycwB4D9VV\nVKdQXY771swc/rKf8WqSpIZN6+c8usQS/JyHuphrHupGTX/OQ5K0AzI8JEnFDA9JUjHDQ5JUzPCQ\nJBUzPCRJxQwPSVIxw0OSVMzwkCQVMzwkScUMD0lSMcNDklTM8JAkFTM8JEnFDA9JUjHDQ5JUzPCQ\nJBUzPCRJxab7HuYARMRLgG8DZ2TmhXXbAuAyqtvEPgqcmplrJqpJkpo37SOPiJgHnAesHFE6B7gm\nMw8CTgOWR0RPGzVJUsO2x7TVBcD5wMMj2t8IXASQmdcCm4DD26hJkho2rdNWEXEssEdmfiUiXtPS\n3gv0ZGZroNwNLIyIO8eqAddPti+9vXMnu6s07fr65jXdBalIR+ERETcBi8YqA+cCr+jkHFNlYGAj\ng4NDTXdDepy+vnn0929ouhvSNmbN6hn3TXdH4ZGZh45Vi4gjgf2A6yICYG/guIjYKzM/FBFExN4t\nI4xFwD2ZOTBWrZO+SpKmzrRNW9VrFfsMP46ILwA3DF9tBVwJLAM+XAfNrsCNbdQkSQ3bLpfqjuE9\nVFdRnUJ1Oe5bM3OwjZokqWE9Q0M7/DrAEuAu1zzUrVzzUDdqWfPYH1j7uPr27pAkaeYzPCRJxQwP\nSVIxw0OSVMzwkCQVMzwkScUMD0lSMcNDklTM8JAkFTM8JEnFDA9JUjHDQ5JUzPCQJBUzPCRJxQwP\nSVIxw0OSVMzwkCQVMzwkScWm/R7mEXE6cBrwGLA1Mw+p23cDLgEOA7YAZ2bmVRPVJEnNm9aRR0Qc\nD7wBWJqZzwZe1VI+E1ifmU8HjgM+FxFz26hJkho23dNWfwWcnZkbADLzFy21NwEX1+0/A24Ajm2j\nJklq2HRPWx0MvDAiPgzsAlycmZ+ta4uAdS3b3g0sbKM2Kb29DlzUvfr65jXdBalIR+ERETdRvdCP\nZgGwE9WL/pHA3sB3IyIz85pOzjsZAwMbGRwc2t6nlSbU1zeP/v4NTXdD2sasWT3jvunuKDwy89Dx\n6hFxN/ClzBwEHoqIbwHPB66hGk0sBvrrzRcB36l/Hq8mSWrYdK95XA68GiAingS8GPhhXbsSeGdd\nOxBYCnyjjZokqWHTHR6fABZGxK3AdcDyzPxWXTsfmB8RPweuAk4dXlifoCZJaljP0NAOvw6wBLjL\nNQ91K9c81I1a1jz2B9Y+rr69OyRJmvkMD0lSMcNDklTM8JAkFTM8JEnFDA9JUjHDQ5JUzPCQJBUz\nPCRJxQwPSVIxw0OSVMzwkCQVMzwkScUMD0lSMcNDklTM8JAkFTM8JEnFDA9JUrHZ03nwiDgI+Aww\nH5gDXJGZZ9e13YBLgMOALcCZmXnVRDVJUvOme+TxUeArmXkIsBR4e0Q8v66dCazPzKcDxwGfi4i5\nbdQkSQ2b7vAYAvaof96tfvxQ/fhNwMUAmfkz4Abg2DZqkqSGTXd4vBt4U0TcB6wFzs/MtXVtEbCu\nZdu7gYVt1CRJDetozSMibqJ6oR/NAuCdwGWZeX5E7AdcHRE3ZOaaTs47Gb29znqpe/X1zWu6C1KR\njsIjMw8drx4Rfw48rd72gYj4T+AoYA3VaGIx0F9vvgj4Tv3zeLVJGRjYyODgUCeHkKZFX988+vs3\nNN0NaRuzZvWM+6Z7uqet7gJeDRAR84AXA7fUtSupRiZExIFUC+rfaKMmSWrYdIfH24BlEfFDqtHG\nlzNzZV07H5gfET8HrgJOzcwNbdQkSQ3rGRra4adylgB3OW2lbuW0lbpRy7TV/lQXPG1b394dkiTN\nfIaHJKmY4SFJKmZ4SJKKGR6SpGKGhySpmOEhSSpmeEiSihkekqRihockqZjhIUkqZnhIkooZHpKk\nYoaHJKmY4SFJKmZ4SJKKGR6SpGKGhySp2OxODxARJwFnAQcD787MC1tquwGXAIcBW4AzM/OqTmqS\npOZNxcjjZuAE4PJRamcC6zPz6cBxwOciYm6HNUlSwzoOj8y8JTNvAwZHKb8JuLje7mfADcCxHdYk\nSQ2b7jWPRcC6lsd3Aws7rEmSGjbhmkdE3ET1Yj6aBZm5dWq7ND16e531Uvfq65vXdBekIhOGR2Ye\n2sHx7wYWA/3140XAdzqsTcrAwEYGB4c6OYQ0Lfr65tHfv6HpbkjbmDWrZ9w33dM9bXUl8E6AiDgQ\nWAp8o8OaJKlhHYdHRJwYEfcCbwD+ISLujYiD6/L5wPyI+DlwFXBqZm7osCZJaljP0NAOP5WzBLjL\naSt1K6et1I1apq32B9Y+rr69OyRJmvkMD0lSMcNDklTM8JAkFTM8JEnFDA9JUjHDQ5JUzPCQJBUz\nPCRJxQwPSVIxw0OSVMzwkCQVMzwkScUMD0lSMcNDklTM8JAkFTM8JEnFDA9JUjHDQ5JUbHanB4iI\nk4CzgIOBd2fmhS21TwHHAJuAjcAZmXlDXVsAXEZ1j/FHgVMzc81ENUlS86Zi5HEzcAJw+Si1lcCz\nM/O5wDnAFS21c4BrMvMg4DRgeUT0tFGTJDWs4/DIzFsy8zZgcJTaVZn5WP1wNfDUiBg+5xuBi+rt\nrqUanRzeRk2S1LCOp60KvAv4WmYORkQv0JOZD7fU7wYWRsSdY9WA6yd78t7euZPdVZp2fX3zmu6C\nVGTC8IiIm4BFY5QXZObWNo5xAvBm4Kiy7k2dgYGNDA4ONXV6aUx9ffPo79/QdDekbcya1TPum+4J\nwyMzD+2kAxHxeuAjwDGZ+Yv6mAMRQUTs3TLCWATcM16tk35IkqbOtF6qGxGvAS4AXpWZa0eUrwSW\n1dsdCewK3NhGTZLUsJ6hoc6mciLiROB8YE9gM/Ab4JWZeVtE9Ndt/S27HFOPLvYFlgOLqS7HXZaZ\n36uPOWZtEpYAdzltpW7ltJW6Ucu01f7A2pH1jsNjBliC4aEuZnioG00UHn7CXJJUzPCQJBUzPCRJ\nxQwPSVIxw0OSVMzwkCQVMzwkScUMD0lSMcNDklTM8JAkFTM8JEnFtufNoCS1WH3rg6xYdQePrN/E\nXrvP4fijD+BFz9y36W5JbTE8pAasvvVBLl15O5u3VHdvHli/iUtX3g5ggGhGcNpKasCKVXf8LjiG\nbd4yyIpVdzTUI6mM4SE1YGD9pqJ2qdsYHlIDenefU9QudRvDQ2rA8UcfwC6zt/3nt8vsWRx/9AEN\n9Ugq44K51IDhRXGvttJMNRX3MD8JOAs4GHh3Zl44yjYvAb4NnDFcj4gFwGVUt4l9FDg1M9dMVJuE\nJXgbWnUxb0OrbrQ9bkN7M3ACcPloxYiYB5wHrBxROge4JjMPAk4DlkdETxs1SVLDOg6PzLwlM28D\nBsfY5ALgfODhEe1vBC6qj3EtsAk4vI2aJKlh07rmERHHAntk5lci4jUt7b1AT2a2BsrdwMKIuHOs\nGnD9ZPtSD7+krtTXN6/pLkhFJgyPiLgJWDRGeUFmbh1jv/nAucArJt+9qeOah7qVax7qRi1rHqOa\nMDwy89BJnvtZwH7AdREBsDdwXETslZkfiggiYu+WEcYi4J7MHBirNsl+7ATVL0LqVv79VLdp+Tu5\n02j1aZu2qtcq9hl+HBFfAG5ouRrrSmAZ8OGIOBLYFbixjVqp/QD23PNJk9xdmn5Oq6qL7Qc87ntz\npuJS3ROpFsT3BDYDvwFeWS+it273BVrCIyL2BZYDi6kux12Wmd+bqDYJc4ClwAPAqFNskqTH2Ykq\nOK6numhpGx2HhyTpicevJ5EkFTM8JEnFDA9JUjHDQ5JUzPCQJBUzPCRJxQwPSVIxw0OSVMzwkCQV\nMzwkScUMD0lSMcNDklTM8JAmISLmR8RZHez/kogYiojzR7RfXbf7He3qaoaHNDnzgTHDIyLauVdO\nAq+LiJ3qfZ4GeOMZzQjTeg9zaSaIiCHgQ8BrqW489r7M/Le69gKq2ynvXm/+/sz8GvApYH5E3Az8\nNjOPiIirgZuBFwKPAH8QEScDfw0MUd1Q552Z+VB9rI3ArcCrgK8DpwBfBA5v6dvHgKOBXYCHgT/J\nzHV17V3AGcCv6v1Py8y9x3meT6mPv2/dlx7gP1pu0Ca1zZGHVNmamYcA/x34TETsExHzgYuAN2fm\nYcBrgIvr9tOAX2XmIZl5RMtxngYcmZl/EBHPogqeV2bmc4BbgE+OOO8XgFMiogc4Abh8RP3czFya\nmc8FvgScBxARzwHeCxyRmUupRkIT+RfgO5n5TOB0qlCSJsXwkCr/CpCZCdxENXo4AtgfWFmPMFZS\njSCePs5xLs/MLfXPLwW+npkP1I8vBl4+YvurgecArwNuycyBEfVjI+L7EXELcCZwSN3+kvrY/fXj\nz7fxHF8KXFI/z3XAt9vYRxqV01bS2HqAH2XmUSMLEbFkjH02lpwgM4ci4svAZ4G3jzjHYuATwNLM\nvCsijuDxIxOpEY48pMrbASLiQOB5wPeB7wEHRsRLhzeKiKX1FNN6YLcJFsa/Q7XusW/9+B3At0bZ\n7jPAR6lGNq12BzYDD0bELGBZS20V1ahkeI3jlImfIlcPbxcRC4GXtbGPNCrDQ6rMjogfAFdRL2pn\n5i+p1kA+EBE/jIifAGcDPZn5CPA/gR9HxPdGO2Bm3gK8B/hWRPwIeC7VAvfI7e7LzI+2THcNt/8Y\nuBK4DVgD3NVS+yFV4KyOiBuBLcCvJ3iOZwCviIhbgU8D17WxjzSqnqGhoab7IDWqvtpqXmYWTTk1\nLSLmZeaG+uezgadn5knjbL8r8FhmbomI/YDrgWPqdR6piGse0sx1bkT8N6rLeO8ETp1g+wOBL9bT\nbjsDHzQ4NFmOPKQdSEQcQnX570gXZubntnN3tAMzPCRJxVwwlyQVMzwkScUMD0lSMcNDklTs/wN9\nb+/ar53IAgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "253.7626148220352\n",
            "475.60843595216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in4ouFUuFFLy",
        "colab_type": "text"
      },
      "source": [
        "## 딥러닝 모델 구축 시작 (Elu)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e37totm8FI28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#random seeds for stochastic parts of neural network \n",
        "np.random.seed(123)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(123)\n",
        "\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import  Activation,  Lambda, Flatten, LeakyReLU, ELU, Dense\n",
        "from keras.layers import Input, Concatenate, Reshape, Dropout, BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.optimizers import Adam, SGD,RMSprop\n",
        "from keras import  backend as K\n",
        "from keras import metrics\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
        "#from swa.keras import SWA # swa optimizer - https://pypi.org/project/keras-swa/\n",
        "import tensorflow as tf\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "\n",
        "################ Gelu #############\n",
        "\n",
        "class Gelu(Activation):\n",
        "    def __init__(self, activation, **kwargs):\n",
        "        super(Gelu, self).__init__(activation, **kwargs)\n",
        "        self.__name__='gelu'\n",
        "        \n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "\n",
        "get_custom_objects().update({'gelu': Activation(gelu)})\n",
        "\n",
        "################# tanh 활용 ( 범위 늘림 ) ##############\n",
        "def custom_activation(x):\n",
        "  return (K.tanh(x) * 145) + 155\n",
        "\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
        "\n",
        "def create_nn_model():\n",
        "    inp = Input(shape=(20,))\n",
        "    x = Dense(2373)(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.4)(x)\n",
        "    \n",
        "    x = Dense(2355)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.4)(x)\n",
        "\n",
        "    x = Dense(1197)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.35)(x)\n",
        "\n",
        "    x = Dense(1187)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.3)(x)\n",
        "\n",
        "#    x = Dense(612)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "#    x = Activation(gelu)(x)\n",
        "#    x = Dropout(rate = 0.3)(x)\n",
        "\n",
        "    x = Dense(607)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "#    x = ELU()(x)\n",
        "    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.25)(x)\n",
        "\n",
        "\n",
        "    out = Dense(19, activation='softmax')(x) #scalar_coupling_constant    \n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    return model\n",
        "\n",
        "nn_model = create_nn_model(  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IVJlTVTKYT-",
        "colab_type": "code",
        "outputId": "76064689-655e-42ab-a740-758c55aada31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "csv_folder = '0219_이상치1_GELU_DO_0.4_layer수_배치크기조정'\n",
        "rate = ''\n",
        "\n",
        "import os\n",
        "SAVEMODEL_NEWFOLDER0 = 'drive/My Drive/데이콘_천체유형/ModelCheck/' + csv_folder\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "if not os.path.exists(SAVEMODEL_NEWFOLDER1):\n",
        "  os.mkdir(SAVEMODEL_NEWFOLDER1)\n",
        "  print('모델폴더를 새로 생성했습니다.')\n",
        "SUBMISSION_NEWFOLDER = 'drive/My Drive/데이콘_천체유형/파일제출/' + csv_folder\n",
        "if not os.path.exists(SUBMISSION_NEWFOLDER):\n",
        "  os.mkdir(SUBMISSION_NEWFOLDER)\n",
        "  print('제출폴더를 새로 생성했습니다.')\n",
        "\n",
        "################## StratifiedShuffleSplit 를 이용해서 층화분할 #############\n",
        "X_array = train_x.values\n",
        "y_array = train_y.values\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=2, test_size=1/13, random_state=123)\n",
        "index1, index2 = sss.split(X_array, y_array)\n",
        "train_index = index1[0].tolist()\n",
        "val_index = index1[1].tolist()\n",
        "train_input = X_array[train_index]\n",
        "cv_input = X_array[val_index]\n",
        "train_target  = y_array[train_index]\n",
        "cv_target = y_array[val_index]\n",
        "\n",
        "\n",
        "\n",
        "#train_index, val_index = train_test_split(np.arange(len(train_y)),random_state=42, test_size=1/13)\n",
        "#train_input=train_x.iloc[train_index].values\n",
        "#train_target0=train_y[train_index]\n",
        "#train_target=train_target0.values\n",
        "#cv_input=train_x.iloc[val_index].values\n",
        "#cv_target0=train_y[val_index]\n",
        "#cv_target = cv_target0.values\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "모델폴더를 새로 생성했습니다.\n",
            "제출폴더를 새로 생성했습니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFJe8RqARKMq",
        "colab_type": "text"
      },
      "source": [
        "# Reduce LR on Plateau\n",
        "\n",
        "\n",
        "\n",
        "> patience를 지정해놓고 metric(일반적으로 val_loss)를 보면서 줄어들지 않을 때에 (손실감소곡선이 너무 평탄할때) 일정상수를 initial_learning_rate에 곱해 local minima를 탈출하는 방법\n",
        "\n",
        "# 스케쥴러\n",
        "\n",
        "\n",
        "\n",
        "> 스케쥴러도 위의 Reduce LR과 굉장히 유사함. 하지만 스케쥴러는 위와 달리  (Linear, Cosine) 등 여러가지 종류의 스케쥴러로 **patience를 지정해서 쓰는게 아니라** 일정 패턴을 따라 큰 값에서 작은 값을 왔다갔다하며 그 사이에서 학습되도록 함.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oy650Oa7Fv9G"
      },
      "source": [
        "## initial_rate = 1e-4 (과적합 심함)\n",
        "\n",
        "### 1. lr 줄여서 local minima 문제 없앤듯\n",
        "### 2. Drop Out 비율 더 높게 해주고\n",
        "### 배치크기 256*6으로 올림\n",
        "\n",
        "### 3. Gradient Exploding 문제 해결을 하고 싶다면... clip value 지정해줘야 할 듯 (얘는 아직 적용 안해봄)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "250a1fd2-e9ce-4e57-fbe4-65fe4943d00b",
        "id": "tDPK5E_dFv9R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch =  800\n",
        "pat =  80\n",
        "red_pat =  25\n",
        "batchsize = 256*6\n",
        "initial_rate = 1e-4\n",
        "factor = 1/np.sqrt(10) # red_patience 만큼 기다리다가, 학습률을 *factor 배로 줄여버림 \n",
        "minimumlr = 1e-7\n",
        "min_delta = 1e-6\n",
        "\n",
        "import os\n",
        "MODEL_SAVE_FOLDER_PATH0 = SAVEMODEL_NEWFOLDER0 +  '/initial_rate=%s/' % initial_rate ## checkpoint\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "if not os.path.exists(MODEL_SAVE_FOLDER_PATH1):\n",
        "  os.mkdir(MODEL_SAVE_FOLDER_PATH1)\n",
        "check_path = MODEL_SAVE_FOLDER_PATH0 + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "### model early stopping : 더이상 성능 개선이 되지 않으면 멈춤\n",
        "es = EarlyStopping(monitor= 'val_loss', patience = pat, verbose = 1, mode='min',\n",
        "                    restore_best_weights = True\n",
        "                   #,min_delta = 1e-6\n",
        "                   ) \n",
        "\n",
        "### model check point\n",
        "mc = ModelCheckpoint(filepath=check_path, monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "## ReduceLR on Plateau : val_loss가 안 줄어들 때 lr을 작게 할 수 있음 (local minima 대처방법)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor = factor,   # patience 만큼 기다리다가 0.1이면 학습률을 0.1배로 줄여버림 \n",
        "                        patience = red_pat, mode = 'min', verbose = 1,\n",
        "                        min_lr = minimumlr\n",
        "                        )\n",
        "\n",
        "from keras import optimizers\n",
        "optimizer = optimizers.Adam(\n",
        "    lr=initial_rate,\n",
        ")\n",
        "\n",
        "## compile model\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "\n",
        "nn_model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              #metrics=['accuracy']\n",
        "              #metrics=[metrics.sparse_categorical_accuracy]\n",
        "              metrics=[CCE]\n",
        "              )\n",
        "## fitting model\n",
        "hist = nn_model.fit(  train_input, train_target,validation_data=[cv_input, cv_target],\n",
        "                    batch_size=batchsize,\n",
        "                    epochs=epoch,\n",
        "                    callbacks = [es \n",
        "                                 #,mc\n",
        "                                 ,rlr\n",
        "                                ] )\n",
        "\n",
        "## save model\n",
        "model_json = nn_model.to_json()\n",
        "with open(json_path, \"w\") as json_file : \n",
        "    json_file.write(model_json)\n",
        "## model weight save\n",
        "nn_model.save_weights(weight_path)\n",
        "print(\"모델저장완료\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"Loss와 ACC에 대한 Plot을 그립니다\")\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "loss_ax.legend(loc='lower left')\n",
        "\n",
        "#acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "#acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "#acc_ax.set_ylabel('accuracy')\n",
        "#acc_ax.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## evaluate model\n",
        "print('기존nn모델의 train loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(train_input, train_target, batch_size=batchsize, verbose=0)\n",
        "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
        "## evaluate model\n",
        "print('기존nn모델의 valid loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(cv_input, cv_target, batch_size=batchsize, verbose=0)\n",
        "print(\"valid, loss and metric: {}\".format(loss_and_metric))\n",
        "## model weight save ## 기존 모델의 가중치 저장\n",
        "#nn_model.save_weights(weight_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 184521 samples, validate on 15377 samples\n",
            "Epoch 1/800\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "184521/184521 [==============================] - 18s 96us/step - loss: 0.8977 - sparse_categorical_crossentropy: 0.8977 - val_loss: 0.5915 - val_sparse_categorical_crossentropy: 0.5915\n",
            "Epoch 2/800\n",
            "184521/184521 [==============================] - 6s 35us/step - loss: 0.5966 - sparse_categorical_crossentropy: 0.5966 - val_loss: 0.5356 - val_sparse_categorical_crossentropy: 0.5356\n",
            "Epoch 3/800\n",
            "184521/184521 [==============================] - 6s 35us/step - loss: 0.5453 - sparse_categorical_crossentropy: 0.5453 - val_loss: 0.5534 - val_sparse_categorical_crossentropy: 0.5534\n",
            "Epoch 4/800\n",
            "184521/184521 [==============================] - 6s 35us/step - loss: 0.5180 - sparse_categorical_crossentropy: 0.5180 - val_loss: 0.4909 - val_sparse_categorical_crossentropy: 0.4909\n",
            "Epoch 5/800\n",
            "184521/184521 [==============================] - 6s 35us/step - loss: 0.5018 - sparse_categorical_crossentropy: 0.5018 - val_loss: 0.4728 - val_sparse_categorical_crossentropy: 0.4728\n",
            "Epoch 6/800\n",
            "184521/184521 [==============================] - 7s 35us/step - loss: 0.4891 - sparse_categorical_crossentropy: 0.4891 - val_loss: 0.4651 - val_sparse_categorical_crossentropy: 0.4651\n",
            "Epoch 7/800\n",
            "184521/184521 [==============================] - 6s 35us/step - loss: 0.4790 - sparse_categorical_crossentropy: 0.4790 - val_loss: 0.4497 - val_sparse_categorical_crossentropy: 0.4497\n",
            "Epoch 8/800\n",
            "184521/184521 [==============================] - 7s 35us/step - loss: 0.4723 - sparse_categorical_crossentropy: 0.4723 - val_loss: 0.4534 - val_sparse_categorical_crossentropy: 0.4534\n",
            "Epoch 9/800\n",
            "184521/184521 [==============================] - 6s 35us/step - loss: 0.4632 - sparse_categorical_crossentropy: 0.4632 - val_loss: 0.4409 - val_sparse_categorical_crossentropy: 0.4409\n",
            "Epoch 10/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4573 - sparse_categorical_crossentropy: 0.4573 - val_loss: 0.4338 - val_sparse_categorical_crossentropy: 0.4338\n",
            "Epoch 11/800\n",
            "184521/184521 [==============================] - 7s 35us/step - loss: 0.4539 - sparse_categorical_crossentropy: 0.4539 - val_loss: 0.4501 - val_sparse_categorical_crossentropy: 0.4501\n",
            "Epoch 12/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4483 - sparse_categorical_crossentropy: 0.4483 - val_loss: 0.4353 - val_sparse_categorical_crossentropy: 0.4353\n",
            "Epoch 13/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4439 - sparse_categorical_crossentropy: 0.4439 - val_loss: 0.4301 - val_sparse_categorical_crossentropy: 0.4301\n",
            "Epoch 14/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4421 - sparse_categorical_crossentropy: 0.4421 - val_loss: 0.4251 - val_sparse_categorical_crossentropy: 0.4251\n",
            "Epoch 15/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4377 - sparse_categorical_crossentropy: 0.4377 - val_loss: 0.4169 - val_sparse_categorical_crossentropy: 0.4169\n",
            "Epoch 16/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4340 - sparse_categorical_crossentropy: 0.4340 - val_loss: 0.4148 - val_sparse_categorical_crossentropy: 0.4148\n",
            "Epoch 17/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4321 - sparse_categorical_crossentropy: 0.4321 - val_loss: 0.4173 - val_sparse_categorical_crossentropy: 0.4173\n",
            "Epoch 18/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4295 - sparse_categorical_crossentropy: 0.4295 - val_loss: 0.4164 - val_sparse_categorical_crossentropy: 0.4164\n",
            "Epoch 19/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4257 - sparse_categorical_crossentropy: 0.4257 - val_loss: 0.4210 - val_sparse_categorical_crossentropy: 0.4210\n",
            "Epoch 20/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4239 - sparse_categorical_crossentropy: 0.4239 - val_loss: 0.4189 - val_sparse_categorical_crossentropy: 0.4189\n",
            "Epoch 21/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4228 - sparse_categorical_crossentropy: 0.4228 - val_loss: 0.4203 - val_sparse_categorical_crossentropy: 0.4203\n",
            "Epoch 22/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4220 - sparse_categorical_crossentropy: 0.4220 - val_loss: 0.4029 - val_sparse_categorical_crossentropy: 0.4029\n",
            "Epoch 23/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4177 - sparse_categorical_crossentropy: 0.4177 - val_loss: 0.4027 - val_sparse_categorical_crossentropy: 0.4027\n",
            "Epoch 24/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4167 - sparse_categorical_crossentropy: 0.4167 - val_loss: 0.4100 - val_sparse_categorical_crossentropy: 0.4100\n",
            "Epoch 25/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4189 - sparse_categorical_crossentropy: 0.4189 - val_loss: 0.3979 - val_sparse_categorical_crossentropy: 0.3979\n",
            "Epoch 26/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4157 - sparse_categorical_crossentropy: 0.4157 - val_loss: 0.3996 - val_sparse_categorical_crossentropy: 0.3996\n",
            "Epoch 27/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4117 - sparse_categorical_crossentropy: 0.4117 - val_loss: 0.3965 - val_sparse_categorical_crossentropy: 0.3965\n",
            "Epoch 28/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4098 - sparse_categorical_crossentropy: 0.4098 - val_loss: 0.3924 - val_sparse_categorical_crossentropy: 0.3924\n",
            "Epoch 29/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4092 - sparse_categorical_crossentropy: 0.4092 - val_loss: 0.3935 - val_sparse_categorical_crossentropy: 0.3935\n",
            "Epoch 30/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4073 - sparse_categorical_crossentropy: 0.4073 - val_loss: 0.3887 - val_sparse_categorical_crossentropy: 0.3887\n",
            "Epoch 31/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4074 - sparse_categorical_crossentropy: 0.4074 - val_loss: 0.3990 - val_sparse_categorical_crossentropy: 0.3990\n",
            "Epoch 32/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4063 - sparse_categorical_crossentropy: 0.4063 - val_loss: 0.3917 - val_sparse_categorical_crossentropy: 0.3917\n",
            "Epoch 33/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4033 - sparse_categorical_crossentropy: 0.4033 - val_loss: 0.3872 - val_sparse_categorical_crossentropy: 0.3872\n",
            "Epoch 34/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4040 - sparse_categorical_crossentropy: 0.4040 - val_loss: 0.3870 - val_sparse_categorical_crossentropy: 0.3870\n",
            "Epoch 35/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.4015 - sparse_categorical_crossentropy: 0.4015 - val_loss: 0.3911 - val_sparse_categorical_crossentropy: 0.3911\n",
            "Epoch 36/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.4014 - sparse_categorical_crossentropy: 0.4014 - val_loss: 0.3871 - val_sparse_categorical_crossentropy: 0.3871\n",
            "Epoch 37/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.4009 - sparse_categorical_crossentropy: 0.4009 - val_loss: 0.3905 - val_sparse_categorical_crossentropy: 0.3905\n",
            "Epoch 38/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.4003 - sparse_categorical_crossentropy: 0.4003 - val_loss: 0.3818 - val_sparse_categorical_crossentropy: 0.3818\n",
            "Epoch 39/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.4000 - sparse_categorical_crossentropy: 0.4000 - val_loss: 0.3838 - val_sparse_categorical_crossentropy: 0.3838\n",
            "Epoch 40/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.3972 - sparse_categorical_crossentropy: 0.3972 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 41/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3963 - sparse_categorical_crossentropy: 0.3963 - val_loss: 0.3943 - val_sparse_categorical_crossentropy: 0.3943\n",
            "Epoch 42/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3984 - sparse_categorical_crossentropy: 0.3984 - val_loss: 0.3840 - val_sparse_categorical_crossentropy: 0.3840\n",
            "Epoch 43/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3941 - sparse_categorical_crossentropy: 0.3941 - val_loss: 0.3824 - val_sparse_categorical_crossentropy: 0.3824\n",
            "Epoch 44/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3941 - sparse_categorical_crossentropy: 0.3941 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "Epoch 45/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3931 - sparse_categorical_crossentropy: 0.3931 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 46/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3934 - sparse_categorical_crossentropy: 0.3934 - val_loss: 0.3798 - val_sparse_categorical_crossentropy: 0.3798\n",
            "Epoch 47/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3905 - sparse_categorical_crossentropy: 0.3905 - val_loss: 0.3808 - val_sparse_categorical_crossentropy: 0.3808\n",
            "Epoch 48/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3916 - sparse_categorical_crossentropy: 0.3916 - val_loss: 0.3782 - val_sparse_categorical_crossentropy: 0.3782\n",
            "Epoch 49/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3905 - sparse_categorical_crossentropy: 0.3905 - val_loss: 0.3799 - val_sparse_categorical_crossentropy: 0.3799\n",
            "Epoch 50/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3907 - sparse_categorical_crossentropy: 0.3907 - val_loss: 0.3804 - val_sparse_categorical_crossentropy: 0.3804\n",
            "Epoch 51/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3889 - sparse_categorical_crossentropy: 0.3889 - val_loss: 0.3813 - val_sparse_categorical_crossentropy: 0.3813\n",
            "Epoch 52/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3887 - sparse_categorical_crossentropy: 0.3887 - val_loss: 0.3777 - val_sparse_categorical_crossentropy: 0.3777\n",
            "Epoch 53/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3880 - sparse_categorical_crossentropy: 0.3880 - val_loss: 0.3768 - val_sparse_categorical_crossentropy: 0.3768\n",
            "Epoch 54/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3883 - sparse_categorical_crossentropy: 0.3883 - val_loss: 0.3745 - val_sparse_categorical_crossentropy: 0.3745\n",
            "Epoch 55/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3864 - sparse_categorical_crossentropy: 0.3864 - val_loss: 0.3758 - val_sparse_categorical_crossentropy: 0.3758\n",
            "Epoch 56/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3868 - sparse_categorical_crossentropy: 0.3868 - val_loss: 0.3824 - val_sparse_categorical_crossentropy: 0.3824\n",
            "Epoch 57/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3852 - sparse_categorical_crossentropy: 0.3852 - val_loss: 0.3742 - val_sparse_categorical_crossentropy: 0.3742\n",
            "Epoch 58/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3833 - sparse_categorical_crossentropy: 0.3833 - val_loss: 0.3780 - val_sparse_categorical_crossentropy: 0.3780\n",
            "Epoch 59/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3852 - sparse_categorical_crossentropy: 0.3852 - val_loss: 0.3808 - val_sparse_categorical_crossentropy: 0.3808\n",
            "Epoch 60/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3854 - sparse_categorical_crossentropy: 0.3854 - val_loss: 0.3768 - val_sparse_categorical_crossentropy: 0.3768\n",
            "Epoch 61/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3848 - sparse_categorical_crossentropy: 0.3848 - val_loss: 0.3763 - val_sparse_categorical_crossentropy: 0.3763\n",
            "Epoch 62/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3833 - sparse_categorical_crossentropy: 0.3833 - val_loss: 0.3795 - val_sparse_categorical_crossentropy: 0.3795\n",
            "Epoch 63/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3829 - sparse_categorical_crossentropy: 0.3829 - val_loss: 0.3757 - val_sparse_categorical_crossentropy: 0.3757\n",
            "Epoch 64/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3820 - sparse_categorical_crossentropy: 0.3820 - val_loss: 0.3752 - val_sparse_categorical_crossentropy: 0.3752\n",
            "Epoch 65/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3817 - sparse_categorical_crossentropy: 0.3817 - val_loss: 0.3803 - val_sparse_categorical_crossentropy: 0.3803\n",
            "Epoch 66/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3798 - sparse_categorical_crossentropy: 0.3798 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 67/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3814 - sparse_categorical_crossentropy: 0.3814 - val_loss: 0.3865 - val_sparse_categorical_crossentropy: 0.3865\n",
            "Epoch 68/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3810 - sparse_categorical_crossentropy: 0.3810 - val_loss: 0.3779 - val_sparse_categorical_crossentropy: 0.3779\n",
            "Epoch 69/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3813 - sparse_categorical_crossentropy: 0.3813 - val_loss: 0.3766 - val_sparse_categorical_crossentropy: 0.3766\n",
            "Epoch 70/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3786 - sparse_categorical_crossentropy: 0.3786 - val_loss: 0.3700 - val_sparse_categorical_crossentropy: 0.3700\n",
            "Epoch 71/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3801 - sparse_categorical_crossentropy: 0.3801 - val_loss: 0.3766 - val_sparse_categorical_crossentropy: 0.3766\n",
            "Epoch 72/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3794 - sparse_categorical_crossentropy: 0.3794 - val_loss: 0.3710 - val_sparse_categorical_crossentropy: 0.3710\n",
            "Epoch 73/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3794 - sparse_categorical_crossentropy: 0.3794 - val_loss: 0.3728 - val_sparse_categorical_crossentropy: 0.3728\n",
            "Epoch 74/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3774 - sparse_categorical_crossentropy: 0.3774 - val_loss: 0.3732 - val_sparse_categorical_crossentropy: 0.3732\n",
            "Epoch 75/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.3772 - sparse_categorical_crossentropy: 0.3772 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 76/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3770 - sparse_categorical_crossentropy: 0.3770 - val_loss: 0.3767 - val_sparse_categorical_crossentropy: 0.3767\n",
            "Epoch 77/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3768 - sparse_categorical_crossentropy: 0.3768 - val_loss: 0.3775 - val_sparse_categorical_crossentropy: 0.3775\n",
            "Epoch 78/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.3758 - sparse_categorical_crossentropy: 0.3758 - val_loss: 0.3741 - val_sparse_categorical_crossentropy: 0.3741\n",
            "Epoch 79/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3749 - sparse_categorical_crossentropy: 0.3749 - val_loss: 0.3735 - val_sparse_categorical_crossentropy: 0.3735\n",
            "Epoch 80/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3755 - sparse_categorical_crossentropy: 0.3755 - val_loss: 0.3725 - val_sparse_categorical_crossentropy: 0.3725\n",
            "Epoch 81/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3756 - sparse_categorical_crossentropy: 0.3756 - val_loss: 0.3765 - val_sparse_categorical_crossentropy: 0.3765\n",
            "Epoch 82/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3735 - sparse_categorical_crossentropy: 0.3735 - val_loss: 0.3736 - val_sparse_categorical_crossentropy: 0.3736\n",
            "Epoch 83/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3739 - sparse_categorical_crossentropy: 0.3739 - val_loss: 0.3726 - val_sparse_categorical_crossentropy: 0.3726\n",
            "Epoch 84/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3737 - sparse_categorical_crossentropy: 0.3737 - val_loss: 0.3720 - val_sparse_categorical_crossentropy: 0.3720\n",
            "Epoch 85/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3748 - sparse_categorical_crossentropy: 0.3748 - val_loss: 0.3679 - val_sparse_categorical_crossentropy: 0.3679\n",
            "Epoch 86/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3723 - sparse_categorical_crossentropy: 0.3723 - val_loss: 0.3719 - val_sparse_categorical_crossentropy: 0.3719\n",
            "Epoch 87/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3735 - sparse_categorical_crossentropy: 0.3735 - val_loss: 0.3724 - val_sparse_categorical_crossentropy: 0.3724\n",
            "Epoch 88/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3719 - sparse_categorical_crossentropy: 0.3719 - val_loss: 0.3696 - val_sparse_categorical_crossentropy: 0.3696\n",
            "Epoch 89/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3722 - sparse_categorical_crossentropy: 0.3722 - val_loss: 0.3773 - val_sparse_categorical_crossentropy: 0.3773\n",
            "Epoch 90/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3707 - sparse_categorical_crossentropy: 0.3707 - val_loss: 0.3678 - val_sparse_categorical_crossentropy: 0.3678\n",
            "Epoch 91/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3716 - sparse_categorical_crossentropy: 0.3716 - val_loss: 0.3697 - val_sparse_categorical_crossentropy: 0.3697\n",
            "Epoch 92/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.3707 - sparse_categorical_crossentropy: 0.3707 - val_loss: 0.3672 - val_sparse_categorical_crossentropy: 0.3672\n",
            "Epoch 93/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3720 - sparse_categorical_crossentropy: 0.3720 - val_loss: 0.3718 - val_sparse_categorical_crossentropy: 0.3718\n",
            "Epoch 94/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3709 - sparse_categorical_crossentropy: 0.3709 - val_loss: 0.3677 - val_sparse_categorical_crossentropy: 0.3677\n",
            "Epoch 95/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3693 - sparse_categorical_crossentropy: 0.3693 - val_loss: 0.3702 - val_sparse_categorical_crossentropy: 0.3702\n",
            "Epoch 96/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3702 - sparse_categorical_crossentropy: 0.3702 - val_loss: 0.3731 - val_sparse_categorical_crossentropy: 0.3731\n",
            "Epoch 97/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3694 - sparse_categorical_crossentropy: 0.3694 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 98/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3685 - sparse_categorical_crossentropy: 0.3685 - val_loss: 0.3700 - val_sparse_categorical_crossentropy: 0.3700\n",
            "Epoch 99/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3696 - sparse_categorical_crossentropy: 0.3696 - val_loss: 0.3718 - val_sparse_categorical_crossentropy: 0.3718\n",
            "Epoch 100/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3693 - sparse_categorical_crossentropy: 0.3693 - val_loss: 0.3759 - val_sparse_categorical_crossentropy: 0.3759\n",
            "Epoch 101/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3686 - sparse_categorical_crossentropy: 0.3686 - val_loss: 0.3707 - val_sparse_categorical_crossentropy: 0.3707\n",
            "Epoch 102/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3672 - sparse_categorical_crossentropy: 0.3672 - val_loss: 0.3761 - val_sparse_categorical_crossentropy: 0.3761\n",
            "Epoch 103/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3695 - sparse_categorical_crossentropy: 0.3695 - val_loss: 0.3742 - val_sparse_categorical_crossentropy: 0.3742\n",
            "Epoch 104/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.3663 - sparse_categorical_crossentropy: 0.3663 - val_loss: 0.3734 - val_sparse_categorical_crossentropy: 0.3734\n",
            "Epoch 105/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3672 - sparse_categorical_crossentropy: 0.3672 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 106/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3671 - sparse_categorical_crossentropy: 0.3671 - val_loss: 0.3703 - val_sparse_categorical_crossentropy: 0.3703\n",
            "Epoch 107/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3668 - sparse_categorical_crossentropy: 0.3668 - val_loss: 0.3689 - val_sparse_categorical_crossentropy: 0.3689\n",
            "Epoch 108/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3665 - sparse_categorical_crossentropy: 0.3665 - val_loss: 0.3702 - val_sparse_categorical_crossentropy: 0.3702\n",
            "Epoch 109/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3677 - sparse_categorical_crossentropy: 0.3677 - val_loss: 0.3677 - val_sparse_categorical_crossentropy: 0.3677\n",
            "Epoch 110/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3655 - sparse_categorical_crossentropy: 0.3655 - val_loss: 0.3744 - val_sparse_categorical_crossentropy: 0.3744\n",
            "Epoch 111/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3683 - sparse_categorical_crossentropy: 0.3683 - val_loss: 0.3707 - val_sparse_categorical_crossentropy: 0.3707\n",
            "Epoch 112/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3653 - sparse_categorical_crossentropy: 0.3653 - val_loss: 0.3686 - val_sparse_categorical_crossentropy: 0.3686\n",
            "Epoch 113/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3641 - sparse_categorical_crossentropy: 0.3641 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 114/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3635 - sparse_categorical_crossentropy: 0.3635 - val_loss: 0.3697 - val_sparse_categorical_crossentropy: 0.3697\n",
            "Epoch 115/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3650 - sparse_categorical_crossentropy: 0.3650 - val_loss: 0.3705 - val_sparse_categorical_crossentropy: 0.3705\n",
            "Epoch 116/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3651 - sparse_categorical_crossentropy: 0.3651 - val_loss: 0.3691 - val_sparse_categorical_crossentropy: 0.3691\n",
            "Epoch 117/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.3651 - sparse_categorical_crossentropy: 0.3651 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 118/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3664 - sparse_categorical_crossentropy: 0.3664 - val_loss: 0.3797 - val_sparse_categorical_crossentropy: 0.3797\n",
            "Epoch 119/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3640 - sparse_categorical_crossentropy: 0.3640 - val_loss: 0.3669 - val_sparse_categorical_crossentropy: 0.3669\n",
            "Epoch 120/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3636 - sparse_categorical_crossentropy: 0.3636 - val_loss: 0.3705 - val_sparse_categorical_crossentropy: 0.3705\n",
            "Epoch 121/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3652 - sparse_categorical_crossentropy: 0.3652 - val_loss: 0.3751 - val_sparse_categorical_crossentropy: 0.3751\n",
            "Epoch 122/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3637 - sparse_categorical_crossentropy: 0.3637 - val_loss: 0.3751 - val_sparse_categorical_crossentropy: 0.3751\n",
            "Epoch 123/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3631 - sparse_categorical_crossentropy: 0.3631 - val_loss: 0.3760 - val_sparse_categorical_crossentropy: 0.3760\n",
            "Epoch 124/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3639 - sparse_categorical_crossentropy: 0.3639 - val_loss: 0.3687 - val_sparse_categorical_crossentropy: 0.3687\n",
            "Epoch 125/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3627 - sparse_categorical_crossentropy: 0.3627 - val_loss: 0.3672 - val_sparse_categorical_crossentropy: 0.3672\n",
            "Epoch 126/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3608 - sparse_categorical_crossentropy: 0.3608 - val_loss: 0.3721 - val_sparse_categorical_crossentropy: 0.3721\n",
            "Epoch 127/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3615 - sparse_categorical_crossentropy: 0.3615 - val_loss: 0.3699 - val_sparse_categorical_crossentropy: 0.3699\n",
            "Epoch 128/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3625 - sparse_categorical_crossentropy: 0.3625 - val_loss: 0.3683 - val_sparse_categorical_crossentropy: 0.3683\n",
            "Epoch 129/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3624 - sparse_categorical_crossentropy: 0.3624 - val_loss: 0.3707 - val_sparse_categorical_crossentropy: 0.3707\n",
            "Epoch 130/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3602 - sparse_categorical_crossentropy: 0.3602 - val_loss: 0.3688 - val_sparse_categorical_crossentropy: 0.3688\n",
            "Epoch 131/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3608 - sparse_categorical_crossentropy: 0.3608 - val_loss: 0.3727 - val_sparse_categorical_crossentropy: 0.3727\n",
            "Epoch 132/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3614 - sparse_categorical_crossentropy: 0.3614 - val_loss: 0.3724 - val_sparse_categorical_crossentropy: 0.3724\n",
            "Epoch 133/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.3608 - sparse_categorical_crossentropy: 0.3608 - val_loss: 0.3697 - val_sparse_categorical_crossentropy: 0.3697\n",
            "Epoch 134/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3589 - sparse_categorical_crossentropy: 0.3589 - val_loss: 0.3677 - val_sparse_categorical_crossentropy: 0.3677\n",
            "Epoch 135/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3613 - sparse_categorical_crossentropy: 0.3613 - val_loss: 0.3744 - val_sparse_categorical_crossentropy: 0.3744\n",
            "Epoch 136/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3602 - sparse_categorical_crossentropy: 0.3602 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 137/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3591 - sparse_categorical_crossentropy: 0.3591 - val_loss: 0.3689 - val_sparse_categorical_crossentropy: 0.3689\n",
            "Epoch 138/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3596 - sparse_categorical_crossentropy: 0.3596 - val_loss: 0.3710 - val_sparse_categorical_crossentropy: 0.3710\n",
            "\n",
            "Epoch 00138: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.\n",
            "Epoch 139/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3553 - sparse_categorical_crossentropy: 0.3553 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 140/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3553 - sparse_categorical_crossentropy: 0.3553 - val_loss: 0.3710 - val_sparse_categorical_crossentropy: 0.3710\n",
            "Epoch 141/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3551 - sparse_categorical_crossentropy: 0.3551 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 142/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3545 - sparse_categorical_crossentropy: 0.3545 - val_loss: 0.3661 - val_sparse_categorical_crossentropy: 0.3661\n",
            "Epoch 143/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3529 - sparse_categorical_crossentropy: 0.3529 - val_loss: 0.3664 - val_sparse_categorical_crossentropy: 0.3664\n",
            "Epoch 144/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3533 - sparse_categorical_crossentropy: 0.3533 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 145/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3533 - sparse_categorical_crossentropy: 0.3533 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "Epoch 146/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3533 - sparse_categorical_crossentropy: 0.3533 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 147/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3535 - sparse_categorical_crossentropy: 0.3535 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 148/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3514 - sparse_categorical_crossentropy: 0.3514 - val_loss: 0.3664 - val_sparse_categorical_crossentropy: 0.3664\n",
            "Epoch 149/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.3525 - sparse_categorical_crossentropy: 0.3525 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 150/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3520 - sparse_categorical_crossentropy: 0.3520 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 151/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3516 - sparse_categorical_crossentropy: 0.3516 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 152/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3528 - sparse_categorical_crossentropy: 0.3528 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 153/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3525 - sparse_categorical_crossentropy: 0.3525 - val_loss: 0.3651 - val_sparse_categorical_crossentropy: 0.3651\n",
            "Epoch 154/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3522 - sparse_categorical_crossentropy: 0.3522 - val_loss: 0.3653 - val_sparse_categorical_crossentropy: 0.3653\n",
            "Epoch 155/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3520 - sparse_categorical_crossentropy: 0.3520 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 156/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3521 - sparse_categorical_crossentropy: 0.3521 - val_loss: 0.3686 - val_sparse_categorical_crossentropy: 0.3686\n",
            "Epoch 157/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3523 - sparse_categorical_crossentropy: 0.3523 - val_loss: 0.3665 - val_sparse_categorical_crossentropy: 0.3665\n",
            "Epoch 158/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3506 - sparse_categorical_crossentropy: 0.3506 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 159/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3509 - sparse_categorical_crossentropy: 0.3509 - val_loss: 0.3668 - val_sparse_categorical_crossentropy: 0.3668\n",
            "Epoch 160/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3515 - sparse_categorical_crossentropy: 0.3515 - val_loss: 0.3678 - val_sparse_categorical_crossentropy: 0.3678\n",
            "Epoch 161/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.3507 - sparse_categorical_crossentropy: 0.3507 - val_loss: 0.3667 - val_sparse_categorical_crossentropy: 0.3667\n",
            "Epoch 162/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3502 - sparse_categorical_crossentropy: 0.3502 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 163/800\n",
            "184521/184521 [==============================] - 7s 36us/step - loss: 0.3506 - sparse_categorical_crossentropy: 0.3506 - val_loss: 0.3703 - val_sparse_categorical_crossentropy: 0.3703\n",
            "Epoch 164/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3507 - sparse_categorical_crossentropy: 0.3507 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 165/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3505 - sparse_categorical_crossentropy: 0.3505 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Epoch 166/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3515 - sparse_categorical_crossentropy: 0.3515 - val_loss: 0.3670 - val_sparse_categorical_crossentropy: 0.3670\n",
            "Epoch 167/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3508 - sparse_categorical_crossentropy: 0.3508 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 168/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3495 - sparse_categorical_crossentropy: 0.3495 - val_loss: 0.3674 - val_sparse_categorical_crossentropy: 0.3674\n",
            "Epoch 169/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3510 - sparse_categorical_crossentropy: 0.3510 - val_loss: 0.3668 - val_sparse_categorical_crossentropy: 0.3668\n",
            "Epoch 170/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3494 - sparse_categorical_crossentropy: 0.3494 - val_loss: 0.3673 - val_sparse_categorical_crossentropy: 0.3673\n",
            "Epoch 171/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3502 - sparse_categorical_crossentropy: 0.3502 - val_loss: 0.3663 - val_sparse_categorical_crossentropy: 0.3663\n",
            "Epoch 172/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3497 - sparse_categorical_crossentropy: 0.3497 - val_loss: 0.3665 - val_sparse_categorical_crossentropy: 0.3665\n",
            "Epoch 173/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3495 - sparse_categorical_crossentropy: 0.3495 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 174/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3497 - sparse_categorical_crossentropy: 0.3497 - val_loss: 0.3668 - val_sparse_categorical_crossentropy: 0.3668\n",
            "Epoch 175/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3496 - sparse_categorical_crossentropy: 0.3496 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 176/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3498 - sparse_categorical_crossentropy: 0.3498 - val_loss: 0.3698 - val_sparse_categorical_crossentropy: 0.3698\n",
            "Epoch 177/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3490 - sparse_categorical_crossentropy: 0.3490 - val_loss: 0.3661 - val_sparse_categorical_crossentropy: 0.3661\n",
            "Epoch 178/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3493 - sparse_categorical_crossentropy: 0.3493 - val_loss: 0.3649 - val_sparse_categorical_crossentropy: 0.3649\n",
            "Epoch 179/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3486 - sparse_categorical_crossentropy: 0.3486 - val_loss: 0.3662 - val_sparse_categorical_crossentropy: 0.3662\n",
            "Epoch 180/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3499 - sparse_categorical_crossentropy: 0.3499 - val_loss: 0.3670 - val_sparse_categorical_crossentropy: 0.3670\n",
            "Epoch 181/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3493 - sparse_categorical_crossentropy: 0.3493 - val_loss: 0.3676 - val_sparse_categorical_crossentropy: 0.3676\n",
            "Epoch 182/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3487 - sparse_categorical_crossentropy: 0.3487 - val_loss: 0.3661 - val_sparse_categorical_crossentropy: 0.3661\n",
            "Epoch 183/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3494 - sparse_categorical_crossentropy: 0.3494 - val_loss: 0.3669 - val_sparse_categorical_crossentropy: 0.3669\n",
            "Epoch 184/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3496 - sparse_categorical_crossentropy: 0.3496 - val_loss: 0.3666 - val_sparse_categorical_crossentropy: 0.3666\n",
            "Epoch 185/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3482 - sparse_categorical_crossentropy: 0.3482 - val_loss: 0.3687 - val_sparse_categorical_crossentropy: 0.3687\n",
            "Epoch 186/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3500 - sparse_categorical_crossentropy: 0.3500 - val_loss: 0.3678 - val_sparse_categorical_crossentropy: 0.3678\n",
            "Epoch 187/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3486 - sparse_categorical_crossentropy: 0.3486 - val_loss: 0.3672 - val_sparse_categorical_crossentropy: 0.3672\n",
            "Epoch 188/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3483 - sparse_categorical_crossentropy: 0.3483 - val_loss: 0.3703 - val_sparse_categorical_crossentropy: 0.3703\n",
            "Epoch 189/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3476 - sparse_categorical_crossentropy: 0.3476 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 190/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3475 - sparse_categorical_crossentropy: 0.3475 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 191/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3477 - sparse_categorical_crossentropy: 0.3477 - val_loss: 0.3664 - val_sparse_categorical_crossentropy: 0.3664\n",
            "Epoch 192/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3475 - sparse_categorical_crossentropy: 0.3475 - val_loss: 0.3663 - val_sparse_categorical_crossentropy: 0.3663\n",
            "Epoch 193/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3489 - sparse_categorical_crossentropy: 0.3489 - val_loss: 0.3699 - val_sparse_categorical_crossentropy: 0.3699\n",
            "Epoch 194/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3481 - sparse_categorical_crossentropy: 0.3481 - val_loss: 0.3663 - val_sparse_categorical_crossentropy: 0.3663\n",
            "Epoch 195/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3477 - sparse_categorical_crossentropy: 0.3477 - val_loss: 0.3672 - val_sparse_categorical_crossentropy: 0.3672\n",
            "Epoch 196/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3473 - sparse_categorical_crossentropy: 0.3473 - val_loss: 0.3648 - val_sparse_categorical_crossentropy: 0.3648\n",
            "Epoch 197/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3484 - sparse_categorical_crossentropy: 0.3484 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Epoch 198/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3480 - sparse_categorical_crossentropy: 0.3480 - val_loss: 0.3671 - val_sparse_categorical_crossentropy: 0.3671\n",
            "Epoch 199/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3481 - sparse_categorical_crossentropy: 0.3481 - val_loss: 0.3681 - val_sparse_categorical_crossentropy: 0.3681\n",
            "Epoch 200/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3471 - sparse_categorical_crossentropy: 0.3471 - val_loss: 0.3666 - val_sparse_categorical_crossentropy: 0.3666\n",
            "Epoch 201/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3462 - sparse_categorical_crossentropy: 0.3462 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 202/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3475 - sparse_categorical_crossentropy: 0.3475 - val_loss: 0.3681 - val_sparse_categorical_crossentropy: 0.3681\n",
            "Epoch 203/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3467 - sparse_categorical_crossentropy: 0.3467 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "\n",
            "Epoch 00203: ReduceLROnPlateau reducing learning rate to 9.999999259090306e-06.\n",
            "Epoch 204/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3467 - sparse_categorical_crossentropy: 0.3467 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 205/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3459 - sparse_categorical_crossentropy: 0.3459 - val_loss: 0.3652 - val_sparse_categorical_crossentropy: 0.3652\n",
            "Epoch 206/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3461 - sparse_categorical_crossentropy: 0.3461 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 207/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3454 - sparse_categorical_crossentropy: 0.3454 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 208/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3446 - sparse_categorical_crossentropy: 0.3446 - val_loss: 0.3667 - val_sparse_categorical_crossentropy: 0.3667\n",
            "Epoch 209/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3443 - sparse_categorical_crossentropy: 0.3443 - val_loss: 0.3654 - val_sparse_categorical_crossentropy: 0.3654\n",
            "Epoch 210/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3437 - sparse_categorical_crossentropy: 0.3437 - val_loss: 0.3661 - val_sparse_categorical_crossentropy: 0.3661\n",
            "Epoch 211/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3458 - sparse_categorical_crossentropy: 0.3458 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 212/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3438 - sparse_categorical_crossentropy: 0.3438 - val_loss: 0.3663 - val_sparse_categorical_crossentropy: 0.3663\n",
            "Epoch 213/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3446 - sparse_categorical_crossentropy: 0.3446 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 214/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3452 - sparse_categorical_crossentropy: 0.3452 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 215/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3448 - sparse_categorical_crossentropy: 0.3448 - val_loss: 0.3666 - val_sparse_categorical_crossentropy: 0.3666\n",
            "Epoch 216/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3459 - sparse_categorical_crossentropy: 0.3459 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 217/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3462 - sparse_categorical_crossentropy: 0.3462 - val_loss: 0.3665 - val_sparse_categorical_crossentropy: 0.3665\n",
            "Epoch 218/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3454 - sparse_categorical_crossentropy: 0.3454 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 219/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3458 - sparse_categorical_crossentropy: 0.3458 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 220/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3453 - sparse_categorical_crossentropy: 0.3453 - val_loss: 0.3661 - val_sparse_categorical_crossentropy: 0.3661\n",
            "Epoch 221/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3450 - sparse_categorical_crossentropy: 0.3450 - val_loss: 0.3665 - val_sparse_categorical_crossentropy: 0.3665\n",
            "Epoch 222/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3445 - sparse_categorical_crossentropy: 0.3445 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 223/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3447 - sparse_categorical_crossentropy: 0.3447 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 224/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3452 - sparse_categorical_crossentropy: 0.3452 - val_loss: 0.3663 - val_sparse_categorical_crossentropy: 0.3663\n",
            "Epoch 225/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3455 - sparse_categorical_crossentropy: 0.3455 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 226/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3443 - sparse_categorical_crossentropy: 0.3443 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 227/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3455 - sparse_categorical_crossentropy: 0.3455 - val_loss: 0.3663 - val_sparse_categorical_crossentropy: 0.3663\n",
            "Epoch 228/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3454 - sparse_categorical_crossentropy: 0.3454 - val_loss: 0.3665 - val_sparse_categorical_crossentropy: 0.3665\n",
            "\n",
            "Epoch 00228: ReduceLROnPlateau reducing learning rate to 3.162277292675049e-06.\n",
            "Epoch 229/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3444 - sparse_categorical_crossentropy: 0.3444 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 230/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3437 - sparse_categorical_crossentropy: 0.3437 - val_loss: 0.3661 - val_sparse_categorical_crossentropy: 0.3661\n",
            "Epoch 231/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3438 - sparse_categorical_crossentropy: 0.3438 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 232/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3438 - sparse_categorical_crossentropy: 0.3438 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 233/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3458 - sparse_categorical_crossentropy: 0.3458 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 234/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3447 - sparse_categorical_crossentropy: 0.3447 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 235/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3444 - sparse_categorical_crossentropy: 0.3444 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 236/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3455 - sparse_categorical_crossentropy: 0.3455 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 237/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3432 - sparse_categorical_crossentropy: 0.3432 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 238/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3440 - sparse_categorical_crossentropy: 0.3440 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 239/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3429 - sparse_categorical_crossentropy: 0.3429 - val_loss: 0.3664 - val_sparse_categorical_crossentropy: 0.3664\n",
            "Epoch 240/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3441 - sparse_categorical_crossentropy: 0.3441 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 241/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3429 - sparse_categorical_crossentropy: 0.3429 - val_loss: 0.3661 - val_sparse_categorical_crossentropy: 0.3661\n",
            "Epoch 242/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3428 - sparse_categorical_crossentropy: 0.3428 - val_loss: 0.3661 - val_sparse_categorical_crossentropy: 0.3661\n",
            "Epoch 243/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3442 - sparse_categorical_crossentropy: 0.3442 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 244/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3431 - sparse_categorical_crossentropy: 0.3431 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 245/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3438 - sparse_categorical_crossentropy: 0.3438 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 246/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3435 - sparse_categorical_crossentropy: 0.3435 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 247/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3446 - sparse_categorical_crossentropy: 0.3446 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 248/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3444 - sparse_categorical_crossentropy: 0.3444 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 249/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3427 - sparse_categorical_crossentropy: 0.3427 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 250/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3446 - sparse_categorical_crossentropy: 0.3446 - val_loss: 0.3667 - val_sparse_categorical_crossentropy: 0.3667\n",
            "Epoch 251/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3432 - sparse_categorical_crossentropy: 0.3432 - val_loss: 0.3664 - val_sparse_categorical_crossentropy: 0.3664\n",
            "Epoch 252/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3433 - sparse_categorical_crossentropy: 0.3433 - val_loss: 0.3663 - val_sparse_categorical_crossentropy: 0.3663\n",
            "Epoch 253/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3446 - sparse_categorical_crossentropy: 0.3446 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "\n",
            "Epoch 00253: ReduceLROnPlateau reducing learning rate to 9.999999115286567e-07.\n",
            "Epoch 254/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3438 - sparse_categorical_crossentropy: 0.3438 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 255/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3442 - sparse_categorical_crossentropy: 0.3442 - val_loss: 0.3661 - val_sparse_categorical_crossentropy: 0.3661\n",
            "Epoch 256/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3437 - sparse_categorical_crossentropy: 0.3437 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 257/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3434 - sparse_categorical_crossentropy: 0.3434 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 258/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3433 - sparse_categorical_crossentropy: 0.3433 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 259/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3440 - sparse_categorical_crossentropy: 0.3440 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 260/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3439 - sparse_categorical_crossentropy: 0.3439 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 261/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3419 - sparse_categorical_crossentropy: 0.3419 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 262/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3437 - sparse_categorical_crossentropy: 0.3437 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 263/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3434 - sparse_categorical_crossentropy: 0.3434 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 264/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3424 - sparse_categorical_crossentropy: 0.3424 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 265/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3442 - sparse_categorical_crossentropy: 0.3442 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 266/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3437 - sparse_categorical_crossentropy: 0.3437 - val_loss: 0.3655 - val_sparse_categorical_crossentropy: 0.3655\n",
            "Epoch 267/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3437 - sparse_categorical_crossentropy: 0.3437 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 268/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3434 - sparse_categorical_crossentropy: 0.3434 - val_loss: 0.3659 - val_sparse_categorical_crossentropy: 0.3659\n",
            "Epoch 269/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3442 - sparse_categorical_crossentropy: 0.3442 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 270/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3435 - sparse_categorical_crossentropy: 0.3435 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 271/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3441 - sparse_categorical_crossentropy: 0.3441 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 272/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3424 - sparse_categorical_crossentropy: 0.3424 - val_loss: 0.3657 - val_sparse_categorical_crossentropy: 0.3657\n",
            "Epoch 273/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3443 - sparse_categorical_crossentropy: 0.3443 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Epoch 274/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3421 - sparse_categorical_crossentropy: 0.3421 - val_loss: 0.3656 - val_sparse_categorical_crossentropy: 0.3656\n",
            "Epoch 275/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3435 - sparse_categorical_crossentropy: 0.3435 - val_loss: 0.3660 - val_sparse_categorical_crossentropy: 0.3660\n",
            "Epoch 276/800\n",
            "184521/184521 [==============================] - 7s 37us/step - loss: 0.3433 - sparse_categorical_crossentropy: 0.3433 - val_loss: 0.3658 - val_sparse_categorical_crossentropy: 0.3658\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00276: early stopping\n",
            "모델저장완료\n",
            "==================================================\n",
            "Loss와 ACC에 대한 Plot을 그립니다\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAETCAYAAABnSkJLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niAGXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "기존nn모델의 train loss를 출력합니다\n",
            "train, loss and metric: [0.3196354617643328, 0.3196354617643328]\n",
            "기존nn모델의 valid loss를 출력합니다\n",
            "valid, loss and metric: [0.3648399589329506, 0.3648399589329506]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5oJjBcLFzzL",
        "colab_type": "code",
        "outputId": "cdecd1ef-f53d-412d-b00f-6d1721b762ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "savingpath_csv = 'drive/My Drive/데이콘_천체유형/파일제출/' + csv_folder + '/이상치1_layer수_배치크기_조정_initial_rate_%s.csv' % initial_rate\n",
        "\n",
        "print('best_n_epoch는 다음과 같습니다')\n",
        "print(np.argmin(hist.history['val_loss']))\n",
        "print('='*50)\n",
        "print('final rate는 다음과 같습니다')\n",
        "print(hist.history['lr'][np.argmin(hist.history['val_loss'])])\n",
        "print('='*25)\n",
        "y_pred = nn_model.predict(test_x)\n",
        "submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n",
        "submission.to_csv(savingpath_csv, index=True)\n",
        "print('csv 저장완료')\n",
        "\n",
        "print('='*50)\n",
        "best_val_loss = np.min(hist.history['val_loss'])\n",
        "print(\"best_valid_loss: {}\".format(best_val_loss))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best_n_epoch는 다음과 같습니다\n",
            "195\n",
            "==================================================\n",
            "final rate는 다음과 같습니다\n",
            "3.1622774e-05\n",
            "=========================\n",
            "csv 저장완료\n",
            "==================================================\n",
            "best_valid_loss: 0.3648399589329506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7UR0QNdWEF9",
        "colab_type": "text"
      },
      "source": [
        "# 저장된 모델 로드해오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1W33dlgNXSTo",
        "outputId": "9084d431-0d8c-4358-bfdd-77be9e884eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "initial_rate = 1e-4\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "\n",
        "## model load\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "from keras.models import model_from_json\n",
        "json_file = open(json_path, \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "## model weight load\n",
        "loaded_model.load_weights(weight_path)\n",
        "\n",
        "## model load and evaluation\n",
        "from keras import optimizers\n",
        "final_rate = hist.history['lr'][np.argmin(hist.history['val_loss'])]\n",
        "print(\"Loaded model from disk\")\n",
        "load_optimizer = optimizers.Adam(\n",
        "    lr=final_rate,\n",
        ")\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "loaded_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=load_optimizer\n",
        "                     ,metrics=[CCE]\n",
        "                     )\n",
        "batchsize=1024\n",
        "train_score = loaded_model.evaluate(train_input,train_target,verbose=0, batch_size=batchsize)\n",
        "print('Load한 Check모델의 train loss를 출력합니다')\n",
        "print(\"train, loss and metric: {}\".format(train_score))\n",
        "cv_score = loaded_model.evaluate(cv_input,cv_target,verbose=0, batch_size=batchsize)\n",
        "print('Load한 Check모델의 val loss를 출력합니다')\n",
        "print(\"valid, loss and metric: {}\".format(cv_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "Load한 Check모델의 train loss를 출력합니다\n",
            "train, loss and metric: [0.3077159990481829, 0.3077159990481829]\n",
            "Load한 Check모델의 val loss를 출력합니다\n",
            "valid, loss and metric: [0.3495782041837298, 0.3495782041837298]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNI-3z8BzMAo",
        "colab_type": "code",
        "outputId": "7bbf4e55-17fd-40eb-9216-7aa424129587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "csv_folder = ''\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "initial_rate = 0.01\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "\n",
        "## model load\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "from keras.models import model_from_json\n",
        "json_file = open(json_path, \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "## model weight load\n",
        "loaded_model.load_weights(weight_path)\n",
        "\n",
        "## model load and evaluation\n",
        "from keras import optimizers\n",
        "#final_rate = hist.history['lr'][np.argmin(hist.history['val_loss'])]\n",
        "print(\"Loaded model from disk\")\n",
        "load_optimizer = optimizers.Adam(\n",
        "    #lr=final_rate\n",
        "    lr = 0.0003162278\n",
        ")\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "loaded_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=load_optimizer\n",
        "                     ,metrics=[CCE]\n",
        "                     )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKAM00J7zwTN",
        "colab_type": "code",
        "outputId": "189ebe48-d72e-456b-81a3-7963ea664198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        }
      },
      "source": [
        "t = 1\n",
        "for layer in loaded_model.layers:\n",
        "  print('='*25 + '{0}번째'.format(t) + '='*25)\n",
        "  weights = layer.get_weights()\n",
        "  print(  (np.array(weights) ).shape )\n",
        "  t += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================1번째=========================\n",
            "(0,)\n",
            "=========================2번째=========================\n",
            "(2,)\n",
            "=========================3번째=========================\n",
            "(4, 2373)\n",
            "=========================4번째=========================\n",
            "(0,)\n",
            "=========================5번째=========================\n",
            "(0,)\n",
            "=========================6번째=========================\n",
            "(2,)\n",
            "=========================7번째=========================\n",
            "(4, 2355)\n",
            "=========================8번째=========================\n",
            "(0,)\n",
            "=========================9번째=========================\n",
            "(0,)\n",
            "=========================10번째=========================\n",
            "(2,)\n",
            "=========================11번째=========================\n",
            "(4, 1197)\n",
            "=========================12번째=========================\n",
            "(0,)\n",
            "=========================13번째=========================\n",
            "(0,)\n",
            "=========================14번째=========================\n",
            "(2,)\n",
            "=========================15번째=========================\n",
            "(4, 1187)\n",
            "=========================16번째=========================\n",
            "(0,)\n",
            "=========================17번째=========================\n",
            "(0,)\n",
            "=========================18번째=========================\n",
            "(2,)\n",
            "=========================19번째=========================\n",
            "(4, 612)\n",
            "=========================20번째=========================\n",
            "(0,)\n",
            "=========================21번째=========================\n",
            "(0,)\n",
            "=========================22번째=========================\n",
            "(2,)\n",
            "=========================23번째=========================\n",
            "(4, 602)\n",
            "=========================24번째=========================\n",
            "(0,)\n",
            "=========================25번째=========================\n",
            "(0,)\n",
            "=========================26번째=========================\n",
            "(2,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}