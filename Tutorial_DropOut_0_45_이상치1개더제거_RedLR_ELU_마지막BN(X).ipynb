{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_DropOut_0.45_이상치1개더제거_RedLR_ELU_마지막BN(X).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "d4h9A4H3E_XB"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoonJaeHoon/Dacon_Sloan-Digital-Sky-Survey_SDSS/blob/master/Tutorial_DropOut_0_45_%EC%9D%B4%EC%83%81%EC%B9%981%EA%B0%9C%EB%8D%94%EC%A0%9C%EA%B1%B0_RedLR_ELU_%EB%A7%88%EC%A7%80%EB%A7%89BN(X).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkIsyTZNAJ2P",
        "colab_type": "code",
        "outputId": "278c4cbb-87d9-45cb-8953-4e2b9a031f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoP1zL0pARNV",
        "colab_type": "code",
        "outputId": "52ba90af-6f22-4170-a9b7-582a945f2be2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report #for model evaluation\n",
        "from sklearn.metrics import confusion_matrix #for model evaluation\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit #for data splitting\n",
        "np.random.seed(123) #ensure reproducibility\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('seaborn') # matplotlib 도 종류가 다양하기 때문에 seaborn 스타일로 지정한 거임.\n",
        "sns.set(font_scale=1) # (기본으로) 폰트 크기 2.5로 지정 미리 해놓는거임, 2.5면 꽤 크게 나옴\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd \n",
        "\n",
        "from numpy.random import seed\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "#train = pd.read_csv('drive/My Drive/데이콘_천체유형/train.csv', index_col=0)\n",
        "train = pd.read_csv('drive/My Drive/데이콘_천체유형/이상치_최대한_적게_train.csv', index_col=None)\n",
        "test = pd.read_csv('drive/My Drive/데이콘_천체유형/test.csv', index_col=0)\n",
        "sample_submission = pd.read_csv('drive/My Drive/데이콘_천체유형/sample_submission.csv', index_col=0)\n",
        "\n",
        "#######################################################################################################\n",
        "## petroMag_g 이상치 하나 더 없애기\n",
        "name = 'petroMag_g'\n",
        "drop_index = np.argmax(np.array(train[name]))\n",
        "train = train.drop(index=drop_index,axis=0)\n",
        "######################################################################################################\n",
        "\n",
        "print('csv 파일 (train, test, sample)을 불러왔습니다')\n",
        "print('train shape : {0}'.format(train.shape))\n",
        "print('test shape : {0}'.format(test.shape))\n",
        "print('='*50)\n",
        "\n",
        "train_type_num = train['type_num']\n",
        "needscaling_train = train.drop(['type','fiberID','type_num'],axis=1)\n",
        "needscaling_test = test.drop(['fiberID'], axis=1)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(needscaling_train)\n",
        "scaled_train = pd.DataFrame(sc.transform(needscaling_train),\n",
        "                      columns=needscaling_train.columns,\n",
        "                      index = needscaling_train.index)\n",
        "\n",
        "scaled_test = pd.DataFrame(sc.transform(needscaling_test),\n",
        "                      columns=needscaling_test.columns,\n",
        "                      index = needscaling_test.index)\n",
        "\n",
        "scaled_train['type_num'] = train_type_num\n",
        "train_x = scaled_train.drop(['type_num'],axis=1)\n",
        "train_y = scaled_train['type_num']\n",
        "test_x = scaled_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "csv 파일 (train, test, sample)을 불러왔습니다\n",
            "train shape : (199898, 23)\n",
            "test shape : (10009, 21)\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4h9A4H3E_XB",
        "colab_type": "text"
      },
      "source": [
        "## petroMag_g 의 maximum 값 하나는 없애도 됨. ( 합리적으로)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKEZBrFkCwzC",
        "colab_type": "code",
        "outputId": "849981fa-bc18-4741-89ce-5d856547cd7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# setting outlier symbol, title, xlabel\n",
        "name = 'petroMag_g'\n",
        "plt.boxplot(train[name], sym=\"bo\")\n",
        "plt.title('Box plot of {0}'.format(name))\n",
        "plt.xticks([1], [name])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# setting outlier symbol, title, xlabel\n",
        "name = 'petroMag_g'\n",
        "plt.boxplot(test[name], sym=\"bo\")\n",
        "plt.title('Box plot of {0}'.format(name))\n",
        "plt.xticks([1], [name])\n",
        "plt.show()\n",
        "\n",
        "Q1 = train.describe().loc['25%',name]\n",
        "Q3 = train.describe().loc['75%',name]\n",
        "minimum = train.describe().loc['min',name]\n",
        "maximum = train.describe().loc['max',name]\n",
        "IQR = Q3-Q1\n",
        "\n",
        "print(maximum - Q3 + (1.5*IQR))\n",
        "print(Q1 - (1.5*IQR) -  minimum)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEPCAYAAABY9lNGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAabElEQVR4nO3de5hddX3v8fckISOQjJBhIEBJgiN8\nqSmKKEfhIPHeEytHTVsVRaIeL/jgpe2x1kvrrV7w2lbhMVQPGKHyIDV6etBYq0fCoUYUBZVIvmCA\ncDMSJuiA4oSZzPljrcGdYc3svZPZ2Xsn79fz8DD791trr98OZD57/S7r1zM+Po4kSZPNancDJEmd\nyYCQJFUyICRJlQwISVIlA0KSVMmAkCRVmtPuBkitEhFXApdk5uf2wLXeALwXOBBYnJlDrb6m1GoG\nhGZcRNwGHAaMAQ8B3wXOzsw72tisKUXEEuBWYL/MHN2F8/cDPgk8NTN/3Eltm/Qe12fmE2vKDwHu\nBu7OzCW73VjtdexiUqucnpnzgMOBXwKfbnN7Wukw4FHAhnZcPCIa/aJ3QET8Uc3rl1EEh1TJOwi1\nVGb+LiL+FfjHibKIeDRFYCwHfgt8FvhQZu6IiM8Ah2bmn5bHfgR4MvDszNxp2X9EvBJ4LXAd8Arg\nF8A5mfntye2IiFnAO8vj9we+AbwpM38NXFUe9quIAHhOZq6fdH4v8BHgxWXRl4C/ARaX1584//uZ\n+cxJ5y6h+EX8eopuqB7gE5n58Zq2va1s20HAtynuuLZVtQ2I8tjvA2cBn4mId0/z+SZcDKwE/rp8\nfRbwhfKciba+vXx9KHAH8K7M/EpZNxv4aPke9wOfoPjvOO3dTUQcDawGnghcAyTw6Mw8c6pz1Bm8\ng1BLRcQBwEuA79UUfxp4NPAYYBnFL6pXlXX/Ezg+Il4ZEU8D/gewcnI41HgKsAk4BHgPsCYiFlQc\n98ryn2eU150HnFfWnVb++6DMnDc5HErvAp4KnAA8AfgvwN9m5k3A0przn1lx7oRnAMcAzwX+JiKe\nXZa/CXghxZ/FEcB9wPl12vYU4BaKu5cP1vl8Ey4BXhoRsyPiceUx10w6ZhPwNIr/Pu8DLomIw8u6\n11KE+gnAiWWbG/FFijDrpwjIVzR4ntrMgFCrfDUifgX8muJb78fg4W+hLwXekZn3Z+ZtFN9EXwGQ\nmb8tf/4kxS+0N2XmndNc5x7gHzPzocy8jOLb6Z9UHPdy4JOZeUtmPgC8g+KXZaN30S8H3p+Z92Tm\nVopfns3+ontfZv4mM38KXAScUZafTfFN/c7MHKH4Jfpnddp2d2Z+OjNHM/PBBj/fnRR/Ps+mCOWL\nJ79pZl6emXdn5o7yz/NmijCE4u7pn8p23gecW+8DR8Qi4CTg3Zm5PTOvBv6t3nnqDHYxqVVemJnf\nKgPhBcC68lvrOLAfsLnm2M3AkRMvMvOaiLiFopvjS3Wuc9eku4vNFN/CJzui4ppzKL6BN6Lq/Krr\nTKd2kH4zcHz582LgKxGxo6Z+rE7bJg/4N/r5vkBxp3EKxZ3CsbWVEXEW8FfAkrJoHsXd2cQ1aq/b\nyKSDI4BtZfDXnndUA+eqzbyDUEtl5lhmrqH4hXcqcC/FzKbFNYctAu6aeBER5wC9FDNs3lbnEkdG\nRM+k97q74ri7K645SjGA3sgjjavOr7rOdGp/KdaefwewPDMPqvnnUZl51zRtm1w+3eer9WWKO6xb\nMvP22oqIWEwxHvRGoD8zDwJuoBgzgWKM5w+m+DxT+QWwoOxqbOY8dQADQi0VET0R8QLgYODGzByj\nuCv4YETML38p/RVFdxIRcSzwAeBMii6ct0XECdNc4lDgzRGxX0T8OfCHwNcrjrsU+MuIODoi5gEf\nAi4rB1e3Ajso+u6ncinwtxExUE4PffdEm5vwdxFxQEQspRhzuawsX0Xx57EYoLzGC8q6RtpW7/M9\nLDN/AzwTeE3FexxIETxby3a8Cqid9fQl4C0RcWREHEQxSD+tzNwMXAu8NyLmRsTJwOn1zlNnsItJ\nrfJ/ImKM4hfOZoqB5olpoG+iGKi+BfgdxbfWC8v+8kuAj0ysJ4iIdwIXR8STy/75ya6hGPi9l+Lb\n8p9NsUjtQorujqsopqT+e9kOMvO3EfFB4D/LNQ3/LTO/N+n8DwB9wE/K15eXZc1YB/yc4ovZxzPz\nm2X5P1F8S/9mRBxBMa5yGfC/q9o2xXtP+fkmy8xrpyj/WUR8AlhPEUpfAP6z5pDPUnRJ/QQYBj4F\nPJ3i7nA6Lwc+DwxRDFZfBsyuc446QI8bBqlbldNcX5OZp7a7LdOZicVunSgilgOrMnNx3YN3Pu8y\nYGNmvqc1LdNM8Q5CUkMiYn+KabTfpBj8fg/wlQbOOwnYRhGSz6WYtFB3BpTaz4CQ1Kgeium9lwEP\nAl+jGIshIh6Y4pzlFIv/1lCsg7gTeENmXjfF8eogdjFJkirN2B1ERHwVOJpicOsBigVO15ezUlZT\nfHsYAs7KzJvLc6asa1IvxWKcX1B/wEySVJhN8by0HwCPmAQyk11MKyee+1JO0buQYjn+KuD8zLwk\nIs4ELqCYZkedumacBPy/3f0AkrSPehpw9eTClnQxlasx3ww8D7iJYtHNWLmqdohiWmLPVHXlowya\nMQj8/L77fsOOHXaZqfP0989jaGiqbnqpPWbN6uHggw8EeCzFc7h2MqOD1BHxOYpZCj0U87WPongU\nwhgUq2oj4u6yvGeaumYDYgyY+KBSR+rvn9fuJkhTqeyan9GAyMzXAETEKygezvZ3M/n+9QwNPeAd\nhDrSwMB8tm69v93NkHYya1bPtF9cWvKojcy8mGK+9J0Uz8qZDQ8/yXPigV93TFMnSWqzGQmIiJgX\nEUfVvD6dYmHMPcD1/P6xxmcA12Xm1sycsm4m2iRJ2j0z1cV0IHB5RBxI0Ze1jWLLyfGIOBtYXe54\ndR/Fc+gnTFcnSWqjvWWh3BLgVscg1GnWb9jCmnWb2DY8woK+XlYsG+TkpQvb3SwJ2GkM4mjgtsn1\nPmpDapH1G7aweu1Gto8W+wANDY+weu1GAENCXcH9IKQWWbNu08PhMGH76A7WrHvEdHOpIxkQUosM\nDVdtXzF1udRpDAipRfr7epsqlzqNASG1yIplg8yds/NfsblzZrFi2WCbWiQ1x0FqqUUmBqKdxaRu\n5TRXaQ/wURvqRPWmudrFJEmqZEBIkioZEJKkSgaEJKmSASFJqmRASJIqGRCSpEoGhCSpkgEhSapk\nQEiSKhkQkqRKM/KwvojoBy4GBoHtwM3A6zNza0Q8FbgA2J/iWR9nZuY95XlT1kmS2mum7iDGgY9m\nZmTm8cAm4NyImAVcApyTmccCVwHnAkxXJ0lqvxkJiMzclplX1hR9D1gMPAn4XWZeXZavAl5c/jxd\nnSSpzWZ8P4jyzuANwL8Bi4DNE3WZeW9EzIqIBdPVZea2Xbl2+dhaqSMNDMxvdxOkprRiw6BPAw8A\n5wEvasH7T8n9INSp3A9CnahmP4jq+pm8WER8HDgGeElm7gBup+hqmqg/BNhR3iFMVydJarMZC4iI\n+BDFuMILM3OkLP4hsH9EnFq+Phu4vIE6SVKbzciWoxGxFLgBuAl4sCy+NTNfFBGnUExlfRS/n8r6\ny/K8KeuatAS3HFUHs4tJnajelqPuSS210PoNW1izbhPbhkdY0NfLimWDnLx0YbubJQH1A6IVg9SS\nKMJh9dqNbB/dAcDQ8Air124EMCTUFXzUhtQia9ZtejgcJmwf3cGadZva1CKpOQaE1CJDwyNNlUud\nxoCQWqS/r7epcqnTGBBSi6xYNsjcOTv/FZs7ZxYrlg22qUVScxykllpkYiDaWUzqVk5zlfYA10Go\nE9Wb5moXkySpkl1MUgu5UE7dzICQWsSFcup2djFJLeJCOXU7A0JqERfKqdsZEFKLzOpprlzqNAaE\n1CJTzbh2Jra6hQEhtYiP2lC3MyCkFvFRG+p2TnOVWsRHbajb+agNaQ/wURvqRHtsR7mI+DjwpxS/\nrI/PzBvK8mOB1UA/MASclZk316uT9gaupFY3m8kxiK8CpwGbJ5WvAs7PzGOB84ELGqyTutrESuqh\n4RHG+f1K6vUbtrS7aVJDZiwgMvPqzLyjtiwiDgVOBC4tiy4FToyIgenqZqpNUju5klrdrtWD1EcB\nd2XmGEBmjkXE3WV5zzR1W3flYmVfmtQRtk2xYnrb8AgDA/P3cGuk5u1Vs5gcpFYnWdDXW/lYjQV9\nvQ5YqyPUDFJX17f4+ncAR0bEbIDy30eU5dPVSV3PdRDqdi0NiMy8B7geOKMsOgO4LjO3TlfXyjZJ\ne8rJSxeycvlx9Pf10kOxgnrl8uOcxaSuMWPrICLiU8AKYCFwLzCUmUsj4jiKqawHA/dRTGXN8pwp\n65q0BNdBqIO5DkKdqN46CBfKSS3kOgh1sj22UE7SztZv2MJFX7+R0bHiS8vQ8AgXff1GwB3l1B18\nWJ/UIpd+66aHw2HC6Ng4l37rpja1SGqOASG1yAMPjjZVLnUaA0KSVMmAkCRVMiAkSZUMCElSJQNC\nklTJgJAkVTIgJEmVDAhJUiUDQpJUyYCQJFUyICRJlQwISVIlH/ctNem0057Cxo031j3uT/7yK/T0\n9DyifHx8nEMP7Zv23OOO+0OuuuqaXW6jNBPcMEj7vB/+/Ys49rBHt7sZu+2mX/6aJ/3dV9rdDHUR\nd5ST2uTV5/7fKesufPsz92BLpGodv6NcRBxLsS91PzBEsS/1ze1tlTQ1u5i0r2h7QACrgPMz85KI\nOBO4APDrlTpWo7+4p7qD6Onp4Z57hmeySVJLtHUWU0QcCpwIXFoWXQqcGBED7WuVJAnafwdxFHBX\nZo4BZOZYRNxdlm9t9s3KvjSp4w0MzG93E6S62h0QM8pBanWLrVvvb3cTpNpB6ur6PdiWKncAR0bE\nbIDy30eU5ZKkNmprQGTmPcD1wBll0RnAdZnZdPeSJGlmdUIX09nA6oh4N3AfcFab2yNJogMCIjM3\nAk9pdzskSTtr9xiEJKlDGRCSpEoGhCSpkgEhtcgjn8I0fbnUaQwIqUXm7lcdBVOVS53GgJBaZOSh\n6lX9U5VLncaAkCRVMiAkSZUMCElSJQNCklTJgJBapL+vt6lyqdMYEFKLrFg2yNw5O/8VmztnFiuW\nDbapRVJz2v6wPmlvdfLShQCsWbeJbcMjLOjrZcWywYfLpU7XMz6+V8zJXgLc6o5y6lQDA/PdRU4d\np2ZHuaOB2x5Rv6cbJEnqDgaEJKmSASFJqmRASJIq7fYspog4E3gb8DjgLzLzvJq6A4CLgCcBo8Bb\nM/OKenWSpPabiTuI64GXAl+sqHsrMJyZjwVOBz4XEfMaqJMktdluB0Rm3pCZPwN2VFS/BLigPO5m\n4FpgeQN1kqQ2a/VCuUXA5prXtwNHNVC3S8r5vFJHGhiY3+4mSE2pGxAR8SOKX+ZVDsvMsZlt0q5z\noZw6lQvl1IlqFspVqhsQmXniblz/dmAxsLV8vQj4TgN1kqQ2a/U018uB1wNExDHAScA3GqiTJLXZ\nbgdERJwREXcCfw78fUTcGRGPK6s/BhwUET8HrgBel5n3N1AnSWozH9Yn7QGOQagT+bA+SdIuMSAk\nSZUMCElSJQNCklTJgJAkVTIgJEmVDAhJUiUDQpJUyYCQJFUyICRJlQwISVIlA0KSVMmAkCRVavWW\no9I+bf2GLaxZt4ltwyMs6OtlxbJBTl66sN3NkhpiQEgtsn7DFlav3cj20R0ADA2PsHrtRgBDQl3B\nLiapRdas2/RwOEzYPrqDNes2talFUnMMCKlFhoZHmiqXOo0BIbVIf19vU+VSp9ntMYiIOB94FjAC\nPAC8JTOvLesOAy6m2BL0QYp9p6+pVyftDVYsG+TCK37GWM0uuLN7inKpG8zEHcRa4PjMfALwYeCy\nmroPA1dl5rHAOcAlEdHTQJ20V+iZ1TPta6mT7XZAZOYVmflQ+XI98AcRMfG+LwZWlcddTXGX8eQG\n6qSut2bdJkZrbx+A0bFxB6nVNWZ6musbga9l5o6I6Ad6MvPemvrbgaMi4pap6oAf7OrF+/vn7eqp\n0ozbNsVg9LbhEQYG5u/h1kjNqxsQEfEjYNEU1Ydl5lh53EuBlwGnzVzzmjM09AA7dozXP1DaAxb0\n9VbOWFrQ18vWrfe3oUXSzmbN6pn2i3XdgMjME+sdExEvAj4IPCszf1meNxQRRMQhNXcKi4A7pqur\ndy2pW6xYNrjTQjmAuXNmOUitrrHbYxAR8Xzgk8AfZ+Ztk6ovB84ujzsV2B/4YQN1Utc7eelCVi4/\njv6+XnoopreuXH6cq6jVNXrGx3evSyYitgLbga01xc8q7xIWApcAiymmsp6dmd8tz5uybhcsAW61\ni0mdamBgvt1K6jg1XUxHA7dNrt/tgOgQSzAg1MEMCHWiegHhSmpJUiUDQpJUyYCQJFUyICRJldww\nSGohd5RTNzMgpBZxRzl1O7uYpBZxRzl1OwNCahF3lFO3MyCkFnFHOXU7A0JqkccP9jdVLnUaA0Jq\nkZ9sGmqqXOo0BoTUIo5BqNsZEFKLOAahbmdASC2yYtkgc+fs/FfMDYPUTVwoJ7XIxGI4V1KrW7kf\nhLQHuB+EOpH7QUiSdokBIUmqtNtjEBHxLuAlwBjQA3w4My8r6w4ALgKeBIwCb83MK+rVSZLabybu\nIM7LzMdn5hOB5wGfjYiDy7q3AsOZ+VjgdOBzETGvgTpJUpvtdkBk5q9rXs4Dxmve9yXABeVxNwPX\nAssbqJMktdmMTHONiLOBvwCOAl6dmRPPElgEbK459PbymHp1u6QcjZc60sDA/HY3QWpK3YCIiB9R\n/DKvclhmjmXmKmBVRBwP/EtEfKsmJPYYp7mqUznNVZ2oZpprpboBkZknNnqxzPxpRNwNPB34MsVd\nwWJga3nIIuA75c/T1UmS2my3xyAi4nE1Px8NPBH4WVl0OfD6su4Y4CTgGw3USZLabCbGIN4bEUuB\nhyimur45M28s6z4GfD4ifl7WvS4z72+gTpLUZj5qQ9oDHINQJ/JRG5KkXWJASJIqGRCSpEoGhCSp\nkgEhSapkQEiSKhkQkqRKBoQkqdKMPM1VUrX1G7awZt0mtg2PsKCvlxXLBjl56cJ2N0tqiAEhtcj6\nDVtYvXYj20d3ADA0PMLqtRsBDAl1BbuYpBZZs27Tw+EwYfvoDtas29SmFknNMSCkFhkaHmmqXOo0\nBoTUIv19vU2VS53GgJBaZMWyQebO2fmv2Nw5s1ixbLBNLZKa4yC11CITA9HOYlK3cj8IaQ9wPwh1\nIveDkCTtEgNCklRpxsYgIuLpwLeBt2TmeWXZYcDFFF1AD1LsO31NvTpJUvvNyB1ERMwHPgKsnVT1\nYeCqzDwWOAe4JCJ6GqiTJLXZTHUxfRL4GHDvpPIXA6sAMvNqYAR4cgN1kqQ22+0upohYDjw6M/81\nIp5fU94P9GRmbWjcDhwVEbdMVQf8YFfbUo7GSx1pYGB+u5sgNaVuQETEj4BFU1UD5wLPmclG7Sqn\nuapTOc1VnahmmmulugGRmSdOVRcRpwKHA9+PCIBDgNMjYkFmvj8iiIhDau4UFgF3ZObQVHWNfSxJ\nUqvtVhdTOXZw6MTriPg8cO3ELCbgcuBs4ANlmOwP/LCBOklSm7X6URtvp5idtJJiKusrMnNHA3WS\npDbzURvSHuAYhDqRj9qQJO0SA0KSVMmAkCRVMiAkSZUMCElSJQNCklTJgJAkVTIgJEmVWr2SWtqn\nrd+whTXrNrFteIQFfb2sWDbIyUsXtrtZUkMMCKlF1m/Ywuq1G9k+WjxBZmh4hNVrNwIYEuoKdjFJ\nLbJm3aaHw2HC9tEdrFm3qU0tkppjQEgtMjQ80lS51GkMCKlF+vt6myqXOo0BIbXIimWDzJ2z81+x\nuXNmsWLZYJtaJDXHQWqpRSYGop3FpG7lfhDSHuB+EOpE7gchSdolBoQkqZIBIUmqZEBIkirtLbOY\nZkMx4CJ1Kv//VKep+X9ydlX93hIQhwMcfPCB7W6HNKVytojUiQ4HHvEMmL1lmmsvcBLwC2CszW2R\npG4xmyIcfgA84hkwe0tASJJmmIPUkqRKBoQkqZIBIUmqZEBIkioZEJKkSgaEJKmSASFJqmRASJIq\nGRCSpEoGhCSpkgEhSapkQEiSKhkQ0hQi4qCIeNtunP/0iBiPiI9NKr+yLPf53+poBoQ0tYOAKQMi\nIhrZTyWBF0bE7PKcxwBuXKKusLdsGCRNKyLGgfcDLwD2B96ZmV8u654CnAv0lYe/OzO/BpwPHBQR\n1wO/zcxTIuJK4HrgqcA24HkRcRbw18A4xaYrr8/Me8r3egDYAPwx8HVgJfAF4Mk1bfs4sAyYC9wL\nvDozN5d1bwTeAvyqPP+czDxkms95ZPn+C8u29AD/npnn7cIfm/Zx3kFoXzKWmScA/x3454g4NCIO\nAlYBL8vMJwHPBy4oy88BfpWZJ2TmKTXv8xjg1Mx8XkT8EUW4PDczHw/cAHx60nU/D6yMiB7gpcAX\nJ9Wfm5knZeYTgEuBjwBExOOBdwCnZOZJFHc09XwK+E5mLgXeRBE80i4xILQv+V8AmZnAjyjuAk4B\njgbWlncKaynuBB47zft8MTNHy5+fAXw9M39Rvr4AePak468EHg+8ELghM4cm1S+PiO9FxA3AW4ET\nyvKnl++9tXx9YQOf8RnAReXn3Ax8u4FzpEp2MWlf1wP8JDNPm1wREUumOOeBZi6QmeMR8SXgs8Cr\nJl1jMfAPwEmZeWtEnMIj7zCktvAOQvuSVwFExDHAE4HvAd8FjomIZ0wcFBEnld1Bw8ABdQajv0Mx\nDrGwfP1a4D8qjvtn4KMUdyi1+oDtwJaImAWcXVO3juLuYmLMYWX9j8iVE8dFxFHAMxs4R6pkQGhf\nMicirgOuoBxIzsz7KMYk3hMRP46IG4H3Aj2ZuQ34F+CnEfHdqjfMzBuAtwP/ERE/AZ5AMag8+bi7\nMvOjNV1TE+U/BS4HfgZcA9xaU/djilBZHxE/BEaBX9f5jG8BnhMRG4DPAN9v4BypUs/4+Hi72yC1\nXDmLaX5mNtU91G4RMT8z7y9/fi/w2Mw8c5rj9wceyszRiDgc+AHwrHLcRWqKYxBSZzs3Iv4rxRTY\nW4DX1Tn+GOALZRfZfsD7DAftKu8gpC4TESdQTJ2d7LzM/Nwebo72YgaEJKmSg9SSpEoGhCSpkgEh\nSapkQEiSKv1/86kb8I35pz8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEPCAYAAAC6Kkg/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZRUlEQVR4nO3de5TdZX3v8feEQAomEBiGgJoLInwr\n3hCIFw6CijdaOSqtCoqgPRXThRTbUqq2VbRaQBTbiktQKyI5uBAbe85BY/VYCQeN4SYqIF8VSLjL\nMKhJlCYkM+eP329cO8Nc9jN7Jr894f1aK4vZz/d3efaQ7M9+nue3969naGgISZJKzGq6A5Kkmcfw\nkCQVMzwkScUMD0lSMcNDklTM8JAkFZvddAekJkTE1cDyzPzcdjjXnwFnA08CFmfmwHSfU5puhoe2\nq4hYCywAtgKPAd8DlmXmPQ12a0wRsQS4C9g5M7dMYv+dgQuAF2bmD7upbyOOcXNmPq+lfW/gfuD+\nzFzScWe1w3HaSk04LjPnAvsBvwA+2XB/ptMC4PeAW5s4eUS0+wZxt4h4VsvjN1OFijQqRx5qTGb+\nV0R8Bfin4baI2IMqTI4Ffgt8FvjHzByMiE8D+2TmH9XbngccDrw8M7f5qoSIeBvwDuAHwFuBB4DT\nMvPbI/sREbOA99Xb7wp8Azg9M38NXFNv9quIAHhFZq4esf8c4DzgjXXTl4G/ARbX5x/e/7rMfNmI\nfZdQvUi/k2pqqwf4eGZ+rKVvZ9V9mw98m2qk9shofQOi3vY64GTg0xHx/nGe37DLgFOAv64fnwx8\nsd5nuK/vqR/vA9wD/G1mfrWu7QR8tD7GBuDjVP8fxx0VRcT+wKXA84A1QAJ7ZOZJY+2j7uDIQ42J\niN2ANwHfb2n+JLAH8DTgaKoXsbfXtb8Cnh0Rb4uIFwP/AzhlZHC0eAFwB7A38AFgRUTsNcp2b6v/\nvLQ+71zgwrp2VP3f+Zk5d2Rw1P4WeCFwCPBc4PnA32XmT4Fntuz/slH2HfZS4EDglcDfRMTL6/bT\ngddR/S6eDPwS+NQEfXsBcCfVqOcjEzy/YcuBEyJip4g4uN5mzYht7gBeTPX/54PA8ojYr669gyrw\nDwEOrfvcjsupgq6XKjzf2uZ+apjhoSb8e0T8Cvg11bvl8+F3715PAN6bmRsycy3VO9i3AmTmb+uf\nL6B6sTs9M+8d5zwPAf+UmY9l5hVU72r/cJTt3gJckJl3ZuZG4L1UL6TtjszfAnwoMx/KzH6qF9bS\nF8EPZuZvMvPHwCXAiXX7Mqp3+Pdm5iaqF9g/nqBv92fmJzNzS2Y+2ubzu5fq9/NyqsC+bORBM/PK\nzLw/Mwfr3+fPqIISqlHXP9f9/CVw7kRPOCIWAUuB92fm5sy8FvjfE+2n7uC0lZrwusz8v3VYvBZY\nVb/bHQJ2Bta1bLsOeMrwg8xcExF3Uk2dfHmC89w3YlSyjurd+0hPHuWcs6neubdjtP1HO894Wi8Y\nWAc8u/55MfDViBhsqW+doG8jLz5o9/l9kWqEcgTVCOOg1mJEnAz8JbCkbppLNaobPkfredu5AOLJ\nwCP1m4LW/Ra2sa8a5shDjcnMrZm5gurF8EjgYaorsBa3bLYIuG/4QUScBsyhuhLorAlO8ZSI6Blx\nrPtH2e7+Uc65hWoxv52vnR5t/9HOM57WF8zW/e8Bjs3M+S1/fi8z7xunbyPbx3t+rf6NamR2Z2be\n3VqIiMVU60/vAnozcz5wC9UaDVRrSk8d4/mM5QFgr3r6smQ/dQHDQ42JiJ6IeC2wJ/CTzNxKNZr4\nSETMq1+w/pJqioqIOAj4MHAS1bTQWRFxyDin2Af484jYOSLeADwD+Poo230J+IuI2D8i5gL/CFxR\nL/T2A4NUawVj+RLwdxHRV1/i+v7hPhf4+4jYLSKeSbXGc0XdfhHV72MxQH2O19a1dvo20fP7ncz8\nDfAy4E9HOcaTqEKpv+7H24HWq7O+DJwREU+JiPlUFwyMKzPXATcAZ0fELhHxIuC4ifZTd3DaSk34\nPxGxlerFaB3VovfwpaynUy2a3wn8F9W73c/X8/PLgfOGPy8REe8DLouIw+v1gJHWUC1CP0z1LvuP\nx/iA3uepplCuobqs9j/qfpCZv42IjwDfrT+z8erM/P6I/T8M7A78qH58Zd1WYhXwc6o3dB/LzG/W\n7f9M9e7+mxHxZKp1nCuA/zVa38Y49pjPb6TMvGGM9tsi4uPAaqrA+iLw3ZZNPks1zfUjYD3wL8BL\nqEaV43kL8AVggGrh/Apgpwn2URfo8WZQ2hHVl+r+aWYe2XRfxjMVH/TrRhFxLHBRZi6ecONt97sC\nuD0zPzA9PdNUceQhqWMRsSvVpcDfpFqI/wDw1Tb2Wwo8QhWgr6S6gGLCK7XUPMND0lToobpE+Qrg\nUeBrVGs/RMTGMfY5luqDjyuoPudxL/BnmfmDMbZXF3HaSpJU7Ikw8phD9UGkB5h48U6SVNmJ6vvn\nrgced0HKEyE8lgL/r+lOSNIM9WLg2pGNT4TweADgl7/8DYODTtGp+/T2zmVgYKxlAakZs2b1sOee\nT4L6NXSkJ0J4bAUYHBwyPNS1/LupLjbqdL+fMJckFTM8JEnFDA9JUjHDQ5JU7ImwYC51pdW3PsiK\nVXfwyPpN7LX7HI4/+gBe9Mx9m+6W1BbDQ2rA6lsf5NKVt7N5S3WPp4H1m7h05e0ABohmBKetpAas\nWHXH74Jj2OYtg6xYdUdDPZLKGB5SAwbWj3b7kbHbpW5jeEgN6N19TlG71G0MD6kBxx99ALvM3vaf\n3y6zZ3H80Qc01COpjAvmUgOGF8W92kozlSMPSVIxRx5SA7xUVzOdIw+pAV6qq5nO8JAa4KW6mukM\nD0lSMcNDklTM8JAkFfNqK2kKHXXUC7j99p9MuN0f/sVX6enpeVz70NAQ++yz+7j7/v7vP4Nrrlkz\n6T5KU6FnaKh7750cEQcBlwK9wABwcmb+rPAwS4C7BgY2ep9oFbvxH17PQQv2aLobHfvpL37NYX//\n1aa7oRlk1qweenvnAuwPrB1Z7/aRx0XApzJzeUScBFwMvKzhPukJpPQFdypGHl/7xOvH3XcyI4/D\niraWJta14RER+wCHAq+om74EXBgRfZnZ31zPpLG1+6L+J+f+56jtPT09PPTQ+qnskjQtujY8gIXA\nfZm5FSAzt0bE/XV7cXjUwy+p6/X1zWu6C9KEujk8ppRrHpop+vs3NN0FqXXNY/T6duxLqXuAp0TE\nTgD1f59ct0uSGtS14ZGZDwE3AyfWTScCP3C9QzuCOTs/frF8vHap23RteNSWAadHxE+B0+vH0ox3\n8qufwciLrXp6qnZpJujqNY/MvB14QdP9kKaaN4PSTNfVHxKcIkvwQ4LqYn1981wkV9eZ6EOC3T5t\nJUnqQoaHJKmY4SFJKmZ4SJKKGR6SpGKGhySpmOEhSSpmeEiSihkekqRihockqVhXf7eVtCNbfeuD\nfreVZizDQ2rA6lsf5NKVt7N5yyAAA+s3cenK2wEMEM0ITltJDVix6o7fBcewzVsGWbHqjoZ6JJUx\nPKQGDKzfVNQudRvDQ2pA7+5zitqlbmN4SA04/ugD2GX2tv/8dpk9i+OPPqChHkllXDCXGuCdBDXT\nTeudBCPiU8AxwCZgI3BGZt5Q1xYAl1Hd6e9R4NTMXDNRbRKW4J0E1cW8k6C6UdN3ElwJPDsznwuc\nA1zRUjsHuCYzDwJOA5ZHRE8bNUlSw6Y1PDLzqsx8rH64GnhqRAyf843ARfV211KNTg5voyZJatj2\nXPN4F/C1zByMiF6gJzMfbqnfDSyMiDvHqgHXT/bk9fBL6kp9ffOa7oJUpKPwiIibgEVjlBdk5tZ6\nuxOANwNHdXK+TrjmoW7lmoe6Ucuax6g6Co/MPHSibSLi9cBHgGMy8xf1fgMRQUTs3TLCWATcM16t\nk75KkqbOtK55RMRrgAuAV2Xm2hHlK4Fl9XZHArsCN7ZRkyQ1bLrXPC4BNgNfiYjhtmMycwB4D9VV\nVKdQXY771swc/rKf8WqSpIZN6+c8usQS/JyHuphrHupGTX/OQ5K0AzI8JEnFDA9JUjHDQ5JUzPCQ\nJBUzPCRJxQwPSVIxw0OSVMzwkCQVMzwkScUMD0lSMcNDklTM8JAkFTM8JEnFDA9JUjHDQ5JUzPCQ\nJBUzPCRJxab7HuYARMRLgG8DZ2TmhXXbAuAyqtvEPgqcmplrJqpJkpo37SOPiJgHnAesHFE6B7gm\nMw8CTgOWR0RPGzVJUsO2x7TVBcD5wMMj2t8IXASQmdcCm4DD26hJkho2rdNWEXEssEdmfiUiXtPS\n3gv0ZGZroNwNLIyIO8eqAddPti+9vXMnu6s07fr65jXdBalIR+ERETcBi8YqA+cCr+jkHFNlYGAj\ng4NDTXdDepy+vnn0929ouhvSNmbN6hn3TXdH4ZGZh45Vi4gjgf2A6yICYG/guIjYKzM/FBFExN4t\nI4xFwD2ZOTBWrZO+SpKmzrRNW9VrFfsMP46ILwA3DF9tBVwJLAM+XAfNrsCNbdQkSQ3bLpfqjuE9\nVFdRnUJ1Oe5bM3OwjZokqWE9Q0M7/DrAEuAu1zzUrVzzUDdqWfPYH1j7uPr27pAkaeYzPCRJxQwP\nSVIxw0OSVMzwkCQVMzwkScUMD0lSMcNDklTM8JAkFTM8JEnFDA9JUjHDQ5JUzPCQJBUzPCRJxQwP\nSVIxw0OSVMzwkCQVMzwkScWm/R7mEXE6cBrwGLA1Mw+p23cDLgEOA7YAZ2bmVRPVJEnNm9aRR0Qc\nD7wBWJqZzwZe1VI+E1ifmU8HjgM+FxFz26hJkho23dNWfwWcnZkbADLzFy21NwEX1+0/A24Ajm2j\nJklq2HRPWx0MvDAiPgzsAlycmZ+ta4uAdS3b3g0sbKM2Kb29DlzUvfr65jXdBalIR+ERETdRvdCP\nZgGwE9WL/pHA3sB3IyIz85pOzjsZAwMbGRwc2t6nlSbU1zeP/v4NTXdD2sasWT3jvunuKDwy89Dx\n6hFxN/ClzBwEHoqIbwHPB66hGk0sBvrrzRcB36l/Hq8mSWrYdK95XA68GiAingS8GPhhXbsSeGdd\nOxBYCnyjjZokqWHTHR6fABZGxK3AdcDyzPxWXTsfmB8RPweuAk4dXlifoCZJaljP0NAOvw6wBLjL\nNQ91K9c81I1a1jz2B9Y+rr69OyRJmvkMD0lSMcNDklTM8JAkFTM8JEnFDA9JUjHDQ5JUzPCQJBUz\nPCRJxQwPSVIxw0OSVMzwkCQVMzwkScUMD0lSMcNDklTM8JAkFTM8JEnFDA9JUrHZ03nwiDgI+Aww\nH5gDXJGZZ9e13YBLgMOALcCZmXnVRDVJUvOme+TxUeArmXkIsBR4e0Q8v66dCazPzKcDxwGfi4i5\nbdQkSQ2b7vAYAvaof96tfvxQ/fhNwMUAmfkz4Abg2DZqkqSGTXd4vBt4U0TcB6wFzs/MtXVtEbCu\nZdu7gYVt1CRJDetozSMibqJ6oR/NAuCdwGWZeX5E7AdcHRE3ZOaaTs47Gb29znqpe/X1zWu6C1KR\njsIjMw8drx4Rfw48rd72gYj4T+AoYA3VaGIx0F9vvgj4Tv3zeLVJGRjYyODgUCeHkKZFX988+vs3\nNN0NaRuzZvWM+6Z7uqet7gJeDRAR84AXA7fUtSupRiZExIFUC+rfaKMmSWrYdIfH24BlEfFDqtHG\nlzNzZV07H5gfET8HrgJOzcwNbdQkSQ3rGRra4adylgB3OW2lbuW0lbpRy7TV/lQXPG1b394dkiTN\nfIaHJKmY4SFJKmZ4SJKKGR6SpGKGhySpmOEhSSpmeEiSihkekqRihockqZjhIUkqZnhIkooZHpKk\nYoaHJKmY4SFJKmZ4SJKKGR6SpGKGhySp2OxODxARJwFnAQcD787MC1tquwGXAIcBW4AzM/OqTmqS\npOZNxcjjZuAE4PJRamcC6zPz6cBxwOciYm6HNUlSwzoOj8y8JTNvAwZHKb8JuLje7mfADcCxHdYk\nSQ2b7jWPRcC6lsd3Aws7rEmSGjbhmkdE3ET1Yj6aBZm5dWq7ND16e531Uvfq65vXdBekIhOGR2Ye\n2sHx7wYWA/3140XAdzqsTcrAwEYGB4c6OYQ0Lfr65tHfv6HpbkjbmDWrZ9w33dM9bXUl8E6AiDgQ\nWAp8o8OaJKlhHYdHRJwYEfcCbwD+ISLujYiD6/L5wPyI+DlwFXBqZm7osCZJaljP0NAOP5WzBLjL\naSt1K6et1I1apq32B9Y+rr69OyRJmvkMD0lSMcNDklTM8JAkFTM8JEnFDA9JUjHDQ5JUzPCQJBUz\nPCRJxQwPSVIxw0OSVMzwkCQVMzwkScUMD0lSMcNDklTM8JAkFTM8JEnFDA9JUjHDQ5JUbHanB4iI\nk4CzgIOBd2fmhS21TwHHAJuAjcAZmXlDXVsAXEZ1j/FHgVMzc81ENUlS86Zi5HEzcAJw+Si1lcCz\nM/O5wDnAFS21c4BrMvMg4DRgeUT0tFGTJDWs4/DIzFsy8zZgcJTaVZn5WP1wNfDUiBg+5xuBi+rt\nrqUanRzeRk2S1LCOp60KvAv4WmYORkQv0JOZD7fU7wYWRsSdY9WA6yd78t7euZPdVZp2fX3zmu6C\nVGTC8IiIm4BFY5QXZObWNo5xAvBm4Kiy7k2dgYGNDA4ONXV6aUx9ffPo79/QdDekbcya1TPum+4J\nwyMzD+2kAxHxeuAjwDGZ+Yv6mAMRQUTs3TLCWATcM16tk35IkqbOtF6qGxGvAS4AXpWZa0eUrwSW\n1dsdCewK3NhGTZLUsJ6hoc6mciLiROB8YE9gM/Ab4JWZeVtE9Ndt/S27HFOPLvYFlgOLqS7HXZaZ\n36uPOWZtEpYAdzltpW7ltJW6Ucu01f7A2pH1jsNjBliC4aEuZnioG00UHn7CXJJUzPCQJBUzPCRJ\nxQwPSVIxw0OSVMzwkCQVMzwkScUMD0lSMcNDklTM8JAkFTM8JEnFtufNoCS1WH3rg6xYdQePrN/E\nXrvP4fijD+BFz9y36W5JbTE8pAasvvVBLl15O5u3VHdvHli/iUtX3g5ggGhGcNpKasCKVXf8LjiG\nbd4yyIpVdzTUI6mM4SE1YGD9pqJ2qdsYHlIDenefU9QudRvDQ2rA8UcfwC6zt/3nt8vsWRx/9AEN\n9Ugq44K51IDhRXGvttJMNRX3MD8JOAs4GHh3Zl44yjYvAb4NnDFcj4gFwGVUt4l9FDg1M9dMVJuE\nJXgbWnUxb0OrbrQ9bkN7M3ACcPloxYiYB5wHrBxROge4JjMPAk4DlkdETxs1SVLDOg6PzLwlM28D\nBsfY5ALgfODhEe1vBC6qj3EtsAk4vI2aJKlh07rmERHHAntk5lci4jUt7b1AT2a2BsrdwMKIuHOs\nGnD9ZPtSD7+krtTXN6/pLkhFJgyPiLgJWDRGeUFmbh1jv/nAucArJt+9qeOah7qVax7qRi1rHqOa\nMDwy89BJnvtZwH7AdREBsDdwXETslZkfiggiYu+WEcYi4J7MHBirNsl+7ATVL0LqVv79VLdp+Tu5\n02j1aZu2qtcq9hl+HBFfAG5ouRrrSmAZ8OGIOBLYFbixjVqp/QD23PNJk9xdmn5Oq6qL7Qc87ntz\npuJS3ROpFsT3BDYDvwFeWS+it273BVrCIyL2BZYDi6kux12Wmd+bqDYJc4ClwAPAqFNskqTH2Ykq\nOK6numhpGx2HhyTpicevJ5EkFTM8JEnFDA9JUjHDQ5JUzPCQJBUzPCRJxQwPSVIxw0OSVMzwkCQV\nMzwkScUMD0lSMcNDklTM8JAmISLmR8RZHez/kogYiojzR7RfXbf7He3qaoaHNDnzgTHDIyLauVdO\nAq+LiJ3qfZ4GeOMZzQjTeg9zaSaIiCHgQ8BrqW489r7M/Le69gKq2ynvXm/+/sz8GvApYH5E3Az8\nNjOPiIirgZuBFwKPAH8QEScDfw0MUd1Q552Z+VB9rI3ArcCrgK8DpwBfBA5v6dvHgKOBXYCHgT/J\nzHV17V3AGcCv6v1Py8y9x3meT6mPv2/dlx7gP1pu0Ca1zZGHVNmamYcA/x34TETsExHzgYuAN2fm\nYcBrgIvr9tOAX2XmIZl5RMtxngYcmZl/EBHPogqeV2bmc4BbgE+OOO8XgFMiogc4Abh8RP3czFya\nmc8FvgScBxARzwHeCxyRmUupRkIT+RfgO5n5TOB0qlCSJsXwkCr/CpCZCdxENXo4AtgfWFmPMFZS\njSCePs5xLs/MLfXPLwW+npkP1I8vBl4+YvurgecArwNuycyBEfVjI+L7EXELcCZwSN3+kvrY/fXj\nz7fxHF8KXFI/z3XAt9vYRxqV01bS2HqAH2XmUSMLEbFkjH02lpwgM4ci4svAZ4G3jzjHYuATwNLM\nvCsijuDxIxOpEY48pMrbASLiQOB5wPeB7wEHRsRLhzeKiKX1FNN6YLcJFsa/Q7XusW/9+B3At0bZ\n7jPAR6lGNq12BzYDD0bELGBZS20V1ahkeI3jlImfIlcPbxcRC4GXtbGPNCrDQ6rMjogfAFdRL2pn\n5i+p1kA+EBE/jIifAGcDPZn5CPA/gR9HxPdGO2Bm3gK8B/hWRPwIeC7VAvfI7e7LzI+2THcNt/8Y\nuBK4DVgD3NVS+yFV4KyOiBuBLcCvJ3iOZwCviIhbgU8D17WxjzSqnqGhoab7IDWqvtpqXmYWTTk1\nLSLmZeaG+uezgadn5knjbL8r8FhmbomI/YDrgWPqdR6piGse0sx1bkT8N6rLeO8ETp1g+wOBL9bT\nbjsDHzQ4NFmOPKQdSEQcQnX570gXZubntnN3tAMzPCRJxVwwlyQVMzwkScUMD0lSMcNDklTs/wN9\nb+/ar53IAgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "253.7626148220352\n",
            "475.60843595216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in4ouFUuFFLy",
        "colab_type": "text"
      },
      "source": [
        "## 딥러닝 모델 구축 시작 (Elu)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e37totm8FI28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#random seeds for stochastic parts of neural network \n",
        "np.random.seed(123)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(123)\n",
        "\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import  Activation,  Lambda, Flatten, LeakyReLU, ELU, Dense\n",
        "from keras.layers import Input, Concatenate, Reshape, Dropout, BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.optimizers import Adam, SGD,RMSprop\n",
        "from keras import  backend as K\n",
        "from keras import metrics\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
        "#from swa.keras import SWA # swa optimizer - https://pypi.org/project/keras-swa/\n",
        "import tensorflow as tf\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "\n",
        "################ Gelu #############\n",
        "\n",
        "class Gelu(Activation):\n",
        "    def __init__(self, activation, **kwargs):\n",
        "        super(Gelu, self).__init__(activation, **kwargs)\n",
        "        self.__name__='gelu'\n",
        "        \n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "\n",
        "get_custom_objects().update({'gelu': Activation(gelu)})\n",
        "\n",
        "################# tanh 활용 ( 범위 늘림 ) ##############\n",
        "def custom_activation(x):\n",
        "  return (K.tanh(x) * 145) + 155\n",
        "\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
        "\n",
        "def create_nn_model():\n",
        "    inp = Input(shape=(20,))\n",
        "    x = Dense(2373)(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ELU()(x)\n",
        "#    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.45)(x)\n",
        "    \n",
        "    x = Dense(2355)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ELU()(x)\n",
        "#    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.45)(x)\n",
        "\n",
        "    x = Dense(1197)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ELU()(x)\n",
        "#    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.4)(x)\n",
        "\n",
        "    x = Dense(1187)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ELU()(x)\n",
        "#    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.35)(x)\n",
        "\n",
        "    x = Dense(612)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ELU()(x)\n",
        "#    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.35)(x)\n",
        "\n",
        "    x = Dense(602)(x)\n",
        "#    x = BatchNormalization()(x)\n",
        "    x = ELU()(x)\n",
        "#    x = Activation(gelu)(x)\n",
        "    x = Dropout(rate = 0.3)(x)\n",
        "\n",
        "\n",
        "    out = Dense(19, activation='softmax')(x) #scalar_coupling_constant    \n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    return model\n",
        "\n",
        "nn_model = create_nn_model(  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IVJlTVTKYT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_folder = '0218_이상치한개더제거_ELU_RedLR_DO_0.45'\n",
        "rate = ''\n",
        "\n",
        "import os\n",
        "SAVEMODEL_NEWFOLDER0 = 'drive/My Drive/데이콘_천체유형/ModelCheck/' + csv_folder\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "if not os.path.exists(SAVEMODEL_NEWFOLDER1):\n",
        "  os.mkdir(SAVEMODEL_NEWFOLDER1)\n",
        "  print('모델폴더를 새로 생성했습니다.')\n",
        "SUBMISSION_NEWFOLDER = 'drive/My Drive/데이콘_천체유형/파일제출/' + csv_folder\n",
        "if not os.path.exists(SUBMISSION_NEWFOLDER):\n",
        "  os.mkdir(SUBMISSION_NEWFOLDER)\n",
        "  print('제출폴더를 새로 생성했습니다.')\n",
        "\n",
        "################## StratifiedShuffleSplit 를 이용해서 층화분할 #############\n",
        "X_array = train_x.values\n",
        "y_array = train_y.values\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=2, test_size=1/13, random_state=123)\n",
        "index1, index2 = sss.split(X_array, y_array)\n",
        "train_index = index1[0].tolist()\n",
        "val_index = index1[1].tolist()\n",
        "train_input = X_array[train_index]\n",
        "cv_input = X_array[val_index]\n",
        "train_target  = y_array[train_index]\n",
        "cv_target = y_array[val_index]\n",
        "\n",
        "\n",
        "\n",
        "#train_index, val_index = train_test_split(np.arange(len(train_y)),random_state=42, test_size=1/13)\n",
        "#train_input=train_x.iloc[train_index].values\n",
        "#train_target0=train_y[train_index]\n",
        "#train_target=train_target0.values\n",
        "#cv_input=train_x.iloc[val_index].values\n",
        "#cv_target0=train_y[val_index]\n",
        "#cv_target = cv_target0.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFJe8RqARKMq",
        "colab_type": "text"
      },
      "source": [
        "# Reduce LR on Plateau\n",
        "\n",
        "\n",
        "\n",
        "> patience를 지정해놓고 metric(일반적으로 val_loss)를 보면서 줄어들지 않을 때에 (손실감소곡선이 너무 평탄할때) 일정상수를 initial_learning_rate에 곱해 local minima를 탈출하는 방법\n",
        "\n",
        "# 스케쥴러\n",
        "\n",
        "\n",
        "\n",
        "> 스케쥴러도 위의 Reduce LR과 굉장히 유사함. 하지만 스케쥴러는 위와 달리  (Linear, Cosine) 등 여러가지 종류의 스케쥴러로 **patience를 지정해서 쓰는게 아니라** 일정 패턴을 따라 큰 값에서 작은 값을 왔다갔다하며 그 사이에서 학습되도록 함.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5OARe2ydAAt",
        "colab_type": "text"
      },
      "source": [
        "## ELU로 해보니까 1e-4로 했는데도 적합이 덜 되어버리는 문제가 생김..\n",
        "\n",
        "## local minima에 빠진건 아닐까? => learning rate 더 줄이기 (3e-5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3645b4ba-eedd-4807-a5c2-9b2a5f1b11f4",
        "id": "NIHtRYs2c-xn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch =  800\n",
        "pat =  80\n",
        "red_pat =  25\n",
        "batchsize = 256*4\n",
        "initial_rate = 3e-5\n",
        "factor = 1/np.sqrt(10) # red_patience 만큼 기다리다가, 학습률을 *factor 배로 줄여버림 \n",
        "minimumlr = (3e-8)/np.sqrt(10)\n",
        "\n",
        "import os\n",
        "MODEL_SAVE_FOLDER_PATH0 = SAVEMODEL_NEWFOLDER0 +  '/initial_rate=%s/' % initial_rate ## checkpoint\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "if not os.path.exists(MODEL_SAVE_FOLDER_PATH1):\n",
        "  os.mkdir(MODEL_SAVE_FOLDER_PATH1)\n",
        "check_path = MODEL_SAVE_FOLDER_PATH0 + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "### model early stopping : 더이상 성능 개선이 되지 않으면 멈춤\n",
        "es = EarlyStopping(monitor= 'val_loss', patience = pat, verbose = 1, mode='min',\n",
        "                    restore_best_weights = True\n",
        "                   ) \n",
        "\n",
        "### model check point\n",
        "mc = ModelCheckpoint(filepath=check_path, monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "## ReduceLR on Plateau : val_loss가 안 줄어들 때 lr을 작게 할 수 있음 (local minima 대처방법)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor = factor,   # patience 만큼 기다리다가 0.1이면 학습률을 0.1배로 줄여버림 \n",
        "                        patience = red_pat, mode = 'min', verbose = 1,\n",
        "                        min_lr = minimumlr\n",
        "                        )\n",
        "\n",
        "from keras import optimizers\n",
        "optimizer = optimizers.Adam(\n",
        "    lr=initial_rate,\n",
        ")\n",
        "\n",
        "## compile model\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "\n",
        "nn_model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              #metrics=['accuracy']\n",
        "              #metrics=[metrics.sparse_categorical_accuracy]\n",
        "              metrics=[CCE]\n",
        "              )\n",
        "\n",
        "## fitting model\n",
        "hist = nn_model.fit(  train_input, train_target,validation_data=[cv_input, cv_target],\n",
        "                    batch_size=batchsize,\n",
        "                    epochs=epoch,\n",
        "                    callbacks = [es \n",
        "                                 #,mc\n",
        "                                 ,rlr\n",
        "                                ] )\n",
        "\n",
        "## save model\n",
        "model_json = nn_model.to_json()\n",
        "with open(json_path, \"w\") as json_file : \n",
        "    json_file.write(model_json)\n",
        "## model weight save\n",
        "nn_model.save_weights(weight_path)\n",
        "print(\"모델저장완료\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"Loss와 ACC에 대한 Plot을 그립니다\")\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "loss_ax.legend(loc='lower left')\n",
        "\n",
        "#acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "#acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "#acc_ax.set_ylabel('accuracy')\n",
        "#acc_ax.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## evaluate model\n",
        "print('기존nn모델의 train loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(train_input, train_target, batch_size=batchsize, verbose=0)\n",
        "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
        "## evaluate model\n",
        "print('기존nn모델의 valid loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(cv_input, cv_target, batch_size=batchsize, verbose=0)\n",
        "print(\"valid, loss and metric: {}\".format(loss_and_metric))\n",
        "## model weight save ## 기존 모델의 가중치 저장\n",
        "#nn_model.save_weights(weight_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 184521 samples, validate on 15377 samples\n",
            "Epoch 1/800\n",
            "184521/184521 [==============================] - 7s 39us/step - loss: 1.3057 - sparse_categorical_crossentropy: 1.3057 - val_loss: 0.8896 - val_sparse_categorical_crossentropy: 0.8896\n",
            "Epoch 2/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.9065 - sparse_categorical_crossentropy: 0.9065 - val_loss: 0.7538 - val_sparse_categorical_crossentropy: 0.7538\n",
            "Epoch 3/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.7954 - sparse_categorical_crossentropy: 0.7954 - val_loss: 0.6923 - val_sparse_categorical_crossentropy: 0.6923\n",
            "Epoch 4/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.7369 - sparse_categorical_crossentropy: 0.7369 - val_loss: 0.6533 - val_sparse_categorical_crossentropy: 0.6533\n",
            "Epoch 5/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.7018 - sparse_categorical_crossentropy: 0.7018 - val_loss: 0.6239 - val_sparse_categorical_crossentropy: 0.6239\n",
            "Epoch 6/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.6716 - sparse_categorical_crossentropy: 0.6716 - val_loss: 0.6058 - val_sparse_categorical_crossentropy: 0.6058\n",
            "Epoch 7/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.6517 - sparse_categorical_crossentropy: 0.6517 - val_loss: 0.5886 - val_sparse_categorical_crossentropy: 0.5886\n",
            "Epoch 8/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.6366 - sparse_categorical_crossentropy: 0.6366 - val_loss: 0.5731 - val_sparse_categorical_crossentropy: 0.5731\n",
            "Epoch 9/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.6275 - sparse_categorical_crossentropy: 0.6275 - val_loss: 0.5516 - val_sparse_categorical_crossentropy: 0.5516\n",
            "Epoch 10/800\n",
            "184521/184521 [==============================] - 5s 29us/step - loss: 0.6116 - sparse_categorical_crossentropy: 0.6116 - val_loss: 0.5410 - val_sparse_categorical_crossentropy: 0.5410\n",
            "Epoch 11/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.6017 - sparse_categorical_crossentropy: 0.6017 - val_loss: 0.5304 - val_sparse_categorical_crossentropy: 0.5304\n",
            "Epoch 12/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5942 - sparse_categorical_crossentropy: 0.5942 - val_loss: 0.5490 - val_sparse_categorical_crossentropy: 0.5490\n",
            "Epoch 13/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5885 - sparse_categorical_crossentropy: 0.5885 - val_loss: 0.5147 - val_sparse_categorical_crossentropy: 0.5147\n",
            "Epoch 14/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5821 - sparse_categorical_crossentropy: 0.5821 - val_loss: 0.5226 - val_sparse_categorical_crossentropy: 0.5226\n",
            "Epoch 15/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5773 - sparse_categorical_crossentropy: 0.5773 - val_loss: 0.5134 - val_sparse_categorical_crossentropy: 0.5134\n",
            "Epoch 16/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5707 - sparse_categorical_crossentropy: 0.5707 - val_loss: 0.5253 - val_sparse_categorical_crossentropy: 0.5253\n",
            "Epoch 17/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5653 - sparse_categorical_crossentropy: 0.5653 - val_loss: 0.5002 - val_sparse_categorical_crossentropy: 0.5002\n",
            "Epoch 18/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5595 - sparse_categorical_crossentropy: 0.5595 - val_loss: 0.5108 - val_sparse_categorical_crossentropy: 0.5108\n",
            "Epoch 19/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5546 - sparse_categorical_crossentropy: 0.5546 - val_loss: 0.5067 - val_sparse_categorical_crossentropy: 0.5067\n",
            "Epoch 20/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5498 - sparse_categorical_crossentropy: 0.5498 - val_loss: 0.4946 - val_sparse_categorical_crossentropy: 0.4946\n",
            "Epoch 21/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5451 - sparse_categorical_crossentropy: 0.5451 - val_loss: 0.5053 - val_sparse_categorical_crossentropy: 0.5053\n",
            "Epoch 22/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5410 - sparse_categorical_crossentropy: 0.5410 - val_loss: 0.4901 - val_sparse_categorical_crossentropy: 0.4901\n",
            "Epoch 23/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5415 - sparse_categorical_crossentropy: 0.5415 - val_loss: 0.4850 - val_sparse_categorical_crossentropy: 0.4850\n",
            "Epoch 24/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5365 - sparse_categorical_crossentropy: 0.5365 - val_loss: 0.4787 - val_sparse_categorical_crossentropy: 0.4787\n",
            "Epoch 25/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5323 - sparse_categorical_crossentropy: 0.5323 - val_loss: 0.4772 - val_sparse_categorical_crossentropy: 0.4772\n",
            "Epoch 26/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5297 - sparse_categorical_crossentropy: 0.5297 - val_loss: 0.4784 - val_sparse_categorical_crossentropy: 0.4784\n",
            "Epoch 27/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5260 - sparse_categorical_crossentropy: 0.5260 - val_loss: 0.4848 - val_sparse_categorical_crossentropy: 0.4848\n",
            "Epoch 28/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5253 - sparse_categorical_crossentropy: 0.5253 - val_loss: 0.4693 - val_sparse_categorical_crossentropy: 0.4693\n",
            "Epoch 29/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5214 - sparse_categorical_crossentropy: 0.5214 - val_loss: 0.4736 - val_sparse_categorical_crossentropy: 0.4736\n",
            "Epoch 30/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5203 - sparse_categorical_crossentropy: 0.5203 - val_loss: 0.4820 - val_sparse_categorical_crossentropy: 0.4820\n",
            "Epoch 31/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5177 - sparse_categorical_crossentropy: 0.5177 - val_loss: 0.4629 - val_sparse_categorical_crossentropy: 0.4629\n",
            "Epoch 32/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5137 - sparse_categorical_crossentropy: 0.5137 - val_loss: 0.4690 - val_sparse_categorical_crossentropy: 0.4690\n",
            "Epoch 33/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5131 - sparse_categorical_crossentropy: 0.5131 - val_loss: 0.4656 - val_sparse_categorical_crossentropy: 0.4656\n",
            "Epoch 34/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5102 - sparse_categorical_crossentropy: 0.5102 - val_loss: 0.4601 - val_sparse_categorical_crossentropy: 0.4601\n",
            "Epoch 35/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5090 - sparse_categorical_crossentropy: 0.5090 - val_loss: 0.4599 - val_sparse_categorical_crossentropy: 0.4599\n",
            "Epoch 36/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5064 - sparse_categorical_crossentropy: 0.5064 - val_loss: 0.4644 - val_sparse_categorical_crossentropy: 0.4644\n",
            "Epoch 37/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5037 - sparse_categorical_crossentropy: 0.5037 - val_loss: 0.4536 - val_sparse_categorical_crossentropy: 0.4536\n",
            "Epoch 38/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5036 - sparse_categorical_crossentropy: 0.5036 - val_loss: 0.4589 - val_sparse_categorical_crossentropy: 0.4589\n",
            "Epoch 39/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5024 - sparse_categorical_crossentropy: 0.5024 - val_loss: 0.4528 - val_sparse_categorical_crossentropy: 0.4528\n",
            "Epoch 40/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4998 - sparse_categorical_crossentropy: 0.4998 - val_loss: 0.4512 - val_sparse_categorical_crossentropy: 0.4512\n",
            "Epoch 41/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4984 - sparse_categorical_crossentropy: 0.4984 - val_loss: 0.4475 - val_sparse_categorical_crossentropy: 0.4475\n",
            "Epoch 42/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4976 - sparse_categorical_crossentropy: 0.4976 - val_loss: 0.4479 - val_sparse_categorical_crossentropy: 0.4479\n",
            "Epoch 43/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4954 - sparse_categorical_crossentropy: 0.4954 - val_loss: 0.4531 - val_sparse_categorical_crossentropy: 0.4531\n",
            "Epoch 44/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4953 - sparse_categorical_crossentropy: 0.4953 - val_loss: 0.4542 - val_sparse_categorical_crossentropy: 0.4542\n",
            "Epoch 45/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4916 - sparse_categorical_crossentropy: 0.4916 - val_loss: 0.4506 - val_sparse_categorical_crossentropy: 0.4506\n",
            "Epoch 46/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4914 - sparse_categorical_crossentropy: 0.4914 - val_loss: 0.4459 - val_sparse_categorical_crossentropy: 0.4459\n",
            "Epoch 47/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4924 - sparse_categorical_crossentropy: 0.4924 - val_loss: 0.4499 - val_sparse_categorical_crossentropy: 0.4499\n",
            "Epoch 48/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4885 - sparse_categorical_crossentropy: 0.4885 - val_loss: 0.4497 - val_sparse_categorical_crossentropy: 0.4497\n",
            "Epoch 49/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4885 - sparse_categorical_crossentropy: 0.4885 - val_loss: 0.4474 - val_sparse_categorical_crossentropy: 0.4474\n",
            "Epoch 50/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4874 - sparse_categorical_crossentropy: 0.4874 - val_loss: 0.4402 - val_sparse_categorical_crossentropy: 0.4402\n",
            "Epoch 51/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4860 - sparse_categorical_crossentropy: 0.4860 - val_loss: 0.4446 - val_sparse_categorical_crossentropy: 0.4446\n",
            "Epoch 52/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4828 - sparse_categorical_crossentropy: 0.4828 - val_loss: 0.4357 - val_sparse_categorical_crossentropy: 0.4357\n",
            "Epoch 53/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4848 - sparse_categorical_crossentropy: 0.4848 - val_loss: 0.4342 - val_sparse_categorical_crossentropy: 0.4342\n",
            "Epoch 54/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4813 - sparse_categorical_crossentropy: 0.4813 - val_loss: 0.4360 - val_sparse_categorical_crossentropy: 0.4360\n",
            "Epoch 55/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4816 - sparse_categorical_crossentropy: 0.4816 - val_loss: 0.4327 - val_sparse_categorical_crossentropy: 0.4327\n",
            "Epoch 56/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4801 - sparse_categorical_crossentropy: 0.4801 - val_loss: 0.4325 - val_sparse_categorical_crossentropy: 0.4325\n",
            "Epoch 57/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4793 - sparse_categorical_crossentropy: 0.4793 - val_loss: 0.4354 - val_sparse_categorical_crossentropy: 0.4354\n",
            "Epoch 58/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4797 - sparse_categorical_crossentropy: 0.4797 - val_loss: 0.4342 - val_sparse_categorical_crossentropy: 0.4342\n",
            "Epoch 59/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4772 - sparse_categorical_crossentropy: 0.4772 - val_loss: 0.4341 - val_sparse_categorical_crossentropy: 0.4341\n",
            "Epoch 60/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4763 - sparse_categorical_crossentropy: 0.4763 - val_loss: 0.4306 - val_sparse_categorical_crossentropy: 0.4306\n",
            "Epoch 61/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4747 - sparse_categorical_crossentropy: 0.4747 - val_loss: 0.4443 - val_sparse_categorical_crossentropy: 0.4443\n",
            "Epoch 62/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4750 - sparse_categorical_crossentropy: 0.4750 - val_loss: 0.4320 - val_sparse_categorical_crossentropy: 0.4320\n",
            "Epoch 63/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4720 - sparse_categorical_crossentropy: 0.4720 - val_loss: 0.4341 - val_sparse_categorical_crossentropy: 0.4341\n",
            "Epoch 64/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4731 - sparse_categorical_crossentropy: 0.4731 - val_loss: 0.4289 - val_sparse_categorical_crossentropy: 0.4289\n",
            "Epoch 65/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4721 - sparse_categorical_crossentropy: 0.4721 - val_loss: 0.4280 - val_sparse_categorical_crossentropy: 0.4280\n",
            "Epoch 66/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4723 - sparse_categorical_crossentropy: 0.4723 - val_loss: 0.4256 - val_sparse_categorical_crossentropy: 0.4256\n",
            "Epoch 67/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4697 - sparse_categorical_crossentropy: 0.4697 - val_loss: 0.4241 - val_sparse_categorical_crossentropy: 0.4241\n",
            "Epoch 68/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4683 - sparse_categorical_crossentropy: 0.4683 - val_loss: 0.4274 - val_sparse_categorical_crossentropy: 0.4274\n",
            "Epoch 69/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4692 - sparse_categorical_crossentropy: 0.4692 - val_loss: 0.4354 - val_sparse_categorical_crossentropy: 0.4354\n",
            "Epoch 70/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4674 - sparse_categorical_crossentropy: 0.4674 - val_loss: 0.4288 - val_sparse_categorical_crossentropy: 0.4288\n",
            "Epoch 71/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4685 - sparse_categorical_crossentropy: 0.4685 - val_loss: 0.4233 - val_sparse_categorical_crossentropy: 0.4233\n",
            "Epoch 72/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4664 - sparse_categorical_crossentropy: 0.4664 - val_loss: 0.4355 - val_sparse_categorical_crossentropy: 0.4355\n",
            "Epoch 73/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4660 - sparse_categorical_crossentropy: 0.4660 - val_loss: 0.4247 - val_sparse_categorical_crossentropy: 0.4247\n",
            "Epoch 74/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4638 - sparse_categorical_crossentropy: 0.4638 - val_loss: 0.4351 - val_sparse_categorical_crossentropy: 0.4351\n",
            "Epoch 75/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4655 - sparse_categorical_crossentropy: 0.4655 - val_loss: 0.4282 - val_sparse_categorical_crossentropy: 0.4282\n",
            "Epoch 76/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4643 - sparse_categorical_crossentropy: 0.4643 - val_loss: 0.4258 - val_sparse_categorical_crossentropy: 0.4258\n",
            "Epoch 77/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4628 - sparse_categorical_crossentropy: 0.4628 - val_loss: 0.4291 - val_sparse_categorical_crossentropy: 0.4291\n",
            "Epoch 78/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4619 - sparse_categorical_crossentropy: 0.4619 - val_loss: 0.4288 - val_sparse_categorical_crossentropy: 0.4288\n",
            "Epoch 79/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4635 - sparse_categorical_crossentropy: 0.4635 - val_loss: 0.4213 - val_sparse_categorical_crossentropy: 0.4213\n",
            "Epoch 80/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4597 - sparse_categorical_crossentropy: 0.4597 - val_loss: 0.4281 - val_sparse_categorical_crossentropy: 0.4281\n",
            "Epoch 81/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4598 - sparse_categorical_crossentropy: 0.4598 - val_loss: 0.4272 - val_sparse_categorical_crossentropy: 0.4272\n",
            "Epoch 82/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4593 - sparse_categorical_crossentropy: 0.4593 - val_loss: 0.4215 - val_sparse_categorical_crossentropy: 0.4215\n",
            "Epoch 83/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4595 - sparse_categorical_crossentropy: 0.4595 - val_loss: 0.4189 - val_sparse_categorical_crossentropy: 0.4189\n",
            "Epoch 84/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4594 - sparse_categorical_crossentropy: 0.4594 - val_loss: 0.4267 - val_sparse_categorical_crossentropy: 0.4267\n",
            "Epoch 85/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4585 - sparse_categorical_crossentropy: 0.4585 - val_loss: 0.4239 - val_sparse_categorical_crossentropy: 0.4239\n",
            "Epoch 86/800\n",
            "184521/184521 [==============================] - 5s 29us/step - loss: 0.4586 - sparse_categorical_crossentropy: 0.4586 - val_loss: 0.4193 - val_sparse_categorical_crossentropy: 0.4193\n",
            "Epoch 87/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4554 - sparse_categorical_crossentropy: 0.4554 - val_loss: 0.4156 - val_sparse_categorical_crossentropy: 0.4156\n",
            "Epoch 88/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4545 - sparse_categorical_crossentropy: 0.4545 - val_loss: 0.4148 - val_sparse_categorical_crossentropy: 0.4148\n",
            "Epoch 89/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4544 - sparse_categorical_crossentropy: 0.4544 - val_loss: 0.4210 - val_sparse_categorical_crossentropy: 0.4210\n",
            "Epoch 90/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4558 - sparse_categorical_crossentropy: 0.4558 - val_loss: 0.4190 - val_sparse_categorical_crossentropy: 0.4190\n",
            "Epoch 91/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4534 - sparse_categorical_crossentropy: 0.4534 - val_loss: 0.4160 - val_sparse_categorical_crossentropy: 0.4160\n",
            "Epoch 92/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4552 - sparse_categorical_crossentropy: 0.4552 - val_loss: 0.4182 - val_sparse_categorical_crossentropy: 0.4182\n",
            "Epoch 93/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4538 - sparse_categorical_crossentropy: 0.4538 - val_loss: 0.4172 - val_sparse_categorical_crossentropy: 0.4172\n",
            "Epoch 94/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4534 - sparse_categorical_crossentropy: 0.4534 - val_loss: 0.4223 - val_sparse_categorical_crossentropy: 0.4223\n",
            "Epoch 95/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4519 - sparse_categorical_crossentropy: 0.4519 - val_loss: 0.4190 - val_sparse_categorical_crossentropy: 0.4190\n",
            "Epoch 96/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4522 - sparse_categorical_crossentropy: 0.4522 - val_loss: 0.4151 - val_sparse_categorical_crossentropy: 0.4151\n",
            "Epoch 97/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4536 - sparse_categorical_crossentropy: 0.4536 - val_loss: 0.4145 - val_sparse_categorical_crossentropy: 0.4145\n",
            "Epoch 98/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4519 - sparse_categorical_crossentropy: 0.4519 - val_loss: 0.4142 - val_sparse_categorical_crossentropy: 0.4142\n",
            "Epoch 99/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4492 - sparse_categorical_crossentropy: 0.4492 - val_loss: 0.4116 - val_sparse_categorical_crossentropy: 0.4116\n",
            "Epoch 100/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4497 - sparse_categorical_crossentropy: 0.4497 - val_loss: 0.4139 - val_sparse_categorical_crossentropy: 0.4139\n",
            "Epoch 101/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4495 - sparse_categorical_crossentropy: 0.4495 - val_loss: 0.4207 - val_sparse_categorical_crossentropy: 0.4207\n",
            "Epoch 102/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4512 - sparse_categorical_crossentropy: 0.4512 - val_loss: 0.4129 - val_sparse_categorical_crossentropy: 0.4129\n",
            "Epoch 103/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4487 - sparse_categorical_crossentropy: 0.4487 - val_loss: 0.4187 - val_sparse_categorical_crossentropy: 0.4187\n",
            "Epoch 104/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4485 - sparse_categorical_crossentropy: 0.4485 - val_loss: 0.4107 - val_sparse_categorical_crossentropy: 0.4107\n",
            "Epoch 105/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4456 - sparse_categorical_crossentropy: 0.4456 - val_loss: 0.4117 - val_sparse_categorical_crossentropy: 0.4117\n",
            "Epoch 106/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4475 - sparse_categorical_crossentropy: 0.4475 - val_loss: 0.4160 - val_sparse_categorical_crossentropy: 0.4160\n",
            "Epoch 107/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4469 - sparse_categorical_crossentropy: 0.4469 - val_loss: 0.4148 - val_sparse_categorical_crossentropy: 0.4148\n",
            "Epoch 108/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4464 - sparse_categorical_crossentropy: 0.4464 - val_loss: 0.4148 - val_sparse_categorical_crossentropy: 0.4148\n",
            "Epoch 109/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4468 - sparse_categorical_crossentropy: 0.4468 - val_loss: 0.4098 - val_sparse_categorical_crossentropy: 0.4098\n",
            "Epoch 110/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4451 - sparse_categorical_crossentropy: 0.4451 - val_loss: 0.4099 - val_sparse_categorical_crossentropy: 0.4099\n",
            "Epoch 111/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4464 - sparse_categorical_crossentropy: 0.4464 - val_loss: 0.4084 - val_sparse_categorical_crossentropy: 0.4084\n",
            "Epoch 112/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4458 - sparse_categorical_crossentropy: 0.4458 - val_loss: 0.4107 - val_sparse_categorical_crossentropy: 0.4107\n",
            "Epoch 113/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4429 - sparse_categorical_crossentropy: 0.4429 - val_loss: 0.4081 - val_sparse_categorical_crossentropy: 0.4081\n",
            "Epoch 114/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4440 - sparse_categorical_crossentropy: 0.4440 - val_loss: 0.4073 - val_sparse_categorical_crossentropy: 0.4073\n",
            "Epoch 115/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4430 - sparse_categorical_crossentropy: 0.4430 - val_loss: 0.4138 - val_sparse_categorical_crossentropy: 0.4138\n",
            "Epoch 116/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4427 - sparse_categorical_crossentropy: 0.4427 - val_loss: 0.4153 - val_sparse_categorical_crossentropy: 0.4153\n",
            "Epoch 117/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4431 - sparse_categorical_crossentropy: 0.4431 - val_loss: 0.4068 - val_sparse_categorical_crossentropy: 0.4068\n",
            "Epoch 118/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4412 - sparse_categorical_crossentropy: 0.4412 - val_loss: 0.4114 - val_sparse_categorical_crossentropy: 0.4114\n",
            "Epoch 119/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4429 - sparse_categorical_crossentropy: 0.4429 - val_loss: 0.4083 - val_sparse_categorical_crossentropy: 0.4083\n",
            "Epoch 120/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4422 - sparse_categorical_crossentropy: 0.4422 - val_loss: 0.4092 - val_sparse_categorical_crossentropy: 0.4092\n",
            "Epoch 121/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4412 - sparse_categorical_crossentropy: 0.4412 - val_loss: 0.4059 - val_sparse_categorical_crossentropy: 0.4059\n",
            "Epoch 122/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4407 - sparse_categorical_crossentropy: 0.4407 - val_loss: 0.4075 - val_sparse_categorical_crossentropy: 0.4075\n",
            "Epoch 123/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4411 - sparse_categorical_crossentropy: 0.4411 - val_loss: 0.4065 - val_sparse_categorical_crossentropy: 0.4065\n",
            "Epoch 124/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4425 - sparse_categorical_crossentropy: 0.4425 - val_loss: 0.4102 - val_sparse_categorical_crossentropy: 0.4102\n",
            "Epoch 125/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4399 - sparse_categorical_crossentropy: 0.4399 - val_loss: 0.4078 - val_sparse_categorical_crossentropy: 0.4078\n",
            "Epoch 126/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4402 - sparse_categorical_crossentropy: 0.4402 - val_loss: 0.4071 - val_sparse_categorical_crossentropy: 0.4071\n",
            "Epoch 127/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4399 - sparse_categorical_crossentropy: 0.4399 - val_loss: 0.4091 - val_sparse_categorical_crossentropy: 0.4091\n",
            "Epoch 128/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4408 - sparse_categorical_crossentropy: 0.4408 - val_loss: 0.4076 - val_sparse_categorical_crossentropy: 0.4076\n",
            "Epoch 129/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4378 - sparse_categorical_crossentropy: 0.4378 - val_loss: 0.4069 - val_sparse_categorical_crossentropy: 0.4069\n",
            "Epoch 130/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4400 - sparse_categorical_crossentropy: 0.4400 - val_loss: 0.4069 - val_sparse_categorical_crossentropy: 0.4069\n",
            "Epoch 131/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4377 - sparse_categorical_crossentropy: 0.4377 - val_loss: 0.4075 - val_sparse_categorical_crossentropy: 0.4075\n",
            "Epoch 132/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4386 - sparse_categorical_crossentropy: 0.4386 - val_loss: 0.4099 - val_sparse_categorical_crossentropy: 0.4099\n",
            "Epoch 133/800\n",
            "184521/184521 [==============================] - 5s 29us/step - loss: 0.4369 - sparse_categorical_crossentropy: 0.4369 - val_loss: 0.4064 - val_sparse_categorical_crossentropy: 0.4064\n",
            "Epoch 134/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4382 - sparse_categorical_crossentropy: 0.4382 - val_loss: 0.4056 - val_sparse_categorical_crossentropy: 0.4056\n",
            "Epoch 135/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4375 - sparse_categorical_crossentropy: 0.4375 - val_loss: 0.4092 - val_sparse_categorical_crossentropy: 0.4092\n",
            "Epoch 136/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4370 - sparse_categorical_crossentropy: 0.4370 - val_loss: 0.4025 - val_sparse_categorical_crossentropy: 0.4025\n",
            "Epoch 137/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4357 - sparse_categorical_crossentropy: 0.4357 - val_loss: 0.4154 - val_sparse_categorical_crossentropy: 0.4154\n",
            "Epoch 138/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4381 - sparse_categorical_crossentropy: 0.4381 - val_loss: 0.4084 - val_sparse_categorical_crossentropy: 0.4084\n",
            "Epoch 139/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4342 - sparse_categorical_crossentropy: 0.4342 - val_loss: 0.4151 - val_sparse_categorical_crossentropy: 0.4151\n",
            "Epoch 140/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4356 - sparse_categorical_crossentropy: 0.4356 - val_loss: 0.4053 - val_sparse_categorical_crossentropy: 0.4053\n",
            "Epoch 141/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4354 - sparse_categorical_crossentropy: 0.4354 - val_loss: 0.4017 - val_sparse_categorical_crossentropy: 0.4017\n",
            "Epoch 142/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4354 - sparse_categorical_crossentropy: 0.4354 - val_loss: 0.4057 - val_sparse_categorical_crossentropy: 0.4057\n",
            "Epoch 143/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4358 - sparse_categorical_crossentropy: 0.4358 - val_loss: 0.3994 - val_sparse_categorical_crossentropy: 0.3994\n",
            "Epoch 144/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4339 - sparse_categorical_crossentropy: 0.4339 - val_loss: 0.4037 - val_sparse_categorical_crossentropy: 0.4037\n",
            "Epoch 145/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4349 - sparse_categorical_crossentropy: 0.4349 - val_loss: 0.4092 - val_sparse_categorical_crossentropy: 0.4092\n",
            "Epoch 146/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4333 - sparse_categorical_crossentropy: 0.4333 - val_loss: 0.4055 - val_sparse_categorical_crossentropy: 0.4055\n",
            "Epoch 147/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4338 - sparse_categorical_crossentropy: 0.4338 - val_loss: 0.4051 - val_sparse_categorical_crossentropy: 0.4051\n",
            "Epoch 148/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4326 - sparse_categorical_crossentropy: 0.4326 - val_loss: 0.4029 - val_sparse_categorical_crossentropy: 0.4029\n",
            "Epoch 149/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4330 - sparse_categorical_crossentropy: 0.4330 - val_loss: 0.3991 - val_sparse_categorical_crossentropy: 0.3991\n",
            "Epoch 150/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4334 - sparse_categorical_crossentropy: 0.4334 - val_loss: 0.4019 - val_sparse_categorical_crossentropy: 0.4019\n",
            "Epoch 151/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4324 - sparse_categorical_crossentropy: 0.4324 - val_loss: 0.4055 - val_sparse_categorical_crossentropy: 0.4055\n",
            "Epoch 152/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4319 - sparse_categorical_crossentropy: 0.4319 - val_loss: 0.4048 - val_sparse_categorical_crossentropy: 0.4048\n",
            "Epoch 153/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4318 - sparse_categorical_crossentropy: 0.4318 - val_loss: 0.4009 - val_sparse_categorical_crossentropy: 0.4009\n",
            "Epoch 154/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4316 - sparse_categorical_crossentropy: 0.4316 - val_loss: 0.3998 - val_sparse_categorical_crossentropy: 0.3998\n",
            "Epoch 155/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4312 - sparse_categorical_crossentropy: 0.4312 - val_loss: 0.4004 - val_sparse_categorical_crossentropy: 0.4004\n",
            "Epoch 156/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4312 - sparse_categorical_crossentropy: 0.4312 - val_loss: 0.4062 - val_sparse_categorical_crossentropy: 0.4062\n",
            "Epoch 157/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4315 - sparse_categorical_crossentropy: 0.4315 - val_loss: 0.3987 - val_sparse_categorical_crossentropy: 0.3987\n",
            "Epoch 158/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4305 - sparse_categorical_crossentropy: 0.4305 - val_loss: 0.4086 - val_sparse_categorical_crossentropy: 0.4086\n",
            "Epoch 159/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4312 - sparse_categorical_crossentropy: 0.4312 - val_loss: 0.4017 - val_sparse_categorical_crossentropy: 0.4017\n",
            "Epoch 160/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4316 - sparse_categorical_crossentropy: 0.4316 - val_loss: 0.4030 - val_sparse_categorical_crossentropy: 0.4030\n",
            "Epoch 161/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4297 - sparse_categorical_crossentropy: 0.4297 - val_loss: 0.4034 - val_sparse_categorical_crossentropy: 0.4034\n",
            "Epoch 162/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4299 - sparse_categorical_crossentropy: 0.4299 - val_loss: 0.3983 - val_sparse_categorical_crossentropy: 0.3983\n",
            "Epoch 163/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4291 - sparse_categorical_crossentropy: 0.4291 - val_loss: 0.4001 - val_sparse_categorical_crossentropy: 0.4001\n",
            "Epoch 164/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4314 - sparse_categorical_crossentropy: 0.4314 - val_loss: 0.3993 - val_sparse_categorical_crossentropy: 0.3993\n",
            "Epoch 165/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4284 - sparse_categorical_crossentropy: 0.4284 - val_loss: 0.4028 - val_sparse_categorical_crossentropy: 0.4028\n",
            "Epoch 166/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4285 - sparse_categorical_crossentropy: 0.4285 - val_loss: 0.4031 - val_sparse_categorical_crossentropy: 0.4031\n",
            "Epoch 167/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4280 - sparse_categorical_crossentropy: 0.4280 - val_loss: 0.3986 - val_sparse_categorical_crossentropy: 0.3986\n",
            "Epoch 168/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4277 - sparse_categorical_crossentropy: 0.4277 - val_loss: 0.3991 - val_sparse_categorical_crossentropy: 0.3991\n",
            "Epoch 169/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4271 - sparse_categorical_crossentropy: 0.4271 - val_loss: 0.4016 - val_sparse_categorical_crossentropy: 0.4016\n",
            "Epoch 170/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4271 - sparse_categorical_crossentropy: 0.4271 - val_loss: 0.3998 - val_sparse_categorical_crossentropy: 0.3998\n",
            "Epoch 171/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4289 - sparse_categorical_crossentropy: 0.4289 - val_loss: 0.3980 - val_sparse_categorical_crossentropy: 0.3980\n",
            "Epoch 172/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4289 - sparse_categorical_crossentropy: 0.4289 - val_loss: 0.4011 - val_sparse_categorical_crossentropy: 0.4011\n",
            "Epoch 173/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4263 - sparse_categorical_crossentropy: 0.4263 - val_loss: 0.3987 - val_sparse_categorical_crossentropy: 0.3987\n",
            "Epoch 174/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4247 - sparse_categorical_crossentropy: 0.4247 - val_loss: 0.3974 - val_sparse_categorical_crossentropy: 0.3974\n",
            "Epoch 175/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4273 - sparse_categorical_crossentropy: 0.4273 - val_loss: 0.3996 - val_sparse_categorical_crossentropy: 0.3996\n",
            "Epoch 176/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4253 - sparse_categorical_crossentropy: 0.4253 - val_loss: 0.4025 - val_sparse_categorical_crossentropy: 0.4025\n",
            "Epoch 177/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4270 - sparse_categorical_crossentropy: 0.4270 - val_loss: 0.3964 - val_sparse_categorical_crossentropy: 0.3964\n",
            "Epoch 178/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4272 - sparse_categorical_crossentropy: 0.4272 - val_loss: 0.3965 - val_sparse_categorical_crossentropy: 0.3965\n",
            "Epoch 179/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4249 - sparse_categorical_crossentropy: 0.4249 - val_loss: 0.3972 - val_sparse_categorical_crossentropy: 0.3972\n",
            "Epoch 180/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4261 - sparse_categorical_crossentropy: 0.4261 - val_loss: 0.3977 - val_sparse_categorical_crossentropy: 0.3977\n",
            "Epoch 181/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4265 - sparse_categorical_crossentropy: 0.4265 - val_loss: 0.4052 - val_sparse_categorical_crossentropy: 0.4052\n",
            "Epoch 182/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4264 - sparse_categorical_crossentropy: 0.4264 - val_loss: 0.3950 - val_sparse_categorical_crossentropy: 0.3950\n",
            "Epoch 183/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4266 - sparse_categorical_crossentropy: 0.4266 - val_loss: 0.3989 - val_sparse_categorical_crossentropy: 0.3989\n",
            "Epoch 184/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4253 - sparse_categorical_crossentropy: 0.4253 - val_loss: 0.3966 - val_sparse_categorical_crossentropy: 0.3966\n",
            "Epoch 185/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4246 - sparse_categorical_crossentropy: 0.4246 - val_loss: 0.3995 - val_sparse_categorical_crossentropy: 0.3995\n",
            "Epoch 186/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4253 - sparse_categorical_crossentropy: 0.4253 - val_loss: 0.3986 - val_sparse_categorical_crossentropy: 0.3986\n",
            "Epoch 187/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4247 - sparse_categorical_crossentropy: 0.4247 - val_loss: 0.3964 - val_sparse_categorical_crossentropy: 0.3964\n",
            "Epoch 188/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4239 - sparse_categorical_crossentropy: 0.4239 - val_loss: 0.3932 - val_sparse_categorical_crossentropy: 0.3932\n",
            "Epoch 189/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4228 - sparse_categorical_crossentropy: 0.4228 - val_loss: 0.3976 - val_sparse_categorical_crossentropy: 0.3976\n",
            "Epoch 190/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4242 - sparse_categorical_crossentropy: 0.4242 - val_loss: 0.3985 - val_sparse_categorical_crossentropy: 0.3985\n",
            "Epoch 191/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4231 - sparse_categorical_crossentropy: 0.4231 - val_loss: 0.4007 - val_sparse_categorical_crossentropy: 0.4007\n",
            "Epoch 192/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4250 - sparse_categorical_crossentropy: 0.4250 - val_loss: 0.3943 - val_sparse_categorical_crossentropy: 0.3943\n",
            "Epoch 193/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4231 - sparse_categorical_crossentropy: 0.4231 - val_loss: 0.3957 - val_sparse_categorical_crossentropy: 0.3957\n",
            "Epoch 194/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4232 - sparse_categorical_crossentropy: 0.4232 - val_loss: 0.3960 - val_sparse_categorical_crossentropy: 0.3960\n",
            "Epoch 195/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4212 - sparse_categorical_crossentropy: 0.4212 - val_loss: 0.3928 - val_sparse_categorical_crossentropy: 0.3928\n",
            "Epoch 196/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4233 - sparse_categorical_crossentropy: 0.4233 - val_loss: 0.3947 - val_sparse_categorical_crossentropy: 0.3947\n",
            "Epoch 197/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4227 - sparse_categorical_crossentropy: 0.4227 - val_loss: 0.3955 - val_sparse_categorical_crossentropy: 0.3955\n",
            "Epoch 198/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4220 - sparse_categorical_crossentropy: 0.4220 - val_loss: 0.3978 - val_sparse_categorical_crossentropy: 0.3978\n",
            "Epoch 199/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4208 - sparse_categorical_crossentropy: 0.4208 - val_loss: 0.3942 - val_sparse_categorical_crossentropy: 0.3942\n",
            "Epoch 200/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4218 - sparse_categorical_crossentropy: 0.4218 - val_loss: 0.4046 - val_sparse_categorical_crossentropy: 0.4046\n",
            "Epoch 201/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4226 - sparse_categorical_crossentropy: 0.4226 - val_loss: 0.3951 - val_sparse_categorical_crossentropy: 0.3951\n",
            "Epoch 202/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4212 - sparse_categorical_crossentropy: 0.4212 - val_loss: 0.3937 - val_sparse_categorical_crossentropy: 0.3937\n",
            "Epoch 203/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4212 - sparse_categorical_crossentropy: 0.4212 - val_loss: 0.3940 - val_sparse_categorical_crossentropy: 0.3940\n",
            "Epoch 204/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4211 - sparse_categorical_crossentropy: 0.4211 - val_loss: 0.3969 - val_sparse_categorical_crossentropy: 0.3969\n",
            "Epoch 205/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4206 - sparse_categorical_crossentropy: 0.4206 - val_loss: 0.3994 - val_sparse_categorical_crossentropy: 0.3994\n",
            "Epoch 206/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4199 - sparse_categorical_crossentropy: 0.4199 - val_loss: 0.3966 - val_sparse_categorical_crossentropy: 0.3966\n",
            "Epoch 207/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4199 - sparse_categorical_crossentropy: 0.4199 - val_loss: 0.3947 - val_sparse_categorical_crossentropy: 0.3947\n",
            "Epoch 208/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4223 - sparse_categorical_crossentropy: 0.4223 - val_loss: 0.3949 - val_sparse_categorical_crossentropy: 0.3949\n",
            "Epoch 209/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4198 - sparse_categorical_crossentropy: 0.4198 - val_loss: 0.3927 - val_sparse_categorical_crossentropy: 0.3927\n",
            "Epoch 210/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4193 - sparse_categorical_crossentropy: 0.4193 - val_loss: 0.3920 - val_sparse_categorical_crossentropy: 0.3920\n",
            "Epoch 211/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4207 - sparse_categorical_crossentropy: 0.4207 - val_loss: 0.3949 - val_sparse_categorical_crossentropy: 0.3949\n",
            "Epoch 212/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4198 - sparse_categorical_crossentropy: 0.4198 - val_loss: 0.3962 - val_sparse_categorical_crossentropy: 0.3962\n",
            "Epoch 213/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4199 - sparse_categorical_crossentropy: 0.4199 - val_loss: 0.3985 - val_sparse_categorical_crossentropy: 0.3985\n",
            "Epoch 214/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4189 - sparse_categorical_crossentropy: 0.4189 - val_loss: 0.3941 - val_sparse_categorical_crossentropy: 0.3941\n",
            "Epoch 215/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4199 - sparse_categorical_crossentropy: 0.4199 - val_loss: 0.3940 - val_sparse_categorical_crossentropy: 0.3940\n",
            "Epoch 216/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4178 - sparse_categorical_crossentropy: 0.4178 - val_loss: 0.3909 - val_sparse_categorical_crossentropy: 0.3909\n",
            "Epoch 217/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4185 - sparse_categorical_crossentropy: 0.4185 - val_loss: 0.3920 - val_sparse_categorical_crossentropy: 0.3920\n",
            "Epoch 218/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4185 - sparse_categorical_crossentropy: 0.4185 - val_loss: 0.3940 - val_sparse_categorical_crossentropy: 0.3940\n",
            "Epoch 219/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4182 - sparse_categorical_crossentropy: 0.4182 - val_loss: 0.3935 - val_sparse_categorical_crossentropy: 0.3935\n",
            "Epoch 220/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4175 - sparse_categorical_crossentropy: 0.4175 - val_loss: 0.3953 - val_sparse_categorical_crossentropy: 0.3953\n",
            "Epoch 221/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4181 - sparse_categorical_crossentropy: 0.4181 - val_loss: 0.3923 - val_sparse_categorical_crossentropy: 0.3923\n",
            "Epoch 222/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4190 - sparse_categorical_crossentropy: 0.4190 - val_loss: 0.3934 - val_sparse_categorical_crossentropy: 0.3934\n",
            "Epoch 223/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4189 - sparse_categorical_crossentropy: 0.4189 - val_loss: 0.3910 - val_sparse_categorical_crossentropy: 0.3910\n",
            "Epoch 224/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4163 - sparse_categorical_crossentropy: 0.4163 - val_loss: 0.3919 - val_sparse_categorical_crossentropy: 0.3919\n",
            "Epoch 225/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4175 - sparse_categorical_crossentropy: 0.4175 - val_loss: 0.3955 - val_sparse_categorical_crossentropy: 0.3955\n",
            "Epoch 226/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4169 - sparse_categorical_crossentropy: 0.4169 - val_loss: 0.3937 - val_sparse_categorical_crossentropy: 0.3937\n",
            "Epoch 227/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4170 - sparse_categorical_crossentropy: 0.4170 - val_loss: 0.3974 - val_sparse_categorical_crossentropy: 0.3974\n",
            "Epoch 228/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4175 - sparse_categorical_crossentropy: 0.4175 - val_loss: 0.3911 - val_sparse_categorical_crossentropy: 0.3911\n",
            "Epoch 229/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4163 - sparse_categorical_crossentropy: 0.4163 - val_loss: 0.3895 - val_sparse_categorical_crossentropy: 0.3895\n",
            "Epoch 230/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4171 - sparse_categorical_crossentropy: 0.4171 - val_loss: 0.3927 - val_sparse_categorical_crossentropy: 0.3927\n",
            "Epoch 231/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4162 - sparse_categorical_crossentropy: 0.4162 - val_loss: 0.3917 - val_sparse_categorical_crossentropy: 0.3917\n",
            "Epoch 232/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4163 - sparse_categorical_crossentropy: 0.4163 - val_loss: 0.3900 - val_sparse_categorical_crossentropy: 0.3900\n",
            "Epoch 233/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4157 - sparse_categorical_crossentropy: 0.4157 - val_loss: 0.3903 - val_sparse_categorical_crossentropy: 0.3903\n",
            "Epoch 234/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4149 - sparse_categorical_crossentropy: 0.4149 - val_loss: 0.3934 - val_sparse_categorical_crossentropy: 0.3934\n",
            "Epoch 235/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4165 - sparse_categorical_crossentropy: 0.4165 - val_loss: 0.3957 - val_sparse_categorical_crossentropy: 0.3957\n",
            "Epoch 236/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4170 - sparse_categorical_crossentropy: 0.4170 - val_loss: 0.3924 - val_sparse_categorical_crossentropy: 0.3924\n",
            "Epoch 237/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4151 - sparse_categorical_crossentropy: 0.4151 - val_loss: 0.3896 - val_sparse_categorical_crossentropy: 0.3896\n",
            "Epoch 238/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4158 - sparse_categorical_crossentropy: 0.4158 - val_loss: 0.3884 - val_sparse_categorical_crossentropy: 0.3884\n",
            "Epoch 239/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4151 - sparse_categorical_crossentropy: 0.4151 - val_loss: 0.3924 - val_sparse_categorical_crossentropy: 0.3924\n",
            "Epoch 240/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4148 - sparse_categorical_crossentropy: 0.4148 - val_loss: 0.3898 - val_sparse_categorical_crossentropy: 0.3898\n",
            "Epoch 241/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4150 - sparse_categorical_crossentropy: 0.4150 - val_loss: 0.3922 - val_sparse_categorical_crossentropy: 0.3922\n",
            "Epoch 242/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4142 - sparse_categorical_crossentropy: 0.4142 - val_loss: 0.3911 - val_sparse_categorical_crossentropy: 0.3911\n",
            "Epoch 243/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4139 - sparse_categorical_crossentropy: 0.4139 - val_loss: 0.3909 - val_sparse_categorical_crossentropy: 0.3909\n",
            "Epoch 244/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4146 - sparse_categorical_crossentropy: 0.4146 - val_loss: 0.3867 - val_sparse_categorical_crossentropy: 0.3867\n",
            "Epoch 245/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4137 - sparse_categorical_crossentropy: 0.4137 - val_loss: 0.3962 - val_sparse_categorical_crossentropy: 0.3962\n",
            "Epoch 246/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4141 - sparse_categorical_crossentropy: 0.4141 - val_loss: 0.3931 - val_sparse_categorical_crossentropy: 0.3931\n",
            "Epoch 247/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4140 - sparse_categorical_crossentropy: 0.4140 - val_loss: 0.3896 - val_sparse_categorical_crossentropy: 0.3896\n",
            "Epoch 248/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4131 - sparse_categorical_crossentropy: 0.4131 - val_loss: 0.3886 - val_sparse_categorical_crossentropy: 0.3886\n",
            "Epoch 249/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4150 - sparse_categorical_crossentropy: 0.4150 - val_loss: 0.3875 - val_sparse_categorical_crossentropy: 0.3875\n",
            "Epoch 250/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4142 - sparse_categorical_crossentropy: 0.4142 - val_loss: 0.3912 - val_sparse_categorical_crossentropy: 0.3912\n",
            "Epoch 251/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4138 - sparse_categorical_crossentropy: 0.4138 - val_loss: 0.3928 - val_sparse_categorical_crossentropy: 0.3928\n",
            "Epoch 252/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4132 - sparse_categorical_crossentropy: 0.4132 - val_loss: 0.3926 - val_sparse_categorical_crossentropy: 0.3926\n",
            "Epoch 253/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4132 - sparse_categorical_crossentropy: 0.4132 - val_loss: 0.3889 - val_sparse_categorical_crossentropy: 0.3889\n",
            "Epoch 254/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4132 - sparse_categorical_crossentropy: 0.4132 - val_loss: 0.3896 - val_sparse_categorical_crossentropy: 0.3896\n",
            "Epoch 255/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4128 - sparse_categorical_crossentropy: 0.4128 - val_loss: 0.3894 - val_sparse_categorical_crossentropy: 0.3894\n",
            "Epoch 256/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4133 - sparse_categorical_crossentropy: 0.4133 - val_loss: 0.3893 - val_sparse_categorical_crossentropy: 0.3893\n",
            "Epoch 257/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4133 - sparse_categorical_crossentropy: 0.4133 - val_loss: 0.3875 - val_sparse_categorical_crossentropy: 0.3875\n",
            "Epoch 258/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4128 - sparse_categorical_crossentropy: 0.4128 - val_loss: 0.3895 - val_sparse_categorical_crossentropy: 0.3895\n",
            "Epoch 259/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4110 - sparse_categorical_crossentropy: 0.4110 - val_loss: 0.3902 - val_sparse_categorical_crossentropy: 0.3902\n",
            "Epoch 260/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4131 - sparse_categorical_crossentropy: 0.4131 - val_loss: 0.3943 - val_sparse_categorical_crossentropy: 0.3943\n",
            "Epoch 261/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4121 - sparse_categorical_crossentropy: 0.4121 - val_loss: 0.3874 - val_sparse_categorical_crossentropy: 0.3874\n",
            "Epoch 262/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4129 - sparse_categorical_crossentropy: 0.4129 - val_loss: 0.3897 - val_sparse_categorical_crossentropy: 0.3897\n",
            "Epoch 263/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4120 - sparse_categorical_crossentropy: 0.4120 - val_loss: 0.3897 - val_sparse_categorical_crossentropy: 0.3897\n",
            "Epoch 264/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4121 - sparse_categorical_crossentropy: 0.4121 - val_loss: 0.3876 - val_sparse_categorical_crossentropy: 0.3876\n",
            "Epoch 265/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4125 - sparse_categorical_crossentropy: 0.4125 - val_loss: 0.3890 - val_sparse_categorical_crossentropy: 0.3890\n",
            "Epoch 266/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4115 - sparse_categorical_crossentropy: 0.4115 - val_loss: 0.3892 - val_sparse_categorical_crossentropy: 0.3892\n",
            "Epoch 267/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4122 - sparse_categorical_crossentropy: 0.4122 - val_loss: 0.3885 - val_sparse_categorical_crossentropy: 0.3885\n",
            "Epoch 268/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4108 - sparse_categorical_crossentropy: 0.4108 - val_loss: 0.3884 - val_sparse_categorical_crossentropy: 0.3884\n",
            "Epoch 269/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4111 - sparse_categorical_crossentropy: 0.4111 - val_loss: 0.3876 - val_sparse_categorical_crossentropy: 0.3876\n",
            "\n",
            "Epoch 00269: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.\n",
            "Epoch 270/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4085 - sparse_categorical_crossentropy: 0.4085 - val_loss: 0.3872 - val_sparse_categorical_crossentropy: 0.3872\n",
            "Epoch 271/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4091 - sparse_categorical_crossentropy: 0.4091 - val_loss: 0.3893 - val_sparse_categorical_crossentropy: 0.3893\n",
            "Epoch 272/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4087 - sparse_categorical_crossentropy: 0.4087 - val_loss: 0.3859 - val_sparse_categorical_crossentropy: 0.3859\n",
            "Epoch 273/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4087 - sparse_categorical_crossentropy: 0.4087 - val_loss: 0.3870 - val_sparse_categorical_crossentropy: 0.3870\n",
            "Epoch 274/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4083 - sparse_categorical_crossentropy: 0.4083 - val_loss: 0.3859 - val_sparse_categorical_crossentropy: 0.3859\n",
            "Epoch 275/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4082 - sparse_categorical_crossentropy: 0.4082 - val_loss: 0.3875 - val_sparse_categorical_crossentropy: 0.3875\n",
            "Epoch 276/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4086 - sparse_categorical_crossentropy: 0.4086 - val_loss: 0.3865 - val_sparse_categorical_crossentropy: 0.3865\n",
            "Epoch 277/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4081 - sparse_categorical_crossentropy: 0.4081 - val_loss: 0.3879 - val_sparse_categorical_crossentropy: 0.3879\n",
            "Epoch 278/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4066 - sparse_categorical_crossentropy: 0.4066 - val_loss: 0.3856 - val_sparse_categorical_crossentropy: 0.3856\n",
            "Epoch 279/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4081 - sparse_categorical_crossentropy: 0.4081 - val_loss: 0.3867 - val_sparse_categorical_crossentropy: 0.3867\n",
            "Epoch 280/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4078 - sparse_categorical_crossentropy: 0.4078 - val_loss: 0.3876 - val_sparse_categorical_crossentropy: 0.3876\n",
            "Epoch 281/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4078 - sparse_categorical_crossentropy: 0.4078 - val_loss: 0.3867 - val_sparse_categorical_crossentropy: 0.3867\n",
            "Epoch 282/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4080 - sparse_categorical_crossentropy: 0.4080 - val_loss: 0.3857 - val_sparse_categorical_crossentropy: 0.3857\n",
            "Epoch 283/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4075 - sparse_categorical_crossentropy: 0.4075 - val_loss: 0.3858 - val_sparse_categorical_crossentropy: 0.3858\n",
            "Epoch 284/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4079 - sparse_categorical_crossentropy: 0.4079 - val_loss: 0.3861 - val_sparse_categorical_crossentropy: 0.3861\n",
            "Epoch 285/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4085 - sparse_categorical_crossentropy: 0.4085 - val_loss: 0.3858 - val_sparse_categorical_crossentropy: 0.3858\n",
            "Epoch 286/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4068 - sparse_categorical_crossentropy: 0.4068 - val_loss: 0.3889 - val_sparse_categorical_crossentropy: 0.3889\n",
            "Epoch 287/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4077 - sparse_categorical_crossentropy: 0.4077 - val_loss: 0.3853 - val_sparse_categorical_crossentropy: 0.3853\n",
            "Epoch 288/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4069 - sparse_categorical_crossentropy: 0.4069 - val_loss: 0.3870 - val_sparse_categorical_crossentropy: 0.3870\n",
            "Epoch 289/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4072 - sparse_categorical_crossentropy: 0.4072 - val_loss: 0.3869 - val_sparse_categorical_crossentropy: 0.3869\n",
            "Epoch 290/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4075 - sparse_categorical_crossentropy: 0.4075 - val_loss: 0.3868 - val_sparse_categorical_crossentropy: 0.3868\n",
            "Epoch 291/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4070 - sparse_categorical_crossentropy: 0.4070 - val_loss: 0.3862 - val_sparse_categorical_crossentropy: 0.3862\n",
            "Epoch 292/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4060 - sparse_categorical_crossentropy: 0.4060 - val_loss: 0.3871 - val_sparse_categorical_crossentropy: 0.3871\n",
            "Epoch 293/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4063 - sparse_categorical_crossentropy: 0.4063 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 294/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4065 - sparse_categorical_crossentropy: 0.4065 - val_loss: 0.3867 - val_sparse_categorical_crossentropy: 0.3867\n",
            "Epoch 295/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4069 - sparse_categorical_crossentropy: 0.4069 - val_loss: 0.3861 - val_sparse_categorical_crossentropy: 0.3861\n",
            "Epoch 296/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4076 - sparse_categorical_crossentropy: 0.4076 - val_loss: 0.3861 - val_sparse_categorical_crossentropy: 0.3861\n",
            "Epoch 297/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4066 - sparse_categorical_crossentropy: 0.4066 - val_loss: 0.3865 - val_sparse_categorical_crossentropy: 0.3865\n",
            "Epoch 298/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4066 - sparse_categorical_crossentropy: 0.4066 - val_loss: 0.3868 - val_sparse_categorical_crossentropy: 0.3868\n",
            "Epoch 299/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4063 - sparse_categorical_crossentropy: 0.4063 - val_loss: 0.3858 - val_sparse_categorical_crossentropy: 0.3858\n",
            "Epoch 300/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4066 - sparse_categorical_crossentropy: 0.4066 - val_loss: 0.3852 - val_sparse_categorical_crossentropy: 0.3852\n",
            "Epoch 301/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4076 - sparse_categorical_crossentropy: 0.4076 - val_loss: 0.3858 - val_sparse_categorical_crossentropy: 0.3858\n",
            "Epoch 302/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4065 - sparse_categorical_crossentropy: 0.4065 - val_loss: 0.3887 - val_sparse_categorical_crossentropy: 0.3887\n",
            "Epoch 303/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4070 - sparse_categorical_crossentropy: 0.4070 - val_loss: 0.3871 - val_sparse_categorical_crossentropy: 0.3871\n",
            "Epoch 304/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4053 - sparse_categorical_crossentropy: 0.4053 - val_loss: 0.3849 - val_sparse_categorical_crossentropy: 0.3849\n",
            "Epoch 305/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4062 - sparse_categorical_crossentropy: 0.4062 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 306/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4072 - sparse_categorical_crossentropy: 0.4072 - val_loss: 0.3861 - val_sparse_categorical_crossentropy: 0.3861\n",
            "Epoch 307/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4069 - sparse_categorical_crossentropy: 0.4069 - val_loss: 0.3852 - val_sparse_categorical_crossentropy: 0.3852\n",
            "Epoch 308/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4062 - sparse_categorical_crossentropy: 0.4062 - val_loss: 0.3858 - val_sparse_categorical_crossentropy: 0.3858\n",
            "Epoch 309/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4060 - sparse_categorical_crossentropy: 0.4060 - val_loss: 0.3864 - val_sparse_categorical_crossentropy: 0.3864\n",
            "Epoch 310/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4066 - sparse_categorical_crossentropy: 0.4066 - val_loss: 0.3859 - val_sparse_categorical_crossentropy: 0.3859\n",
            "Epoch 311/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4056 - sparse_categorical_crossentropy: 0.4056 - val_loss: 0.3868 - val_sparse_categorical_crossentropy: 0.3868\n",
            "Epoch 312/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4069 - sparse_categorical_crossentropy: 0.4069 - val_loss: 0.3873 - val_sparse_categorical_crossentropy: 0.3873\n",
            "Epoch 313/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4061 - sparse_categorical_crossentropy: 0.4061 - val_loss: 0.3854 - val_sparse_categorical_crossentropy: 0.3854\n",
            "Epoch 314/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4062 - sparse_categorical_crossentropy: 0.4062 - val_loss: 0.3851 - val_sparse_categorical_crossentropy: 0.3851\n",
            "Epoch 315/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4058 - sparse_categorical_crossentropy: 0.4058 - val_loss: 0.3853 - val_sparse_categorical_crossentropy: 0.3853\n",
            "Epoch 316/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4067 - sparse_categorical_crossentropy: 0.4067 - val_loss: 0.3853 - val_sparse_categorical_crossentropy: 0.3853\n",
            "Epoch 317/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4046 - sparse_categorical_crossentropy: 0.4046 - val_loss: 0.3851 - val_sparse_categorical_crossentropy: 0.3851\n",
            "Epoch 318/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4063 - sparse_categorical_crossentropy: 0.4063 - val_loss: 0.3879 - val_sparse_categorical_crossentropy: 0.3879\n",
            "\n",
            "Epoch 00318: ReduceLROnPlateau reducing learning rate to 2.9999999502915785e-06.\n",
            "Epoch 319/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4058 - sparse_categorical_crossentropy: 0.4058 - val_loss: 0.3847 - val_sparse_categorical_crossentropy: 0.3847\n",
            "Epoch 320/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4052 - sparse_categorical_crossentropy: 0.4052 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "Epoch 321/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4058 - sparse_categorical_crossentropy: 0.4058 - val_loss: 0.3855 - val_sparse_categorical_crossentropy: 0.3855\n",
            "Epoch 322/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4053 - sparse_categorical_crossentropy: 0.4053 - val_loss: 0.3852 - val_sparse_categorical_crossentropy: 0.3852\n",
            "Epoch 323/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4045 - sparse_categorical_crossentropy: 0.4045 - val_loss: 0.3847 - val_sparse_categorical_crossentropy: 0.3847\n",
            "Epoch 324/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4055 - sparse_categorical_crossentropy: 0.4055 - val_loss: 0.3849 - val_sparse_categorical_crossentropy: 0.3849\n",
            "Epoch 325/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4062 - sparse_categorical_crossentropy: 0.4062 - val_loss: 0.3850 - val_sparse_categorical_crossentropy: 0.3850\n",
            "Epoch 326/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4055 - sparse_categorical_crossentropy: 0.4055 - val_loss: 0.3853 - val_sparse_categorical_crossentropy: 0.3853\n",
            "Epoch 327/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4051 - sparse_categorical_crossentropy: 0.4051 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 328/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4039 - sparse_categorical_crossentropy: 0.4039 - val_loss: 0.3855 - val_sparse_categorical_crossentropy: 0.3855\n",
            "Epoch 329/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4045 - sparse_categorical_crossentropy: 0.4045 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 330/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4059 - sparse_categorical_crossentropy: 0.4059 - val_loss: 0.3854 - val_sparse_categorical_crossentropy: 0.3854\n",
            "Epoch 331/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4050 - sparse_categorical_crossentropy: 0.4050 - val_loss: 0.3849 - val_sparse_categorical_crossentropy: 0.3849\n",
            "Epoch 332/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4039 - sparse_categorical_crossentropy: 0.4039 - val_loss: 0.3848 - val_sparse_categorical_crossentropy: 0.3848\n",
            "Epoch 333/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4050 - sparse_categorical_crossentropy: 0.4050 - val_loss: 0.3855 - val_sparse_categorical_crossentropy: 0.3855\n",
            "Epoch 334/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4055 - sparse_categorical_crossentropy: 0.4055 - val_loss: 0.3847 - val_sparse_categorical_crossentropy: 0.3847\n",
            "Epoch 335/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4049 - sparse_categorical_crossentropy: 0.4049 - val_loss: 0.3853 - val_sparse_categorical_crossentropy: 0.3853\n",
            "Epoch 336/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4060 - sparse_categorical_crossentropy: 0.4060 - val_loss: 0.3850 - val_sparse_categorical_crossentropy: 0.3850\n",
            "Epoch 337/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4047 - sparse_categorical_crossentropy: 0.4047 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 338/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4039 - sparse_categorical_crossentropy: 0.4039 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 339/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4045 - sparse_categorical_crossentropy: 0.4045 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 340/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4049 - sparse_categorical_crossentropy: 0.4049 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 341/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4045 - sparse_categorical_crossentropy: 0.4045 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 342/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4045 - sparse_categorical_crossentropy: 0.4045 - val_loss: 0.3847 - val_sparse_categorical_crossentropy: 0.3847\n",
            "Epoch 343/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4041 - sparse_categorical_crossentropy: 0.4041 - val_loss: 0.3856 - val_sparse_categorical_crossentropy: 0.3856\n",
            "Epoch 344/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4052 - sparse_categorical_crossentropy: 0.4052 - val_loss: 0.3849 - val_sparse_categorical_crossentropy: 0.3849\n",
            "Epoch 345/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4052 - sparse_categorical_crossentropy: 0.4052 - val_loss: 0.3847 - val_sparse_categorical_crossentropy: 0.3847\n",
            "Epoch 346/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4031 - sparse_categorical_crossentropy: 0.4031 - val_loss: 0.3848 - val_sparse_categorical_crossentropy: 0.3848\n",
            "Epoch 347/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4042 - sparse_categorical_crossentropy: 0.4042 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 348/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4048 - sparse_categorical_crossentropy: 0.4048 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 349/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4056 - sparse_categorical_crossentropy: 0.4056 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "Epoch 350/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4038 - sparse_categorical_crossentropy: 0.4038 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 351/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4049 - sparse_categorical_crossentropy: 0.4049 - val_loss: 0.3838 - val_sparse_categorical_crossentropy: 0.3838\n",
            "Epoch 352/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4029 - sparse_categorical_crossentropy: 0.4029 - val_loss: 0.3850 - val_sparse_categorical_crossentropy: 0.3850\n",
            "Epoch 353/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4035 - sparse_categorical_crossentropy: 0.4035 - val_loss: 0.3853 - val_sparse_categorical_crossentropy: 0.3853\n",
            "Epoch 354/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4045 - sparse_categorical_crossentropy: 0.4045 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 355/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4041 - sparse_categorical_crossentropy: 0.4041 - val_loss: 0.3851 - val_sparse_categorical_crossentropy: 0.3851\n",
            "Epoch 356/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4052 - sparse_categorical_crossentropy: 0.4052 - val_loss: 0.3853 - val_sparse_categorical_crossentropy: 0.3853\n",
            "Epoch 357/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4044 - sparse_categorical_crossentropy: 0.4044 - val_loss: 0.3848 - val_sparse_categorical_crossentropy: 0.3848\n",
            "Epoch 358/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4043 - sparse_categorical_crossentropy: 0.4043 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 359/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4027 - sparse_categorical_crossentropy: 0.4027 - val_loss: 0.3848 - val_sparse_categorical_crossentropy: 0.3848\n",
            "Epoch 360/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4052 - sparse_categorical_crossentropy: 0.4052 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 361/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4038 - sparse_categorical_crossentropy: 0.4038 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 362/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4038 - sparse_categorical_crossentropy: 0.4038 - val_loss: 0.3839 - val_sparse_categorical_crossentropy: 0.3839\n",
            "Epoch 363/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4052 - sparse_categorical_crossentropy: 0.4052 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 364/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4045 - sparse_categorical_crossentropy: 0.4045 - val_loss: 0.3849 - val_sparse_categorical_crossentropy: 0.3849\n",
            "Epoch 365/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4045 - sparse_categorical_crossentropy: 0.4045 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 366/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4044 - sparse_categorical_crossentropy: 0.4044 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 367/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4033 - sparse_categorical_crossentropy: 0.4033 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 368/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4051 - sparse_categorical_crossentropy: 0.4051 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 369/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4041 - sparse_categorical_crossentropy: 0.4041 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 370/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4057 - sparse_categorical_crossentropy: 0.4057 - val_loss: 0.3849 - val_sparse_categorical_crossentropy: 0.3849\n",
            "Epoch 371/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4033 - sparse_categorical_crossentropy: 0.4033 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 372/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4029 - sparse_categorical_crossentropy: 0.4029 - val_loss: 0.3856 - val_sparse_categorical_crossentropy: 0.3856\n",
            "Epoch 373/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4042 - sparse_categorical_crossentropy: 0.4042 - val_loss: 0.3851 - val_sparse_categorical_crossentropy: 0.3851\n",
            "Epoch 374/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4043 - sparse_categorical_crossentropy: 0.4043 - val_loss: 0.3849 - val_sparse_categorical_crossentropy: 0.3849\n",
            "Epoch 375/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4040 - sparse_categorical_crossentropy: 0.4040 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 376/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4043 - sparse_categorical_crossentropy: 0.4043 - val_loss: 0.3839 - val_sparse_categorical_crossentropy: 0.3839\n",
            "\n",
            "Epoch 00376: ReduceLROnPlateau reducing learning rate to 9.48683259704384e-07.\n",
            "Epoch 377/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4038 - sparse_categorical_crossentropy: 0.4038 - val_loss: 0.3849 - val_sparse_categorical_crossentropy: 0.3849\n",
            "Epoch 378/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4036 - sparse_categorical_crossentropy: 0.4036 - val_loss: 0.3838 - val_sparse_categorical_crossentropy: 0.3838\n",
            "Epoch 379/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4025 - sparse_categorical_crossentropy: 0.4025 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 380/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4052 - sparse_categorical_crossentropy: 0.4052 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 381/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4027 - sparse_categorical_crossentropy: 0.4027 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 382/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4043 - sparse_categorical_crossentropy: 0.4043 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "Epoch 383/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4031 - sparse_categorical_crossentropy: 0.4031 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 384/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4039 - sparse_categorical_crossentropy: 0.4039 - val_loss: 0.3850 - val_sparse_categorical_crossentropy: 0.3850\n",
            "Epoch 385/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4046 - sparse_categorical_crossentropy: 0.4046 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 386/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4033 - sparse_categorical_crossentropy: 0.4033 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 387/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4045 - sparse_categorical_crossentropy: 0.4045 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 388/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4037 - sparse_categorical_crossentropy: 0.4037 - val_loss: 0.3847 - val_sparse_categorical_crossentropy: 0.3847\n",
            "Epoch 389/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4029 - sparse_categorical_crossentropy: 0.4029 - val_loss: 0.3851 - val_sparse_categorical_crossentropy: 0.3851\n",
            "Epoch 390/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4036 - sparse_categorical_crossentropy: 0.4036 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 391/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4039 - sparse_categorical_crossentropy: 0.4039 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "Epoch 392/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4028 - sparse_categorical_crossentropy: 0.4028 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 393/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4031 - sparse_categorical_crossentropy: 0.4031 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 394/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4034 - sparse_categorical_crossentropy: 0.4034 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 395/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4035 - sparse_categorical_crossentropy: 0.4035 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 396/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4047 - sparse_categorical_crossentropy: 0.4047 - val_loss: 0.3839 - val_sparse_categorical_crossentropy: 0.3839\n",
            "Epoch 397/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4025 - sparse_categorical_crossentropy: 0.4025 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 398/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4039 - sparse_categorical_crossentropy: 0.4039 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 399/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4047 - sparse_categorical_crossentropy: 0.4047 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 400/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4026 - sparse_categorical_crossentropy: 0.4026 - val_loss: 0.3840 - val_sparse_categorical_crossentropy: 0.3840\n",
            "Epoch 401/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4031 - sparse_categorical_crossentropy: 0.4031 - val_loss: 0.3848 - val_sparse_categorical_crossentropy: 0.3848\n",
            "\n",
            "Epoch 00401: ReduceLROnPlateau reducing learning rate to 2.9999998064878395e-07.\n",
            "Epoch 402/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4036 - sparse_categorical_crossentropy: 0.4036 - val_loss: 0.3840 - val_sparse_categorical_crossentropy: 0.3840\n",
            "Epoch 403/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4039 - sparse_categorical_crossentropy: 0.4039 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 404/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4047 - sparse_categorical_crossentropy: 0.4047 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 405/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4027 - sparse_categorical_crossentropy: 0.4027 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 406/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4044 - sparse_categorical_crossentropy: 0.4044 - val_loss: 0.3847 - val_sparse_categorical_crossentropy: 0.3847\n",
            "Epoch 407/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4042 - sparse_categorical_crossentropy: 0.4042 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "Epoch 408/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4027 - sparse_categorical_crossentropy: 0.4027 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 409/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4038 - sparse_categorical_crossentropy: 0.4038 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 410/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4049 - sparse_categorical_crossentropy: 0.4049 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 411/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4032 - sparse_categorical_crossentropy: 0.4032 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 412/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4022 - sparse_categorical_crossentropy: 0.4022 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 413/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4047 - sparse_categorical_crossentropy: 0.4047 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 414/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4032 - sparse_categorical_crossentropy: 0.4032 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 415/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4040 - sparse_categorical_crossentropy: 0.4040 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 416/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4034 - sparse_categorical_crossentropy: 0.4034 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 417/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4039 - sparse_categorical_crossentropy: 0.4039 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 418/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4035 - sparse_categorical_crossentropy: 0.4035 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 419/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4035 - sparse_categorical_crossentropy: 0.4035 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "Epoch 420/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4039 - sparse_categorical_crossentropy: 0.4039 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 421/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4053 - sparse_categorical_crossentropy: 0.4053 - val_loss: 0.3847 - val_sparse_categorical_crossentropy: 0.3847\n",
            "Epoch 422/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4037 - sparse_categorical_crossentropy: 0.4037 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 423/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4026 - sparse_categorical_crossentropy: 0.4026 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 424/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4045 - sparse_categorical_crossentropy: 0.4045 - val_loss: 0.3847 - val_sparse_categorical_crossentropy: 0.3847\n",
            "Epoch 425/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4034 - sparse_categorical_crossentropy: 0.4034 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "Epoch 426/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4034 - sparse_categorical_crossentropy: 0.4034 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "\n",
            "Epoch 00426: ReduceLROnPlateau reducing learning rate to 9.486832417289166e-08.\n",
            "Epoch 427/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4043 - sparse_categorical_crossentropy: 0.4043 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 428/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4040 - sparse_categorical_crossentropy: 0.4040 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 429/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4047 - sparse_categorical_crossentropy: 0.4047 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 430/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4044 - sparse_categorical_crossentropy: 0.4044 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 431/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4043 - sparse_categorical_crossentropy: 0.4043 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 432/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4030 - sparse_categorical_crossentropy: 0.4030 - val_loss: 0.3839 - val_sparse_categorical_crossentropy: 0.3839\n",
            "Epoch 433/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4036 - sparse_categorical_crossentropy: 0.4036 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 434/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4040 - sparse_categorical_crossentropy: 0.4040 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 435/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4034 - sparse_categorical_crossentropy: 0.4034 - val_loss: 0.3839 - val_sparse_categorical_crossentropy: 0.3839\n",
            "Epoch 436/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4036 - sparse_categorical_crossentropy: 0.4036 - val_loss: 0.3848 - val_sparse_categorical_crossentropy: 0.3848\n",
            "Epoch 437/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4037 - sparse_categorical_crossentropy: 0.4037 - val_loss: 0.3848 - val_sparse_categorical_crossentropy: 0.3848\n",
            "Epoch 438/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4025 - sparse_categorical_crossentropy: 0.4025 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 439/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4040 - sparse_categorical_crossentropy: 0.4040 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 440/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4031 - sparse_categorical_crossentropy: 0.4031 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 441/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4037 - sparse_categorical_crossentropy: 0.4037 - val_loss: 0.3845 - val_sparse_categorical_crossentropy: 0.3845\n",
            "Epoch 442/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4041 - sparse_categorical_crossentropy: 0.4041 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 443/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4029 - sparse_categorical_crossentropy: 0.4029 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 444/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4033 - sparse_categorical_crossentropy: 0.4033 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 445/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4031 - sparse_categorical_crossentropy: 0.4031 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 446/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4032 - sparse_categorical_crossentropy: 0.4032 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 447/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4027 - sparse_categorical_crossentropy: 0.4027 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 448/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4034 - sparse_categorical_crossentropy: 0.4034 - val_loss: 0.3843 - val_sparse_categorical_crossentropy: 0.3843\n",
            "Epoch 449/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4027 - sparse_categorical_crossentropy: 0.4027 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 450/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4030 - sparse_categorical_crossentropy: 0.4030 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 451/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4041 - sparse_categorical_crossentropy: 0.4041 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "\n",
            "Epoch 00451: ReduceLROnPlateau reducing learning rate to 2.9999998963651765e-08.\n",
            "Epoch 452/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4044 - sparse_categorical_crossentropy: 0.4044 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "Epoch 453/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4032 - sparse_categorical_crossentropy: 0.4032 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 454/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4033 - sparse_categorical_crossentropy: 0.4033 - val_loss: 0.3842 - val_sparse_categorical_crossentropy: 0.3842\n",
            "Epoch 455/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4036 - sparse_categorical_crossentropy: 0.4036 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Epoch 456/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4026 - sparse_categorical_crossentropy: 0.4026 - val_loss: 0.3839 - val_sparse_categorical_crossentropy: 0.3839\n",
            "Epoch 457/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4056 - sparse_categorical_crossentropy: 0.4056 - val_loss: 0.3841 - val_sparse_categorical_crossentropy: 0.3841\n",
            "Epoch 458/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4037 - sparse_categorical_crossentropy: 0.4037 - val_loss: 0.3844 - val_sparse_categorical_crossentropy: 0.3844\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00458: early stopping\n",
            "모델저장완료\n",
            "==================================================\n",
            "Loss와 ACC에 대한 Plot을 그립니다\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAETCAYAAABnSkJLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "기존nn모델의 train loss를 출력합니다\n",
            "train, loss and metric: [0.36619446460301197, 0.36619446460301197]\n",
            "기존nn모델의 valid loss를 출력합니다\n",
            "valid, loss and metric: [0.3837669102556569, 0.3837669102556569]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "08f0391c-2435-4e96-ce35-76f1abfa37ec",
        "id": "3ilqZatxc-yL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "savingpath_csv = 'drive/My Drive/데이콘_천체유형/파일제출/' + csv_folder + '/ELU_이상치1개더제거_initial_rate_%s.csv' % initial_rate\n",
        "\n",
        "print('final rate는 다음과 같습니다')\n",
        "print(hist.history['lr'][np.argmin(hist.history['val_loss'])])\n",
        "print('='*25)\n",
        "y_pred = nn_model.predict(test_x)\n",
        "submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n",
        "submission.to_csv(savingpath_csv, index=True)\n",
        "print('csv 저장완료')\n",
        "\n",
        "print('='*50)\n",
        "best_val_loss = np.min(hist.history['val_loss'])\n",
        "print(\"best_valid_loss: {}\".format(best_val_loss))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final rate는 다음과 같습니다\n",
            "9.4868324e-07\n",
            "=========================\n",
            "csv 저장완료\n",
            "==================================================\n",
            "best_valid_loss: 0.3837669102556569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oy650Oa7Fv9G"
      },
      "source": [
        "## initial_rate = 1e-4 (과적합 심하고 Gradient Exploding 문제도 있음)\n",
        "\n",
        "### 1. lr 줄여서 local minima 문제 없애고\n",
        "### 2. Drop Out 비율 더 높게 해주고\n",
        "### 3. Gradient Exploding 문제 해결을 하고 싶다면... clip value 지정해줘야 할 듯 (얘는 아직 적용 안됨)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bb9cf7b2-38cb-4e3f-f13b-653a03cee027",
        "id": "tDPK5E_dFv9R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch =  800\n",
        "pat =  80\n",
        "red_pat =  25\n",
        "batchsize = 256*4\n",
        "initial_rate = 1e-4\n",
        "factor = 1/np.sqrt(10) # red_patience 만큼 기다리다가, 학습률을 *factor 배로 줄여버림 \n",
        "minimumlr = 1e-7\n",
        "\n",
        "import os\n",
        "MODEL_SAVE_FOLDER_PATH0 = SAVEMODEL_NEWFOLDER0 +  '/initial_rate=%s/' % initial_rate ## checkpoint\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "if not os.path.exists(MODEL_SAVE_FOLDER_PATH1):\n",
        "  os.mkdir(MODEL_SAVE_FOLDER_PATH1)\n",
        "check_path = MODEL_SAVE_FOLDER_PATH0 + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "### model early stopping : 더이상 성능 개선이 되지 않으면 멈춤\n",
        "es = EarlyStopping(monitor= 'val_loss', patience = pat, verbose = 1, mode='min',\n",
        "                    restore_best_weights = True\n",
        "                   ) \n",
        "\n",
        "### model check point\n",
        "mc = ModelCheckpoint(filepath=check_path, monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "## ReduceLR on Plateau : val_loss가 안 줄어들 때 lr을 작게 할 수 있음 (local minima 대처방법)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor = factor,   # patience 만큼 기다리다가 0.1이면 학습률을 0.1배로 줄여버림 \n",
        "                        patience = red_pat, mode = 'min', verbose = 1,\n",
        "                        min_lr = minimumlr\n",
        "                        )\n",
        "\n",
        "from keras import optimizers\n",
        "optimizer = optimizers.Adam(\n",
        "    lr=initial_rate,\n",
        ")\n",
        "\n",
        "## compile model\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "\n",
        "nn_model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              #metrics=['accuracy']\n",
        "              #metrics=[metrics.sparse_categorical_accuracy]\n",
        "              metrics=[CCE]\n",
        "              )\n",
        "\n",
        "## fitting model\n",
        "hist = nn_model.fit(  train_input, train_target,validation_data=[cv_input, cv_target],\n",
        "                    batch_size=batchsize,\n",
        "                    epochs=epoch,\n",
        "                    callbacks = [es \n",
        "                                 #,mc\n",
        "                                 ,rlr\n",
        "                                ] )\n",
        "\n",
        "## save model\n",
        "model_json = nn_model.to_json()\n",
        "with open(json_path, \"w\") as json_file : \n",
        "    json_file.write(model_json)\n",
        "## model weight save\n",
        "nn_model.save_weights(weight_path)\n",
        "print(\"모델저장완료\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"Loss와 ACC에 대한 Plot을 그립니다\")\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "loss_ax.legend(loc='lower left')\n",
        "\n",
        "#acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "#acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "#acc_ax.set_ylabel('accuracy')\n",
        "#acc_ax.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## evaluate model\n",
        "print('기존nn모델의 train loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(train_input, train_target, batch_size=batchsize, verbose=0)\n",
        "print(\"train, loss and metric: {}\".format(loss_and_metric))\n",
        "## evaluate model\n",
        "print('기존nn모델의 valid loss를 출력합니다')\n",
        "loss_and_metric = nn_model.evaluate(cv_input, cv_target, batch_size=batchsize, verbose=0)\n",
        "print(\"valid, loss and metric: {}\".format(loss_and_metric))\n",
        "## model weight save ## 기존 모델의 가중치 저장\n",
        "#nn_model.save_weights(weight_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 184521 samples, validate on 15377 samples\n",
            "Epoch 1/800\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "184521/184521 [==============================] - 16s 86us/step - loss: 0.9910 - sparse_categorical_crossentropy: 0.9910 - val_loss: 0.6677 - val_sparse_categorical_crossentropy: 0.6677\n",
            "Epoch 2/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.6997 - sparse_categorical_crossentropy: 0.6997 - val_loss: 0.5904 - val_sparse_categorical_crossentropy: 0.5904\n",
            "Epoch 3/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.6386 - sparse_categorical_crossentropy: 0.6386 - val_loss: 0.5657 - val_sparse_categorical_crossentropy: 0.5657\n",
            "Epoch 4/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.6082 - sparse_categorical_crossentropy: 0.6082 - val_loss: 0.5256 - val_sparse_categorical_crossentropy: 0.5256\n",
            "Epoch 5/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5877 - sparse_categorical_crossentropy: 0.5877 - val_loss: 0.5203 - val_sparse_categorical_crossentropy: 0.5203\n",
            "Epoch 6/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5718 - sparse_categorical_crossentropy: 0.5718 - val_loss: 0.5036 - val_sparse_categorical_crossentropy: 0.5036\n",
            "Epoch 7/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5566 - sparse_categorical_crossentropy: 0.5566 - val_loss: 0.4990 - val_sparse_categorical_crossentropy: 0.4990\n",
            "Epoch 8/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5497 - sparse_categorical_crossentropy: 0.5497 - val_loss: 0.4862 - val_sparse_categorical_crossentropy: 0.4862\n",
            "Epoch 9/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5438 - sparse_categorical_crossentropy: 0.5438 - val_loss: 0.4731 - val_sparse_categorical_crossentropy: 0.4731\n",
            "Epoch 10/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5308 - sparse_categorical_crossentropy: 0.5308 - val_loss: 0.4708 - val_sparse_categorical_crossentropy: 0.4708\n",
            "Epoch 11/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5249 - sparse_categorical_crossentropy: 0.5249 - val_loss: 0.4762 - val_sparse_categorical_crossentropy: 0.4762\n",
            "Epoch 12/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5182 - sparse_categorical_crossentropy: 0.5182 - val_loss: 0.4796 - val_sparse_categorical_crossentropy: 0.4796\n",
            "Epoch 13/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5154 - sparse_categorical_crossentropy: 0.5154 - val_loss: 0.4671 - val_sparse_categorical_crossentropy: 0.4671\n",
            "Epoch 14/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5095 - sparse_categorical_crossentropy: 0.5095 - val_loss: 0.4548 - val_sparse_categorical_crossentropy: 0.4548\n",
            "Epoch 15/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.5049 - sparse_categorical_crossentropy: 0.5049 - val_loss: 0.4525 - val_sparse_categorical_crossentropy: 0.4525\n",
            "Epoch 16/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.5023 - sparse_categorical_crossentropy: 0.5023 - val_loss: 0.4755 - val_sparse_categorical_crossentropy: 0.4755\n",
            "Epoch 17/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4992 - sparse_categorical_crossentropy: 0.4992 - val_loss: 0.4494 - val_sparse_categorical_crossentropy: 0.4494\n",
            "Epoch 18/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4937 - sparse_categorical_crossentropy: 0.4937 - val_loss: 0.4423 - val_sparse_categorical_crossentropy: 0.4423\n",
            "Epoch 19/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4906 - sparse_categorical_crossentropy: 0.4906 - val_loss: 0.4553 - val_sparse_categorical_crossentropy: 0.4553\n",
            "Epoch 20/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4873 - sparse_categorical_crossentropy: 0.4873 - val_loss: 0.4435 - val_sparse_categorical_crossentropy: 0.4435\n",
            "Epoch 21/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4857 - sparse_categorical_crossentropy: 0.4857 - val_loss: 0.4411 - val_sparse_categorical_crossentropy: 0.4411\n",
            "Epoch 22/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4816 - sparse_categorical_crossentropy: 0.4816 - val_loss: 0.4451 - val_sparse_categorical_crossentropy: 0.4451\n",
            "Epoch 23/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4831 - sparse_categorical_crossentropy: 0.4831 - val_loss: 0.4321 - val_sparse_categorical_crossentropy: 0.4321\n",
            "Epoch 24/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4790 - sparse_categorical_crossentropy: 0.4790 - val_loss: 0.4350 - val_sparse_categorical_crossentropy: 0.4350\n",
            "Epoch 25/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4746 - sparse_categorical_crossentropy: 0.4746 - val_loss: 0.4259 - val_sparse_categorical_crossentropy: 0.4259\n",
            "Epoch 26/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4739 - sparse_categorical_crossentropy: 0.4739 - val_loss: 0.4320 - val_sparse_categorical_crossentropy: 0.4320\n",
            "Epoch 27/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4700 - sparse_categorical_crossentropy: 0.4700 - val_loss: 0.4244 - val_sparse_categorical_crossentropy: 0.4244\n",
            "Epoch 28/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4715 - sparse_categorical_crossentropy: 0.4715 - val_loss: 0.4306 - val_sparse_categorical_crossentropy: 0.4306\n",
            "Epoch 29/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4696 - sparse_categorical_crossentropy: 0.4696 - val_loss: 0.4418 - val_sparse_categorical_crossentropy: 0.4418\n",
            "Epoch 30/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4672 - sparse_categorical_crossentropy: 0.4672 - val_loss: 0.4310 - val_sparse_categorical_crossentropy: 0.4310\n",
            "Epoch 31/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4660 - sparse_categorical_crossentropy: 0.4660 - val_loss: 0.4239 - val_sparse_categorical_crossentropy: 0.4239\n",
            "Epoch 32/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4621 - sparse_categorical_crossentropy: 0.4621 - val_loss: 0.4334 - val_sparse_categorical_crossentropy: 0.4334\n",
            "Epoch 33/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4613 - sparse_categorical_crossentropy: 0.4613 - val_loss: 0.4208 - val_sparse_categorical_crossentropy: 0.4208\n",
            "Epoch 34/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4597 - sparse_categorical_crossentropy: 0.4597 - val_loss: 0.4256 - val_sparse_categorical_crossentropy: 0.4256\n",
            "Epoch 35/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4593 - sparse_categorical_crossentropy: 0.4593 - val_loss: 0.4207 - val_sparse_categorical_crossentropy: 0.4207\n",
            "Epoch 36/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4565 - sparse_categorical_crossentropy: 0.4565 - val_loss: 0.4183 - val_sparse_categorical_crossentropy: 0.4183\n",
            "Epoch 37/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4540 - sparse_categorical_crossentropy: 0.4540 - val_loss: 0.4159 - val_sparse_categorical_crossentropy: 0.4159\n",
            "Epoch 38/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4557 - sparse_categorical_crossentropy: 0.4557 - val_loss: 0.4155 - val_sparse_categorical_crossentropy: 0.4155\n",
            "Epoch 39/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4534 - sparse_categorical_crossentropy: 0.4534 - val_loss: 0.4387 - val_sparse_categorical_crossentropy: 0.4387\n",
            "Epoch 40/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4516 - sparse_categorical_crossentropy: 0.4516 - val_loss: 0.4184 - val_sparse_categorical_crossentropy: 0.4184\n",
            "Epoch 41/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4514 - sparse_categorical_crossentropy: 0.4514 - val_loss: 0.4209 - val_sparse_categorical_crossentropy: 0.4209\n",
            "Epoch 42/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4493 - sparse_categorical_crossentropy: 0.4493 - val_loss: 0.4119 - val_sparse_categorical_crossentropy: 0.4119\n",
            "Epoch 43/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4493 - sparse_categorical_crossentropy: 0.4493 - val_loss: 0.4099 - val_sparse_categorical_crossentropy: 0.4099\n",
            "Epoch 44/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4482 - sparse_categorical_crossentropy: 0.4482 - val_loss: 0.4069 - val_sparse_categorical_crossentropy: 0.4069\n",
            "Epoch 45/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4458 - sparse_categorical_crossentropy: 0.4458 - val_loss: 0.4106 - val_sparse_categorical_crossentropy: 0.4106\n",
            "Epoch 46/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4455 - sparse_categorical_crossentropy: 0.4455 - val_loss: 0.4098 - val_sparse_categorical_crossentropy: 0.4098\n",
            "Epoch 47/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4472 - sparse_categorical_crossentropy: 0.4472 - val_loss: 0.4083 - val_sparse_categorical_crossentropy: 0.4083\n",
            "Epoch 48/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4434 - sparse_categorical_crossentropy: 0.4434 - val_loss: 0.4156 - val_sparse_categorical_crossentropy: 0.4156\n",
            "Epoch 49/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4434 - sparse_categorical_crossentropy: 0.4434 - val_loss: 0.4075 - val_sparse_categorical_crossentropy: 0.4075\n",
            "Epoch 50/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4432 - sparse_categorical_crossentropy: 0.4432 - val_loss: 0.4056 - val_sparse_categorical_crossentropy: 0.4056\n",
            "Epoch 51/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4428 - sparse_categorical_crossentropy: 0.4428 - val_loss: 0.4330 - val_sparse_categorical_crossentropy: 0.4330\n",
            "Epoch 52/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4396 - sparse_categorical_crossentropy: 0.4396 - val_loss: 0.4087 - val_sparse_categorical_crossentropy: 0.4087\n",
            "Epoch 53/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4401 - sparse_categorical_crossentropy: 0.4401 - val_loss: 0.4132 - val_sparse_categorical_crossentropy: 0.4132\n",
            "Epoch 54/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4383 - sparse_categorical_crossentropy: 0.4383 - val_loss: 0.4075 - val_sparse_categorical_crossentropy: 0.4075\n",
            "Epoch 55/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4387 - sparse_categorical_crossentropy: 0.4387 - val_loss: 0.4047 - val_sparse_categorical_crossentropy: 0.4047\n",
            "Epoch 56/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4367 - sparse_categorical_crossentropy: 0.4367 - val_loss: 0.4020 - val_sparse_categorical_crossentropy: 0.4020\n",
            "Epoch 57/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4369 - sparse_categorical_crossentropy: 0.4369 - val_loss: 0.4089 - val_sparse_categorical_crossentropy: 0.4089\n",
            "Epoch 58/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4360 - sparse_categorical_crossentropy: 0.4360 - val_loss: 0.4017 - val_sparse_categorical_crossentropy: 0.4017\n",
            "Epoch 59/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4360 - sparse_categorical_crossentropy: 0.4360 - val_loss: 0.3978 - val_sparse_categorical_crossentropy: 0.3978\n",
            "Epoch 60/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4356 - sparse_categorical_crossentropy: 0.4356 - val_loss: 0.4114 - val_sparse_categorical_crossentropy: 0.4114\n",
            "Epoch 61/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4323 - sparse_categorical_crossentropy: 0.4323 - val_loss: 0.4093 - val_sparse_categorical_crossentropy: 0.4093\n",
            "Epoch 62/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4321 - sparse_categorical_crossentropy: 0.4321 - val_loss: 0.4061 - val_sparse_categorical_crossentropy: 0.4061\n",
            "Epoch 63/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4319 - sparse_categorical_crossentropy: 0.4319 - val_loss: 0.3973 - val_sparse_categorical_crossentropy: 0.3973\n",
            "Epoch 64/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4323 - sparse_categorical_crossentropy: 0.4323 - val_loss: 0.3977 - val_sparse_categorical_crossentropy: 0.3977\n",
            "Epoch 65/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4311 - sparse_categorical_crossentropy: 0.4311 - val_loss: 0.3961 - val_sparse_categorical_crossentropy: 0.3961\n",
            "Epoch 66/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4326 - sparse_categorical_crossentropy: 0.4326 - val_loss: 0.3960 - val_sparse_categorical_crossentropy: 0.3960\n",
            "Epoch 67/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4297 - sparse_categorical_crossentropy: 0.4297 - val_loss: 0.4040 - val_sparse_categorical_crossentropy: 0.4040\n",
            "Epoch 68/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4278 - sparse_categorical_crossentropy: 0.4278 - val_loss: 0.3986 - val_sparse_categorical_crossentropy: 0.3986\n",
            "Epoch 69/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4285 - sparse_categorical_crossentropy: 0.4285 - val_loss: 0.3987 - val_sparse_categorical_crossentropy: 0.3987\n",
            "Epoch 70/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4278 - sparse_categorical_crossentropy: 0.4278 - val_loss: 0.4026 - val_sparse_categorical_crossentropy: 0.4026\n",
            "Epoch 71/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4290 - sparse_categorical_crossentropy: 0.4290 - val_loss: 0.4039 - val_sparse_categorical_crossentropy: 0.4039\n",
            "Epoch 72/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4281 - sparse_categorical_crossentropy: 0.4281 - val_loss: 0.3963 - val_sparse_categorical_crossentropy: 0.3963\n",
            "Epoch 73/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4275 - sparse_categorical_crossentropy: 0.4275 - val_loss: 0.3931 - val_sparse_categorical_crossentropy: 0.3931\n",
            "Epoch 74/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4246 - sparse_categorical_crossentropy: 0.4246 - val_loss: 0.3970 - val_sparse_categorical_crossentropy: 0.3970\n",
            "Epoch 75/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4266 - sparse_categorical_crossentropy: 0.4266 - val_loss: 0.3962 - val_sparse_categorical_crossentropy: 0.3962\n",
            "Epoch 76/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4257 - sparse_categorical_crossentropy: 0.4257 - val_loss: 0.3982 - val_sparse_categorical_crossentropy: 0.3982\n",
            "Epoch 77/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4246 - sparse_categorical_crossentropy: 0.4246 - val_loss: 0.3999 - val_sparse_categorical_crossentropy: 0.3999\n",
            "Epoch 78/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4227 - sparse_categorical_crossentropy: 0.4227 - val_loss: 0.4125 - val_sparse_categorical_crossentropy: 0.4125\n",
            "Epoch 79/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4258 - sparse_categorical_crossentropy: 0.4258 - val_loss: 0.3990 - val_sparse_categorical_crossentropy: 0.3990\n",
            "Epoch 80/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4230 - sparse_categorical_crossentropy: 0.4230 - val_loss: 0.4042 - val_sparse_categorical_crossentropy: 0.4042\n",
            "Epoch 81/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4215 - sparse_categorical_crossentropy: 0.4215 - val_loss: 0.3958 - val_sparse_categorical_crossentropy: 0.3958\n",
            "Epoch 82/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4216 - sparse_categorical_crossentropy: 0.4216 - val_loss: 0.3971 - val_sparse_categorical_crossentropy: 0.3971\n",
            "Epoch 83/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4231 - sparse_categorical_crossentropy: 0.4231 - val_loss: 0.3935 - val_sparse_categorical_crossentropy: 0.3935\n",
            "Epoch 84/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4222 - sparse_categorical_crossentropy: 0.4222 - val_loss: 0.3970 - val_sparse_categorical_crossentropy: 0.3970\n",
            "Epoch 85/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4205 - sparse_categorical_crossentropy: 0.4205 - val_loss: 0.3975 - val_sparse_categorical_crossentropy: 0.3975\n",
            "Epoch 86/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4220 - sparse_categorical_crossentropy: 0.4220 - val_loss: 0.3986 - val_sparse_categorical_crossentropy: 0.3986\n",
            "Epoch 87/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4194 - sparse_categorical_crossentropy: 0.4194 - val_loss: 0.3936 - val_sparse_categorical_crossentropy: 0.3936\n",
            "Epoch 88/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4185 - sparse_categorical_crossentropy: 0.4185 - val_loss: 0.3928 - val_sparse_categorical_crossentropy: 0.3928\n",
            "Epoch 89/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4182 - sparse_categorical_crossentropy: 0.4182 - val_loss: 0.3896 - val_sparse_categorical_crossentropy: 0.3896\n",
            "Epoch 90/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4202 - sparse_categorical_crossentropy: 0.4202 - val_loss: 0.3976 - val_sparse_categorical_crossentropy: 0.3976\n",
            "Epoch 91/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4171 - sparse_categorical_crossentropy: 0.4171 - val_loss: 0.3905 - val_sparse_categorical_crossentropy: 0.3905\n",
            "Epoch 92/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4179 - sparse_categorical_crossentropy: 0.4179 - val_loss: 0.3930 - val_sparse_categorical_crossentropy: 0.3930\n",
            "Epoch 93/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4184 - sparse_categorical_crossentropy: 0.4184 - val_loss: 0.3936 - val_sparse_categorical_crossentropy: 0.3936\n",
            "Epoch 94/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4174 - sparse_categorical_crossentropy: 0.4174 - val_loss: 0.4003 - val_sparse_categorical_crossentropy: 0.4003\n",
            "Epoch 95/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4168 - sparse_categorical_crossentropy: 0.4168 - val_loss: 0.3911 - val_sparse_categorical_crossentropy: 0.3911\n",
            "Epoch 96/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4170 - sparse_categorical_crossentropy: 0.4170 - val_loss: 0.3911 - val_sparse_categorical_crossentropy: 0.3911\n",
            "Epoch 97/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4177 - sparse_categorical_crossentropy: 0.4177 - val_loss: 0.3942 - val_sparse_categorical_crossentropy: 0.3942\n",
            "Epoch 98/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4165 - sparse_categorical_crossentropy: 0.4165 - val_loss: 0.3888 - val_sparse_categorical_crossentropy: 0.3888\n",
            "Epoch 99/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4161 - sparse_categorical_crossentropy: 0.4161 - val_loss: 0.3892 - val_sparse_categorical_crossentropy: 0.3892\n",
            "Epoch 100/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4156 - sparse_categorical_crossentropy: 0.4156 - val_loss: 0.3874 - val_sparse_categorical_crossentropy: 0.3874\n",
            "Epoch 101/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4144 - sparse_categorical_crossentropy: 0.4144 - val_loss: 0.3954 - val_sparse_categorical_crossentropy: 0.3954\n",
            "Epoch 102/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4150 - sparse_categorical_crossentropy: 0.4150 - val_loss: 0.3907 - val_sparse_categorical_crossentropy: 0.3907\n",
            "Epoch 103/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4143 - sparse_categorical_crossentropy: 0.4143 - val_loss: 0.3957 - val_sparse_categorical_crossentropy: 0.3957\n",
            "Epoch 104/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4140 - sparse_categorical_crossentropy: 0.4140 - val_loss: 0.3886 - val_sparse_categorical_crossentropy: 0.3886\n",
            "Epoch 105/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4108 - sparse_categorical_crossentropy: 0.4108 - val_loss: 0.3889 - val_sparse_categorical_crossentropy: 0.3889\n",
            "Epoch 106/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4141 - sparse_categorical_crossentropy: 0.4141 - val_loss: 0.3929 - val_sparse_categorical_crossentropy: 0.3929\n",
            "Epoch 107/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4132 - sparse_categorical_crossentropy: 0.4132 - val_loss: 0.3931 - val_sparse_categorical_crossentropy: 0.3931\n",
            "Epoch 108/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4131 - sparse_categorical_crossentropy: 0.4131 - val_loss: 0.3863 - val_sparse_categorical_crossentropy: 0.3863\n",
            "Epoch 109/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4127 - sparse_categorical_crossentropy: 0.4127 - val_loss: 0.3922 - val_sparse_categorical_crossentropy: 0.3922\n",
            "Epoch 110/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4117 - sparse_categorical_crossentropy: 0.4117 - val_loss: 0.3858 - val_sparse_categorical_crossentropy: 0.3858\n",
            "Epoch 111/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4126 - sparse_categorical_crossentropy: 0.4126 - val_loss: 0.3907 - val_sparse_categorical_crossentropy: 0.3907\n",
            "Epoch 112/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4123 - sparse_categorical_crossentropy: 0.4123 - val_loss: 0.3987 - val_sparse_categorical_crossentropy: 0.3987\n",
            "Epoch 113/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4096 - sparse_categorical_crossentropy: 0.4096 - val_loss: 0.3820 - val_sparse_categorical_crossentropy: 0.3820\n",
            "Epoch 114/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4105 - sparse_categorical_crossentropy: 0.4105 - val_loss: 0.3877 - val_sparse_categorical_crossentropy: 0.3877\n",
            "Epoch 115/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4110 - sparse_categorical_crossentropy: 0.4110 - val_loss: 0.3912 - val_sparse_categorical_crossentropy: 0.3912\n",
            "Epoch 116/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4103 - sparse_categorical_crossentropy: 0.4103 - val_loss: 0.3846 - val_sparse_categorical_crossentropy: 0.3846\n",
            "Epoch 117/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4103 - sparse_categorical_crossentropy: 0.4103 - val_loss: 0.3863 - val_sparse_categorical_crossentropy: 0.3863\n",
            "Epoch 118/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4077 - sparse_categorical_crossentropy: 0.4077 - val_loss: 0.3839 - val_sparse_categorical_crossentropy: 0.3839\n",
            "Epoch 119/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4103 - sparse_categorical_crossentropy: 0.4103 - val_loss: 0.3858 - val_sparse_categorical_crossentropy: 0.3858\n",
            "Epoch 120/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4084 - sparse_categorical_crossentropy: 0.4084 - val_loss: 0.3902 - val_sparse_categorical_crossentropy: 0.3902\n",
            "Epoch 121/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4088 - sparse_categorical_crossentropy: 0.4088 - val_loss: 0.3832 - val_sparse_categorical_crossentropy: 0.3832\n",
            "Epoch 122/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4081 - sparse_categorical_crossentropy: 0.4081 - val_loss: 0.3854 - val_sparse_categorical_crossentropy: 0.3854\n",
            "Epoch 123/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4082 - sparse_categorical_crossentropy: 0.4082 - val_loss: 0.3913 - val_sparse_categorical_crossentropy: 0.3913\n",
            "Epoch 124/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4083 - sparse_categorical_crossentropy: 0.4083 - val_loss: 0.3880 - val_sparse_categorical_crossentropy: 0.3880\n",
            "Epoch 125/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4071 - sparse_categorical_crossentropy: 0.4071 - val_loss: 0.3848 - val_sparse_categorical_crossentropy: 0.3848\n",
            "Epoch 126/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4080 - sparse_categorical_crossentropy: 0.4080 - val_loss: 0.3836 - val_sparse_categorical_crossentropy: 0.3836\n",
            "Epoch 127/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4081 - sparse_categorical_crossentropy: 0.4081 - val_loss: 0.3874 - val_sparse_categorical_crossentropy: 0.3874\n",
            "Epoch 128/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4075 - sparse_categorical_crossentropy: 0.4075 - val_loss: 0.3863 - val_sparse_categorical_crossentropy: 0.3863\n",
            "Epoch 129/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4055 - sparse_categorical_crossentropy: 0.4055 - val_loss: 0.3871 - val_sparse_categorical_crossentropy: 0.3871\n",
            "Epoch 130/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4096 - sparse_categorical_crossentropy: 0.4096 - val_loss: 0.3863 - val_sparse_categorical_crossentropy: 0.3863\n",
            "Epoch 131/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4049 - sparse_categorical_crossentropy: 0.4049 - val_loss: 0.3916 - val_sparse_categorical_crossentropy: 0.3916\n",
            "Epoch 132/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4079 - sparse_categorical_crossentropy: 0.4079 - val_loss: 0.3922 - val_sparse_categorical_crossentropy: 0.3922\n",
            "Epoch 133/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4051 - sparse_categorical_crossentropy: 0.4051 - val_loss: 0.3900 - val_sparse_categorical_crossentropy: 0.3900\n",
            "Epoch 134/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4067 - sparse_categorical_crossentropy: 0.4067 - val_loss: 0.3868 - val_sparse_categorical_crossentropy: 0.3868\n",
            "Epoch 135/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.4049 - sparse_categorical_crossentropy: 0.4049 - val_loss: 0.3917 - val_sparse_categorical_crossentropy: 0.3917\n",
            "Epoch 136/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4059 - sparse_categorical_crossentropy: 0.4059 - val_loss: 0.3839 - val_sparse_categorical_crossentropy: 0.3839\n",
            "Epoch 137/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4037 - sparse_categorical_crossentropy: 0.4037 - val_loss: 0.3854 - val_sparse_categorical_crossentropy: 0.3854\n",
            "Epoch 138/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4061 - sparse_categorical_crossentropy: 0.4061 - val_loss: 0.3916 - val_sparse_categorical_crossentropy: 0.3916\n",
            "\n",
            "Epoch 00138: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.\n",
            "Epoch 139/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.4003 - sparse_categorical_crossentropy: 0.4003 - val_loss: 0.3840 - val_sparse_categorical_crossentropy: 0.3840\n",
            "Epoch 140/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3996 - sparse_categorical_crossentropy: 0.3996 - val_loss: 0.3797 - val_sparse_categorical_crossentropy: 0.3797\n",
            "Epoch 141/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3990 - sparse_categorical_crossentropy: 0.3990 - val_loss: 0.3775 - val_sparse_categorical_crossentropy: 0.3775\n",
            "Epoch 142/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3996 - sparse_categorical_crossentropy: 0.3996 - val_loss: 0.3799 - val_sparse_categorical_crossentropy: 0.3799\n",
            "Epoch 143/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3986 - sparse_categorical_crossentropy: 0.3986 - val_loss: 0.3779 - val_sparse_categorical_crossentropy: 0.3779\n",
            "Epoch 144/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3983 - sparse_categorical_crossentropy: 0.3983 - val_loss: 0.3802 - val_sparse_categorical_crossentropy: 0.3802\n",
            "Epoch 145/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3988 - sparse_categorical_crossentropy: 0.3988 - val_loss: 0.3804 - val_sparse_categorical_crossentropy: 0.3804\n",
            "Epoch 146/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3981 - sparse_categorical_crossentropy: 0.3981 - val_loss: 0.3804 - val_sparse_categorical_crossentropy: 0.3804\n",
            "Epoch 147/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3979 - sparse_categorical_crossentropy: 0.3979 - val_loss: 0.3795 - val_sparse_categorical_crossentropy: 0.3795\n",
            "Epoch 148/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3970 - sparse_categorical_crossentropy: 0.3970 - val_loss: 0.3777 - val_sparse_categorical_crossentropy: 0.3777\n",
            "Epoch 149/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3975 - sparse_categorical_crossentropy: 0.3975 - val_loss: 0.3775 - val_sparse_categorical_crossentropy: 0.3775\n",
            "Epoch 150/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3975 - sparse_categorical_crossentropy: 0.3975 - val_loss: 0.3785 - val_sparse_categorical_crossentropy: 0.3785\n",
            "Epoch 151/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3961 - sparse_categorical_crossentropy: 0.3961 - val_loss: 0.3794 - val_sparse_categorical_crossentropy: 0.3794\n",
            "Epoch 152/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3975 - sparse_categorical_crossentropy: 0.3975 - val_loss: 0.3791 - val_sparse_categorical_crossentropy: 0.3791\n",
            "Epoch 153/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3961 - sparse_categorical_crossentropy: 0.3961 - val_loss: 0.3785 - val_sparse_categorical_crossentropy: 0.3785\n",
            "Epoch 154/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3971 - sparse_categorical_crossentropy: 0.3971 - val_loss: 0.3771 - val_sparse_categorical_crossentropy: 0.3771\n",
            "Epoch 155/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3965 - sparse_categorical_crossentropy: 0.3965 - val_loss: 0.3782 - val_sparse_categorical_crossentropy: 0.3782\n",
            "Epoch 156/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3960 - sparse_categorical_crossentropy: 0.3960 - val_loss: 0.3802 - val_sparse_categorical_crossentropy: 0.3802\n",
            "Epoch 157/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3964 - sparse_categorical_crossentropy: 0.3964 - val_loss: 0.3774 - val_sparse_categorical_crossentropy: 0.3774\n",
            "Epoch 158/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3962 - sparse_categorical_crossentropy: 0.3962 - val_loss: 0.3817 - val_sparse_categorical_crossentropy: 0.3817\n",
            "Epoch 159/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3964 - sparse_categorical_crossentropy: 0.3964 - val_loss: 0.3782 - val_sparse_categorical_crossentropy: 0.3782\n",
            "Epoch 160/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3966 - sparse_categorical_crossentropy: 0.3966 - val_loss: 0.3790 - val_sparse_categorical_crossentropy: 0.3790\n",
            "Epoch 161/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3950 - sparse_categorical_crossentropy: 0.3950 - val_loss: 0.3795 - val_sparse_categorical_crossentropy: 0.3795\n",
            "Epoch 162/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3959 - sparse_categorical_crossentropy: 0.3959 - val_loss: 0.3769 - val_sparse_categorical_crossentropy: 0.3769\n",
            "Epoch 163/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3945 - sparse_categorical_crossentropy: 0.3945 - val_loss: 0.3774 - val_sparse_categorical_crossentropy: 0.3774\n",
            "Epoch 164/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3963 - sparse_categorical_crossentropy: 0.3963 - val_loss: 0.3774 - val_sparse_categorical_crossentropy: 0.3774\n",
            "Epoch 165/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3943 - sparse_categorical_crossentropy: 0.3943 - val_loss: 0.3784 - val_sparse_categorical_crossentropy: 0.3784\n",
            "Epoch 166/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3957 - sparse_categorical_crossentropy: 0.3957 - val_loss: 0.3802 - val_sparse_categorical_crossentropy: 0.3802\n",
            "Epoch 167/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3943 - sparse_categorical_crossentropy: 0.3943 - val_loss: 0.3778 - val_sparse_categorical_crossentropy: 0.3778\n",
            "Epoch 168/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3946 - sparse_categorical_crossentropy: 0.3946 - val_loss: 0.3780 - val_sparse_categorical_crossentropy: 0.3780\n",
            "Epoch 169/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3936 - sparse_categorical_crossentropy: 0.3936 - val_loss: 0.3779 - val_sparse_categorical_crossentropy: 0.3779\n",
            "Epoch 170/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3948 - sparse_categorical_crossentropy: 0.3948 - val_loss: 0.3781 - val_sparse_categorical_crossentropy: 0.3781\n",
            "Epoch 171/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3949 - sparse_categorical_crossentropy: 0.3949 - val_loss: 0.3765 - val_sparse_categorical_crossentropy: 0.3765\n",
            "Epoch 172/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3951 - sparse_categorical_crossentropy: 0.3951 - val_loss: 0.3767 - val_sparse_categorical_crossentropy: 0.3767\n",
            "Epoch 173/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3942 - sparse_categorical_crossentropy: 0.3942 - val_loss: 0.3759 - val_sparse_categorical_crossentropy: 0.3759\n",
            "Epoch 174/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3923 - sparse_categorical_crossentropy: 0.3923 - val_loss: 0.3771 - val_sparse_categorical_crossentropy: 0.3771\n",
            "Epoch 175/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3948 - sparse_categorical_crossentropy: 0.3948 - val_loss: 0.3778 - val_sparse_categorical_crossentropy: 0.3778\n",
            "Epoch 176/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3937 - sparse_categorical_crossentropy: 0.3937 - val_loss: 0.3780 - val_sparse_categorical_crossentropy: 0.3780\n",
            "Epoch 177/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3941 - sparse_categorical_crossentropy: 0.3941 - val_loss: 0.3762 - val_sparse_categorical_crossentropy: 0.3762\n",
            "Epoch 178/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3954 - sparse_categorical_crossentropy: 0.3954 - val_loss: 0.3752 - val_sparse_categorical_crossentropy: 0.3752\n",
            "Epoch 179/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3939 - sparse_categorical_crossentropy: 0.3939 - val_loss: 0.3767 - val_sparse_categorical_crossentropy: 0.3767\n",
            "Epoch 180/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3947 - sparse_categorical_crossentropy: 0.3947 - val_loss: 0.3776 - val_sparse_categorical_crossentropy: 0.3776\n",
            "Epoch 181/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3947 - sparse_categorical_crossentropy: 0.3947 - val_loss: 0.3830 - val_sparse_categorical_crossentropy: 0.3830\n",
            "Epoch 182/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3948 - sparse_categorical_crossentropy: 0.3948 - val_loss: 0.3751 - val_sparse_categorical_crossentropy: 0.3751\n",
            "Epoch 183/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3945 - sparse_categorical_crossentropy: 0.3945 - val_loss: 0.3770 - val_sparse_categorical_crossentropy: 0.3770\n",
            "Epoch 184/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3936 - sparse_categorical_crossentropy: 0.3936 - val_loss: 0.3767 - val_sparse_categorical_crossentropy: 0.3767\n",
            "Epoch 185/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3931 - sparse_categorical_crossentropy: 0.3931 - val_loss: 0.3780 - val_sparse_categorical_crossentropy: 0.3780\n",
            "Epoch 186/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3938 - sparse_categorical_crossentropy: 0.3938 - val_loss: 0.3775 - val_sparse_categorical_crossentropy: 0.3775\n",
            "Epoch 187/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3932 - sparse_categorical_crossentropy: 0.3932 - val_loss: 0.3764 - val_sparse_categorical_crossentropy: 0.3764\n",
            "Epoch 188/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3936 - sparse_categorical_crossentropy: 0.3936 - val_loss: 0.3750 - val_sparse_categorical_crossentropy: 0.3750\n",
            "Epoch 189/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3932 - sparse_categorical_crossentropy: 0.3932 - val_loss: 0.3772 - val_sparse_categorical_crossentropy: 0.3772\n",
            "Epoch 190/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3938 - sparse_categorical_crossentropy: 0.3938 - val_loss: 0.3773 - val_sparse_categorical_crossentropy: 0.3773\n",
            "Epoch 191/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3936 - sparse_categorical_crossentropy: 0.3936 - val_loss: 0.3776 - val_sparse_categorical_crossentropy: 0.3776\n",
            "Epoch 192/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3939 - sparse_categorical_crossentropy: 0.3939 - val_loss: 0.3750 - val_sparse_categorical_crossentropy: 0.3750\n",
            "Epoch 193/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3936 - sparse_categorical_crossentropy: 0.3936 - val_loss: 0.3760 - val_sparse_categorical_crossentropy: 0.3760\n",
            "Epoch 194/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3926 - sparse_categorical_crossentropy: 0.3926 - val_loss: 0.3771 - val_sparse_categorical_crossentropy: 0.3771\n",
            "Epoch 195/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3916 - sparse_categorical_crossentropy: 0.3916 - val_loss: 0.3747 - val_sparse_categorical_crossentropy: 0.3747\n",
            "Epoch 196/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3932 - sparse_categorical_crossentropy: 0.3932 - val_loss: 0.3757 - val_sparse_categorical_crossentropy: 0.3757\n",
            "Epoch 197/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3928 - sparse_categorical_crossentropy: 0.3928 - val_loss: 0.3757 - val_sparse_categorical_crossentropy: 0.3757\n",
            "Epoch 198/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3933 - sparse_categorical_crossentropy: 0.3933 - val_loss: 0.3767 - val_sparse_categorical_crossentropy: 0.3767\n",
            "Epoch 199/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3917 - sparse_categorical_crossentropy: 0.3917 - val_loss: 0.3761 - val_sparse_categorical_crossentropy: 0.3761\n",
            "Epoch 200/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3921 - sparse_categorical_crossentropy: 0.3921 - val_loss: 0.3807 - val_sparse_categorical_crossentropy: 0.3807\n",
            "Epoch 201/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3929 - sparse_categorical_crossentropy: 0.3929 - val_loss: 0.3759 - val_sparse_categorical_crossentropy: 0.3759\n",
            "Epoch 202/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3921 - sparse_categorical_crossentropy: 0.3921 - val_loss: 0.3751 - val_sparse_categorical_crossentropy: 0.3751\n",
            "Epoch 203/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3921 - sparse_categorical_crossentropy: 0.3921 - val_loss: 0.3758 - val_sparse_categorical_crossentropy: 0.3758\n",
            "Epoch 204/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3917 - sparse_categorical_crossentropy: 0.3917 - val_loss: 0.3767 - val_sparse_categorical_crossentropy: 0.3767\n",
            "Epoch 205/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3924 - sparse_categorical_crossentropy: 0.3924 - val_loss: 0.3801 - val_sparse_categorical_crossentropy: 0.3801\n",
            "Epoch 206/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3919 - sparse_categorical_crossentropy: 0.3919 - val_loss: 0.3771 - val_sparse_categorical_crossentropy: 0.3771\n",
            "Epoch 207/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3919 - sparse_categorical_crossentropy: 0.3919 - val_loss: 0.3752 - val_sparse_categorical_crossentropy: 0.3752\n",
            "Epoch 208/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3931 - sparse_categorical_crossentropy: 0.3931 - val_loss: 0.3758 - val_sparse_categorical_crossentropy: 0.3758\n",
            "Epoch 209/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3922 - sparse_categorical_crossentropy: 0.3922 - val_loss: 0.3758 - val_sparse_categorical_crossentropy: 0.3758\n",
            "Epoch 210/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3918 - sparse_categorical_crossentropy: 0.3918 - val_loss: 0.3753 - val_sparse_categorical_crossentropy: 0.3753\n",
            "Epoch 211/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3919 - sparse_categorical_crossentropy: 0.3919 - val_loss: 0.3758 - val_sparse_categorical_crossentropy: 0.3758\n",
            "Epoch 212/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3914 - sparse_categorical_crossentropy: 0.3914 - val_loss: 0.3766 - val_sparse_categorical_crossentropy: 0.3766\n",
            "Epoch 213/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3920 - sparse_categorical_crossentropy: 0.3920 - val_loss: 0.3774 - val_sparse_categorical_crossentropy: 0.3774\n",
            "Epoch 214/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3911 - sparse_categorical_crossentropy: 0.3911 - val_loss: 0.3747 - val_sparse_categorical_crossentropy: 0.3747\n",
            "Epoch 215/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3915 - sparse_categorical_crossentropy: 0.3915 - val_loss: 0.3760 - val_sparse_categorical_crossentropy: 0.3760\n",
            "Epoch 216/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3904 - sparse_categorical_crossentropy: 0.3904 - val_loss: 0.3742 - val_sparse_categorical_crossentropy: 0.3742\n",
            "Epoch 217/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3907 - sparse_categorical_crossentropy: 0.3907 - val_loss: 0.3752 - val_sparse_categorical_crossentropy: 0.3752\n",
            "Epoch 218/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3914 - sparse_categorical_crossentropy: 0.3914 - val_loss: 0.3745 - val_sparse_categorical_crossentropy: 0.3745\n",
            "Epoch 219/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3912 - sparse_categorical_crossentropy: 0.3912 - val_loss: 0.3748 - val_sparse_categorical_crossentropy: 0.3748\n",
            "Epoch 220/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3911 - sparse_categorical_crossentropy: 0.3911 - val_loss: 0.3748 - val_sparse_categorical_crossentropy: 0.3748\n",
            "Epoch 221/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3913 - sparse_categorical_crossentropy: 0.3913 - val_loss: 0.3751 - val_sparse_categorical_crossentropy: 0.3751\n",
            "Epoch 222/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3911 - sparse_categorical_crossentropy: 0.3911 - val_loss: 0.3756 - val_sparse_categorical_crossentropy: 0.3756\n",
            "Epoch 223/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3916 - sparse_categorical_crossentropy: 0.3916 - val_loss: 0.3743 - val_sparse_categorical_crossentropy: 0.3743\n",
            "Epoch 224/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3903 - sparse_categorical_crossentropy: 0.3903 - val_loss: 0.3755 - val_sparse_categorical_crossentropy: 0.3755\n",
            "Epoch 225/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3909 - sparse_categorical_crossentropy: 0.3909 - val_loss: 0.3775 - val_sparse_categorical_crossentropy: 0.3775\n",
            "Epoch 226/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3903 - sparse_categorical_crossentropy: 0.3903 - val_loss: 0.3758 - val_sparse_categorical_crossentropy: 0.3758\n",
            "Epoch 227/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3905 - sparse_categorical_crossentropy: 0.3905 - val_loss: 0.3787 - val_sparse_categorical_crossentropy: 0.3787\n",
            "Epoch 228/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3902 - sparse_categorical_crossentropy: 0.3902 - val_loss: 0.3755 - val_sparse_categorical_crossentropy: 0.3755\n",
            "Epoch 229/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3898 - sparse_categorical_crossentropy: 0.3898 - val_loss: 0.3750 - val_sparse_categorical_crossentropy: 0.3750\n",
            "Epoch 230/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3907 - sparse_categorical_crossentropy: 0.3907 - val_loss: 0.3762 - val_sparse_categorical_crossentropy: 0.3762\n",
            "Epoch 231/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3898 - sparse_categorical_crossentropy: 0.3898 - val_loss: 0.3753 - val_sparse_categorical_crossentropy: 0.3753\n",
            "Epoch 232/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3903 - sparse_categorical_crossentropy: 0.3903 - val_loss: 0.3740 - val_sparse_categorical_crossentropy: 0.3740\n",
            "Epoch 233/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3899 - sparse_categorical_crossentropy: 0.3899 - val_loss: 0.3745 - val_sparse_categorical_crossentropy: 0.3745\n",
            "Epoch 234/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3892 - sparse_categorical_crossentropy: 0.3892 - val_loss: 0.3757 - val_sparse_categorical_crossentropy: 0.3757\n",
            "Epoch 235/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3909 - sparse_categorical_crossentropy: 0.3909 - val_loss: 0.3785 - val_sparse_categorical_crossentropy: 0.3785\n",
            "Epoch 236/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3910 - sparse_categorical_crossentropy: 0.3910 - val_loss: 0.3746 - val_sparse_categorical_crossentropy: 0.3746\n",
            "Epoch 237/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3897 - sparse_categorical_crossentropy: 0.3897 - val_loss: 0.3740 - val_sparse_categorical_crossentropy: 0.3740\n",
            "Epoch 238/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3896 - sparse_categorical_crossentropy: 0.3896 - val_loss: 0.3746 - val_sparse_categorical_crossentropy: 0.3746\n",
            "Epoch 239/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3896 - sparse_categorical_crossentropy: 0.3896 - val_loss: 0.3750 - val_sparse_categorical_crossentropy: 0.3750\n",
            "Epoch 240/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3904 - sparse_categorical_crossentropy: 0.3904 - val_loss: 0.3742 - val_sparse_categorical_crossentropy: 0.3742\n",
            "Epoch 241/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3892 - sparse_categorical_crossentropy: 0.3892 - val_loss: 0.3740 - val_sparse_categorical_crossentropy: 0.3740\n",
            "Epoch 242/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3897 - sparse_categorical_crossentropy: 0.3897 - val_loss: 0.3749 - val_sparse_categorical_crossentropy: 0.3749\n",
            "Epoch 243/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3886 - sparse_categorical_crossentropy: 0.3886 - val_loss: 0.3746 - val_sparse_categorical_crossentropy: 0.3746\n",
            "Epoch 244/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3895 - sparse_categorical_crossentropy: 0.3895 - val_loss: 0.3750 - val_sparse_categorical_crossentropy: 0.3750\n",
            "Epoch 245/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3883 - sparse_categorical_crossentropy: 0.3883 - val_loss: 0.3769 - val_sparse_categorical_crossentropy: 0.3769\n",
            "Epoch 246/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3891 - sparse_categorical_crossentropy: 0.3891 - val_loss: 0.3758 - val_sparse_categorical_crossentropy: 0.3758\n",
            "Epoch 247/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3890 - sparse_categorical_crossentropy: 0.3890 - val_loss: 0.3739 - val_sparse_categorical_crossentropy: 0.3739\n",
            "Epoch 248/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3888 - sparse_categorical_crossentropy: 0.3888 - val_loss: 0.3742 - val_sparse_categorical_crossentropy: 0.3742\n",
            "Epoch 249/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3899 - sparse_categorical_crossentropy: 0.3899 - val_loss: 0.3724 - val_sparse_categorical_crossentropy: 0.3724\n",
            "Epoch 250/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3901 - sparse_categorical_crossentropy: 0.3901 - val_loss: 0.3754 - val_sparse_categorical_crossentropy: 0.3754\n",
            "Epoch 251/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3889 - sparse_categorical_crossentropy: 0.3889 - val_loss: 0.3748 - val_sparse_categorical_crossentropy: 0.3748\n",
            "Epoch 252/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3885 - sparse_categorical_crossentropy: 0.3885 - val_loss: 0.3767 - val_sparse_categorical_crossentropy: 0.3767\n",
            "Epoch 253/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3888 - sparse_categorical_crossentropy: 0.3888 - val_loss: 0.3742 - val_sparse_categorical_crossentropy: 0.3742\n",
            "Epoch 254/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3888 - sparse_categorical_crossentropy: 0.3888 - val_loss: 0.3729 - val_sparse_categorical_crossentropy: 0.3729\n",
            "Epoch 255/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3883 - sparse_categorical_crossentropy: 0.3883 - val_loss: 0.3747 - val_sparse_categorical_crossentropy: 0.3747\n",
            "Epoch 256/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3891 - sparse_categorical_crossentropy: 0.3891 - val_loss: 0.3738 - val_sparse_categorical_crossentropy: 0.3738\n",
            "Epoch 257/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3898 - sparse_categorical_crossentropy: 0.3898 - val_loss: 0.3739 - val_sparse_categorical_crossentropy: 0.3739\n",
            "Epoch 258/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3890 - sparse_categorical_crossentropy: 0.3890 - val_loss: 0.3751 - val_sparse_categorical_crossentropy: 0.3751\n",
            "Epoch 259/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3873 - sparse_categorical_crossentropy: 0.3873 - val_loss: 0.3737 - val_sparse_categorical_crossentropy: 0.3737\n",
            "Epoch 260/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3890 - sparse_categorical_crossentropy: 0.3890 - val_loss: 0.3772 - val_sparse_categorical_crossentropy: 0.3772\n",
            "Epoch 261/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3885 - sparse_categorical_crossentropy: 0.3885 - val_loss: 0.3732 - val_sparse_categorical_crossentropy: 0.3732\n",
            "Epoch 262/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3896 - sparse_categorical_crossentropy: 0.3896 - val_loss: 0.3745 - val_sparse_categorical_crossentropy: 0.3745\n",
            "Epoch 263/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3885 - sparse_categorical_crossentropy: 0.3885 - val_loss: 0.3748 - val_sparse_categorical_crossentropy: 0.3748\n",
            "Epoch 264/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3896 - sparse_categorical_crossentropy: 0.3896 - val_loss: 0.3753 - val_sparse_categorical_crossentropy: 0.3753\n",
            "Epoch 265/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3881 - sparse_categorical_crossentropy: 0.3881 - val_loss: 0.3734 - val_sparse_categorical_crossentropy: 0.3734\n",
            "Epoch 266/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3882 - sparse_categorical_crossentropy: 0.3882 - val_loss: 0.3739 - val_sparse_categorical_crossentropy: 0.3739\n",
            "Epoch 267/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3890 - sparse_categorical_crossentropy: 0.3890 - val_loss: 0.3753 - val_sparse_categorical_crossentropy: 0.3753\n",
            "Epoch 268/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3882 - sparse_categorical_crossentropy: 0.3882 - val_loss: 0.3732 - val_sparse_categorical_crossentropy: 0.3732\n",
            "Epoch 269/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3874 - sparse_categorical_crossentropy: 0.3874 - val_loss: 0.3734 - val_sparse_categorical_crossentropy: 0.3734\n",
            "Epoch 270/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3875 - sparse_categorical_crossentropy: 0.3875 - val_loss: 0.3745 - val_sparse_categorical_crossentropy: 0.3745\n",
            "Epoch 271/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3874 - sparse_categorical_crossentropy: 0.3874 - val_loss: 0.3759 - val_sparse_categorical_crossentropy: 0.3759\n",
            "Epoch 272/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3878 - sparse_categorical_crossentropy: 0.3878 - val_loss: 0.3743 - val_sparse_categorical_crossentropy: 0.3743\n",
            "Epoch 273/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3875 - sparse_categorical_crossentropy: 0.3875 - val_loss: 0.3759 - val_sparse_categorical_crossentropy: 0.3759\n",
            "Epoch 274/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3875 - sparse_categorical_crossentropy: 0.3875 - val_loss: 0.3740 - val_sparse_categorical_crossentropy: 0.3740\n",
            "\n",
            "Epoch 00274: ReduceLROnPlateau reducing learning rate to 9.999999259090306e-06.\n",
            "Epoch 275/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3854 - sparse_categorical_crossentropy: 0.3854 - val_loss: 0.3730 - val_sparse_categorical_crossentropy: 0.3730\n",
            "Epoch 276/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3864 - sparse_categorical_crossentropy: 0.3864 - val_loss: 0.3720 - val_sparse_categorical_crossentropy: 0.3720\n",
            "Epoch 277/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3870 - sparse_categorical_crossentropy: 0.3870 - val_loss: 0.3730 - val_sparse_categorical_crossentropy: 0.3730\n",
            "Epoch 278/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3849 - sparse_categorical_crossentropy: 0.3849 - val_loss: 0.3719 - val_sparse_categorical_crossentropy: 0.3719\n",
            "Epoch 279/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3857 - sparse_categorical_crossentropy: 0.3857 - val_loss: 0.3728 - val_sparse_categorical_crossentropy: 0.3728\n",
            "Epoch 280/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3853 - sparse_categorical_crossentropy: 0.3853 - val_loss: 0.3729 - val_sparse_categorical_crossentropy: 0.3729\n",
            "Epoch 281/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3861 - sparse_categorical_crossentropy: 0.3861 - val_loss: 0.3723 - val_sparse_categorical_crossentropy: 0.3723\n",
            "Epoch 282/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3856 - sparse_categorical_crossentropy: 0.3856 - val_loss: 0.3725 - val_sparse_categorical_crossentropy: 0.3725\n",
            "Epoch 283/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3853 - sparse_categorical_crossentropy: 0.3853 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 284/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3854 - sparse_categorical_crossentropy: 0.3854 - val_loss: 0.3724 - val_sparse_categorical_crossentropy: 0.3724\n",
            "Epoch 285/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3860 - sparse_categorical_crossentropy: 0.3860 - val_loss: 0.3723 - val_sparse_categorical_crossentropy: 0.3723\n",
            "Epoch 286/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3851 - sparse_categorical_crossentropy: 0.3851 - val_loss: 0.3730 - val_sparse_categorical_crossentropy: 0.3730\n",
            "Epoch 287/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3857 - sparse_categorical_crossentropy: 0.3857 - val_loss: 0.3722 - val_sparse_categorical_crossentropy: 0.3722\n",
            "Epoch 288/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3850 - sparse_categorical_crossentropy: 0.3850 - val_loss: 0.3729 - val_sparse_categorical_crossentropy: 0.3729\n",
            "Epoch 289/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3846 - sparse_categorical_crossentropy: 0.3846 - val_loss: 0.3721 - val_sparse_categorical_crossentropy: 0.3721\n",
            "Epoch 290/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3846 - sparse_categorical_crossentropy: 0.3846 - val_loss: 0.3725 - val_sparse_categorical_crossentropy: 0.3725\n",
            "Epoch 291/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3850 - sparse_categorical_crossentropy: 0.3850 - val_loss: 0.3726 - val_sparse_categorical_crossentropy: 0.3726\n",
            "Epoch 292/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3851 - sparse_categorical_crossentropy: 0.3851 - val_loss: 0.3729 - val_sparse_categorical_crossentropy: 0.3729\n",
            "Epoch 293/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3845 - sparse_categorical_crossentropy: 0.3845 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 294/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3848 - sparse_categorical_crossentropy: 0.3848 - val_loss: 0.3724 - val_sparse_categorical_crossentropy: 0.3724\n",
            "Epoch 295/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3846 - sparse_categorical_crossentropy: 0.3846 - val_loss: 0.3722 - val_sparse_categorical_crossentropy: 0.3722\n",
            "Epoch 296/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3851 - sparse_categorical_crossentropy: 0.3851 - val_loss: 0.3722 - val_sparse_categorical_crossentropy: 0.3722\n",
            "Epoch 297/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3842 - sparse_categorical_crossentropy: 0.3842 - val_loss: 0.3724 - val_sparse_categorical_crossentropy: 0.3724\n",
            "Epoch 298/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3847 - sparse_categorical_crossentropy: 0.3847 - val_loss: 0.3724 - val_sparse_categorical_crossentropy: 0.3724\n",
            "Epoch 299/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3838 - sparse_categorical_crossentropy: 0.3838 - val_loss: 0.3719 - val_sparse_categorical_crossentropy: 0.3719\n",
            "Epoch 300/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3850 - sparse_categorical_crossentropy: 0.3850 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 301/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3856 - sparse_categorical_crossentropy: 0.3856 - val_loss: 0.3720 - val_sparse_categorical_crossentropy: 0.3720\n",
            "Epoch 302/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3858 - sparse_categorical_crossentropy: 0.3858 - val_loss: 0.3730 - val_sparse_categorical_crossentropy: 0.3730\n",
            "Epoch 303/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3851 - sparse_categorical_crossentropy: 0.3851 - val_loss: 0.3725 - val_sparse_categorical_crossentropy: 0.3725\n",
            "Epoch 304/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3840 - sparse_categorical_crossentropy: 0.3840 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 305/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3856 - sparse_categorical_crossentropy: 0.3856 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 306/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3860 - sparse_categorical_crossentropy: 0.3860 - val_loss: 0.3723 - val_sparse_categorical_crossentropy: 0.3723\n",
            "Epoch 307/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3847 - sparse_categorical_crossentropy: 0.3847 - val_loss: 0.3719 - val_sparse_categorical_crossentropy: 0.3719\n",
            "Epoch 308/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3853 - sparse_categorical_crossentropy: 0.3853 - val_loss: 0.3728 - val_sparse_categorical_crossentropy: 0.3728\n",
            "Epoch 309/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3849 - sparse_categorical_crossentropy: 0.3849 - val_loss: 0.3723 - val_sparse_categorical_crossentropy: 0.3723\n",
            "Epoch 310/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3849 - sparse_categorical_crossentropy: 0.3849 - val_loss: 0.3728 - val_sparse_categorical_crossentropy: 0.3728\n",
            "Epoch 311/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3848 - sparse_categorical_crossentropy: 0.3848 - val_loss: 0.3730 - val_sparse_categorical_crossentropy: 0.3730\n",
            "Epoch 312/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3843 - sparse_categorical_crossentropy: 0.3843 - val_loss: 0.3726 - val_sparse_categorical_crossentropy: 0.3726\n",
            "Epoch 313/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3849 - sparse_categorical_crossentropy: 0.3849 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 314/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3845 - sparse_categorical_crossentropy: 0.3845 - val_loss: 0.3720 - val_sparse_categorical_crossentropy: 0.3720\n",
            "Epoch 315/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3837 - sparse_categorical_crossentropy: 0.3837 - val_loss: 0.3718 - val_sparse_categorical_crossentropy: 0.3718\n",
            "Epoch 316/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3851 - sparse_categorical_crossentropy: 0.3851 - val_loss: 0.3718 - val_sparse_categorical_crossentropy: 0.3718\n",
            "Epoch 317/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3838 - sparse_categorical_crossentropy: 0.3838 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 318/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3850 - sparse_categorical_crossentropy: 0.3850 - val_loss: 0.3741 - val_sparse_categorical_crossentropy: 0.3741\n",
            "\n",
            "Epoch 00318: ReduceLROnPlateau reducing learning rate to 3.162277292675049e-06.\n",
            "Epoch 319/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3837 - sparse_categorical_crossentropy: 0.3837 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 320/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3847 - sparse_categorical_crossentropy: 0.3847 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 321/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3842 - sparse_categorical_crossentropy: 0.3842 - val_loss: 0.3718 - val_sparse_categorical_crossentropy: 0.3718\n",
            "Epoch 322/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3844 - sparse_categorical_crossentropy: 0.3844 - val_loss: 0.3718 - val_sparse_categorical_crossentropy: 0.3718\n",
            "Epoch 323/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3833 - sparse_categorical_crossentropy: 0.3833 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 324/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3846 - sparse_categorical_crossentropy: 0.3846 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 325/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3840 - sparse_categorical_crossentropy: 0.3840 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 326/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3847 - sparse_categorical_crossentropy: 0.3847 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 327/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3838 - sparse_categorical_crossentropy: 0.3838 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 328/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3823 - sparse_categorical_crossentropy: 0.3823 - val_loss: 0.3720 - val_sparse_categorical_crossentropy: 0.3720\n",
            "Epoch 329/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3839 - sparse_categorical_crossentropy: 0.3839 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 330/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3850 - sparse_categorical_crossentropy: 0.3850 - val_loss: 0.3718 - val_sparse_categorical_crossentropy: 0.3718\n",
            "Epoch 331/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3840 - sparse_categorical_crossentropy: 0.3840 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 332/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3832 - sparse_categorical_crossentropy: 0.3832 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 333/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3845 - sparse_categorical_crossentropy: 0.3845 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 334/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3846 - sparse_categorical_crossentropy: 0.3846 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 335/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3837 - sparse_categorical_crossentropy: 0.3837 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 336/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3844 - sparse_categorical_crossentropy: 0.3844 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 337/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3838 - sparse_categorical_crossentropy: 0.3838 - val_loss: 0.3710 - val_sparse_categorical_crossentropy: 0.3710\n",
            "Epoch 338/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3826 - sparse_categorical_crossentropy: 0.3826 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 339/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3837 - sparse_categorical_crossentropy: 0.3837 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 340/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3838 - sparse_categorical_crossentropy: 0.3838 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 341/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3836 - sparse_categorical_crossentropy: 0.3836 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 342/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3833 - sparse_categorical_crossentropy: 0.3833 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 343/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3841 - sparse_categorical_crossentropy: 0.3841 - val_loss: 0.3719 - val_sparse_categorical_crossentropy: 0.3719\n",
            "Epoch 344/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3844 - sparse_categorical_crossentropy: 0.3844 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 345/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3846 - sparse_categorical_crossentropy: 0.3846 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 346/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3834 - sparse_categorical_crossentropy: 0.3834 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 347/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3829 - sparse_categorical_crossentropy: 0.3829 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 348/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3837 - sparse_categorical_crossentropy: 0.3837 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 349/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3847 - sparse_categorical_crossentropy: 0.3847 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 350/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3834 - sparse_categorical_crossentropy: 0.3834 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 351/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3842 - sparse_categorical_crossentropy: 0.3842 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 352/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3825 - sparse_categorical_crossentropy: 0.3825 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 353/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3824 - sparse_categorical_crossentropy: 0.3824 - val_loss: 0.3719 - val_sparse_categorical_crossentropy: 0.3719\n",
            "Epoch 354/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3834 - sparse_categorical_crossentropy: 0.3834 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 355/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 356/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3841 - sparse_categorical_crossentropy: 0.3841 - val_loss: 0.3721 - val_sparse_categorical_crossentropy: 0.3721\n",
            "Epoch 357/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 358/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 359/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3822 - sparse_categorical_crossentropy: 0.3822 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 360/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3839 - sparse_categorical_crossentropy: 0.3839 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 361/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3828 - sparse_categorical_crossentropy: 0.3828 - val_loss: 0.3710 - val_sparse_categorical_crossentropy: 0.3710\n",
            "Epoch 362/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3823 - sparse_categorical_crossentropy: 0.3823 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 363/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3842 - sparse_categorical_crossentropy: 0.3842 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 364/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3842 - sparse_categorical_crossentropy: 0.3842 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "\n",
            "Epoch 00364: ReduceLROnPlateau reducing learning rate to 9.999999115286567e-07.\n",
            "Epoch 365/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3828 - sparse_categorical_crossentropy: 0.3828 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 366/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 367/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3827 - sparse_categorical_crossentropy: 0.3827 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 368/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3841 - sparse_categorical_crossentropy: 0.3841 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 369/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3836 - sparse_categorical_crossentropy: 0.3836 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 370/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3839 - sparse_categorical_crossentropy: 0.3839 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 371/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3821 - sparse_categorical_crossentropy: 0.3821 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 372/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3822 - sparse_categorical_crossentropy: 0.3822 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 373/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3830 - sparse_categorical_crossentropy: 0.3830 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 374/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3835 - sparse_categorical_crossentropy: 0.3835 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 375/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 376/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3838 - sparse_categorical_crossentropy: 0.3838 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 377/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 378/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3828 - sparse_categorical_crossentropy: 0.3828 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 379/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3827 - sparse_categorical_crossentropy: 0.3827 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 380/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3841 - sparse_categorical_crossentropy: 0.3841 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 381/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3827 - sparse_categorical_crossentropy: 0.3827 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 382/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3842 - sparse_categorical_crossentropy: 0.3842 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 383/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3827 - sparse_categorical_crossentropy: 0.3827 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 384/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3830 - sparse_categorical_crossentropy: 0.3830 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 385/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3840 - sparse_categorical_crossentropy: 0.3840 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 386/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3826 - sparse_categorical_crossentropy: 0.3826 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 387/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3840 - sparse_categorical_crossentropy: 0.3840 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 388/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 389/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3821 - sparse_categorical_crossentropy: 0.3821 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "\n",
            "Epoch 00389: ReduceLROnPlateau reducing learning rate to 3.1622772926750485e-07.\n",
            "Epoch 390/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3832 - sparse_categorical_crossentropy: 0.3832 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 391/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3832 - sparse_categorical_crossentropy: 0.3832 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 392/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3824 - sparse_categorical_crossentropy: 0.3824 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 393/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3828 - sparse_categorical_crossentropy: 0.3828 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 394/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3832 - sparse_categorical_crossentropy: 0.3832 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 395/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3827 - sparse_categorical_crossentropy: 0.3827 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 396/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3843 - sparse_categorical_crossentropy: 0.3843 - val_loss: 0.3710 - val_sparse_categorical_crossentropy: 0.3710\n",
            "Epoch 397/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3821 - sparse_categorical_crossentropy: 0.3821 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 398/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3834 - sparse_categorical_crossentropy: 0.3834 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 399/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3844 - sparse_categorical_crossentropy: 0.3844 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 400/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3829 - sparse_categorical_crossentropy: 0.3829 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 401/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3821 - sparse_categorical_crossentropy: 0.3821 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 402/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3839 - sparse_categorical_crossentropy: 0.3839 - val_loss: 0.3710 - val_sparse_categorical_crossentropy: 0.3710\n",
            "Epoch 403/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3828 - sparse_categorical_crossentropy: 0.3828 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 404/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3840 - sparse_categorical_crossentropy: 0.3840 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 405/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3824 - sparse_categorical_crossentropy: 0.3824 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 406/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3833 - sparse_categorical_crossentropy: 0.3833 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 407/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3832 - sparse_categorical_crossentropy: 0.3832 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 408/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3826 - sparse_categorical_crossentropy: 0.3826 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 409/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3822 - sparse_categorical_crossentropy: 0.3822 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 410/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 411/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3825 - sparse_categorical_crossentropy: 0.3825 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 412/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3823 - sparse_categorical_crossentropy: 0.3823 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 413/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3844 - sparse_categorical_crossentropy: 0.3844 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 414/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3826 - sparse_categorical_crossentropy: 0.3826 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "\n",
            "Epoch 00414: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "Epoch 415/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 416/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3827 - sparse_categorical_crossentropy: 0.3827 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 417/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3836 - sparse_categorical_crossentropy: 0.3836 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 418/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3835 - sparse_categorical_crossentropy: 0.3835 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 419/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3832 - sparse_categorical_crossentropy: 0.3832 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 420/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3827 - sparse_categorical_crossentropy: 0.3827 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 421/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3842 - sparse_categorical_crossentropy: 0.3842 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 422/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3824 - sparse_categorical_crossentropy: 0.3824 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 423/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3830 - sparse_categorical_crossentropy: 0.3830 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 424/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3837 - sparse_categorical_crossentropy: 0.3837 - val_loss: 0.3716 - val_sparse_categorical_crossentropy: 0.3716\n",
            "Epoch 425/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3828 - sparse_categorical_crossentropy: 0.3828 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 426/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3833 - sparse_categorical_crossentropy: 0.3833 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 427/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3832 - sparse_categorical_crossentropy: 0.3832 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 428/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3837 - sparse_categorical_crossentropy: 0.3837 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 429/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3845 - sparse_categorical_crossentropy: 0.3845 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 430/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3838 - sparse_categorical_crossentropy: 0.3838 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 431/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3833 - sparse_categorical_crossentropy: 0.3833 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 432/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3829 - sparse_categorical_crossentropy: 0.3829 - val_loss: 0.3710 - val_sparse_categorical_crossentropy: 0.3710\n",
            "Epoch 433/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3830 - sparse_categorical_crossentropy: 0.3830 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 434/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3834 - sparse_categorical_crossentropy: 0.3834 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 435/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3830 - sparse_categorical_crossentropy: 0.3830 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 436/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3834 - sparse_categorical_crossentropy: 0.3834 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 437/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3830 - sparse_categorical_crossentropy: 0.3830 - val_loss: 0.3717 - val_sparse_categorical_crossentropy: 0.3717\n",
            "Epoch 438/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3818 - sparse_categorical_crossentropy: 0.3818 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 439/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3833 - sparse_categorical_crossentropy: 0.3833 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "\n",
            "Epoch 00439: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "Epoch 440/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3830 - sparse_categorical_crossentropy: 0.3830 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 441/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3835 - sparse_categorical_crossentropy: 0.3835 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Epoch 442/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3835 - sparse_categorical_crossentropy: 0.3835 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 443/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3825 - sparse_categorical_crossentropy: 0.3825 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 444/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 445/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3828 - sparse_categorical_crossentropy: 0.3828 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 446/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3822 - sparse_categorical_crossentropy: 0.3822 - val_loss: 0.3712 - val_sparse_categorical_crossentropy: 0.3712\n",
            "Epoch 447/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3820 - sparse_categorical_crossentropy: 0.3820 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 448/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3831 - sparse_categorical_crossentropy: 0.3831 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 449/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3825 - sparse_categorical_crossentropy: 0.3825 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 450/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3826 - sparse_categorical_crossentropy: 0.3826 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 451/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3836 - sparse_categorical_crossentropy: 0.3836 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 452/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3843 - sparse_categorical_crossentropy: 0.3843 - val_loss: 0.3715 - val_sparse_categorical_crossentropy: 0.3715\n",
            "Epoch 453/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3833 - sparse_categorical_crossentropy: 0.3833 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 454/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3829 - sparse_categorical_crossentropy: 0.3829 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 455/800\n",
            "184521/184521 [==============================] - 5s 28us/step - loss: 0.3832 - sparse_categorical_crossentropy: 0.3832 - val_loss: 0.3713 - val_sparse_categorical_crossentropy: 0.3713\n",
            "Epoch 456/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3822 - sparse_categorical_crossentropy: 0.3822 - val_loss: 0.3709 - val_sparse_categorical_crossentropy: 0.3709\n",
            "Epoch 457/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3844 - sparse_categorical_crossentropy: 0.3844 - val_loss: 0.3711 - val_sparse_categorical_crossentropy: 0.3711\n",
            "Epoch 458/800\n",
            "184521/184521 [==============================] - 5s 27us/step - loss: 0.3834 - sparse_categorical_crossentropy: 0.3834 - val_loss: 0.3714 - val_sparse_categorical_crossentropy: 0.3714\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00458: early stopping\n",
            "모델저장완료\n",
            "==================================================\n",
            "Loss와 ACC에 대한 Plot을 그립니다\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAETCAYAAABnSkJLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhkyAlFJWIgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "기존nn모델의 train loss를 출력합니다\n",
            "train, loss and metric: [0.35024120502668166, 0.35024120502668166]\n",
            "기존nn모델의 valid loss를 출력합니다\n",
            "valid, loss and metric: [0.37085384793893067, 0.37085384793893067]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5oJjBcLFzzL",
        "colab_type": "code",
        "outputId": "78c70466-585e-4ad1-e135-a3f59d8f7ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "savingpath_csv = 'drive/My Drive/데이콘_천체유형/파일제출/' + csv_folder + '/ELU_이상치1개더제거_initial_rate_%s.csv' % initial_rate\n",
        "\n",
        "print('final rate는 다음과 같습니다')\n",
        "print(hist.history['lr'][np.argmin(hist.history['val_loss'])])\n",
        "print('='*25)\n",
        "y_pred = nn_model.predict(test_x)\n",
        "submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n",
        "submission.to_csv(savingpath_csv, index=True)\n",
        "print('csv 저장완료')\n",
        "\n",
        "print('='*50)\n",
        "best_val_loss = np.min(hist.history['val_loss'])\n",
        "print(\"best_valid_loss: {}\".format(best_val_loss))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final rate는 다음과 같습니다\n",
            "9.999999e-07\n",
            "=========================\n",
            "csv 저장완료\n",
            "==================================================\n",
            "best_valid_loss: 0.37085384793893067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7UR0QNdWEF9",
        "colab_type": "text"
      },
      "source": [
        "# 저장된 모델 로드해오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1W33dlgNXSTo",
        "outputId": "9084d431-0d8c-4358-bfdd-77be9e884eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "initial_rate = 1e-2\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "\n",
        "## model load\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "from keras.models import model_from_json\n",
        "json_file = open(json_path, \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "## model weight load\n",
        "loaded_model.load_weights(weight_path)\n",
        "\n",
        "## model load and evaluation\n",
        "from keras import optimizers\n",
        "final_rate = hist.history['lr'][np.argmin(hist.history['val_loss'])]\n",
        "print(\"Loaded model from disk\")\n",
        "load_optimizer = optimizers.Adam(\n",
        "    lr=final_rate,\n",
        ")\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "loaded_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=load_optimizer\n",
        "                     ,metrics=[CCE]\n",
        "                     )\n",
        "batchsize=1024\n",
        "train_score = loaded_model.evaluate(train_input,train_target,verbose=0, batch_size=batchsize)\n",
        "print('Load한 Check모델의 train loss를 출력합니다')\n",
        "print(\"train, loss and metric: {}\".format(train_score))\n",
        "cv_score = loaded_model.evaluate(cv_input,cv_target,verbose=0, batch_size=batchsize)\n",
        "print('Load한 Check모델의 val loss를 출력합니다')\n",
        "print(\"valid, loss and metric: {}\".format(cv_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "Load한 Check모델의 train loss를 출력합니다\n",
            "train, loss and metric: [0.3077159990481829, 0.3077159990481829]\n",
            "Load한 Check모델의 val loss를 출력합니다\n",
            "valid, loss and metric: [0.3495782041837298, 0.3495782041837298]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNI-3z8BzMAo",
        "colab_type": "code",
        "outputId": "7bbf4e55-17fd-40eb-9216-7aa424129587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "csv_folder = ''\n",
        "SAVEMODEL_NEWFOLDER1 = 'drive/My Drive/데이콘_천체유형/기존모델저장/' + csv_folder\n",
        "initial_rate = 0.01\n",
        "MODEL_SAVE_FOLDER_PATH1 = SAVEMODEL_NEWFOLDER1 +  '/initial_rate=%s/' % initial_rate ## 기존모델저장\n",
        "\n",
        "## model load\n",
        "json_path = MODEL_SAVE_FOLDER_PATH1 + \"model1.json\"\n",
        "weight_path = MODEL_SAVE_FOLDER_PATH1 +\"model1.h5\"\n",
        "\n",
        "from keras.models import model_from_json\n",
        "json_file = open(json_path, \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "## model weight load\n",
        "loaded_model.load_weights(weight_path)\n",
        "\n",
        "## model load and evaluation\n",
        "from keras import optimizers\n",
        "#final_rate = hist.history['lr'][np.argmin(hist.history['val_loss'])]\n",
        "print(\"Loaded model from disk\")\n",
        "load_optimizer = optimizers.Adam(\n",
        "    #lr=final_rate\n",
        "    lr = 0.0003162278\n",
        ")\n",
        "CCE = metrics.sparse_categorical_crossentropy\n",
        "loaded_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=load_optimizer\n",
        "                     ,metrics=[CCE]\n",
        "                     )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKAM00J7zwTN",
        "colab_type": "code",
        "outputId": "189ebe48-d72e-456b-81a3-7963ea664198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        }
      },
      "source": [
        "t = 1\n",
        "for layer in loaded_model.layers:\n",
        "  print('='*25 + '{0}번째'.format(t) + '='*25)\n",
        "  weights = layer.get_weights()\n",
        "  print(  (np.array(weights) ).shape )\n",
        "  t += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================1번째=========================\n",
            "(0,)\n",
            "=========================2번째=========================\n",
            "(2,)\n",
            "=========================3번째=========================\n",
            "(4, 2373)\n",
            "=========================4번째=========================\n",
            "(0,)\n",
            "=========================5번째=========================\n",
            "(0,)\n",
            "=========================6번째=========================\n",
            "(2,)\n",
            "=========================7번째=========================\n",
            "(4, 2355)\n",
            "=========================8번째=========================\n",
            "(0,)\n",
            "=========================9번째=========================\n",
            "(0,)\n",
            "=========================10번째=========================\n",
            "(2,)\n",
            "=========================11번째=========================\n",
            "(4, 1197)\n",
            "=========================12번째=========================\n",
            "(0,)\n",
            "=========================13번째=========================\n",
            "(0,)\n",
            "=========================14번째=========================\n",
            "(2,)\n",
            "=========================15번째=========================\n",
            "(4, 1187)\n",
            "=========================16번째=========================\n",
            "(0,)\n",
            "=========================17번째=========================\n",
            "(0,)\n",
            "=========================18번째=========================\n",
            "(2,)\n",
            "=========================19번째=========================\n",
            "(4, 612)\n",
            "=========================20번째=========================\n",
            "(0,)\n",
            "=========================21번째=========================\n",
            "(0,)\n",
            "=========================22번째=========================\n",
            "(2,)\n",
            "=========================23번째=========================\n",
            "(4, 602)\n",
            "=========================24번째=========================\n",
            "(0,)\n",
            "=========================25번째=========================\n",
            "(0,)\n",
            "=========================26번째=========================\n",
            "(2,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}